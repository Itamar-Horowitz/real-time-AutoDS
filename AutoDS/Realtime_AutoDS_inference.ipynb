{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Itamar-Horowitz/real-time-AutoDS/blob/main/AutoDS/Realtime_AutoDS_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpCtYevLHfl4"
      },
      "source": [
        "# **AutoDS**\n",
        "\n",
        "---\n",
        "\n",
        "<font size = 4> Deep-STORM is a neural network capable of image reconstruction from high-density single-molecule localization microscopy (SMLM), first published in 2018 by [Nehme *et al.* in Optica](https://www.osapublishing.org/optica/abstract.cfm?uri=optica-5-4-458). The architecture used here is a U-Net based network without skip connections. This network allows image reconstruction of 2D super-resolution images, in a supervised training manner. The network is trained using simulated high-density SMLM data for which the ground-truth is available. These simulations are obtained from random distribution of single molecules in a field-of-view and therefore do not imprint structural priors during training. The network output a super-resolution image with increased pixel density (typically upsampling factor of 8 in each dimension).\n",
        "\n",
        "<font size = 4> AutoDS is an extension of Deep-STORM automating the reconstruction process and aleviating the need in human intervension. This is done by automatic detection of the experimental condition in the analyzed videos and automatic selection of a Deep-STORM model out of a set of pre-trained model for the data processing.\n",
        "\n",
        "<font size = 4> Additionally, AutoDS pipeline splits each input frame into patches and enables processing of different regions in the field-of-view with different models. This mechanism led to an improvment in the reconstruction quality beyond the capabilities of Deep-STORM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEy4EBXHHyAX"
      },
      "source": [
        "# **Before getting started**\n",
        "---\n",
        "<font size = 4> This notebook contains the code required only for inference of SMLM data using a set of pre-trained Deep-STORM models. For model training please follow this [link](https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/blob/main/AutoDS/AutoDS_training.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OlaDqH75fdK"
      },
      "source": [
        "# **Run configuration**\n",
        "---\n",
        "<font size = 4>**`Data_folder`:** This folder should contain the images that you want to use your trained network on for processing.\n",
        "\n",
        "<font size = 4>**`Result_folder`:** This folder will contain the found localizations csv.\n",
        "\n",
        "<font size = 4>**`threshold`:** This paramter determines threshold for local maxima finding. A higher `threshold` will result in less localizations. **DEFAULT: 10**\n",
        "\n",
        "<font size = 4>**`neighborhood_size`:** This paramter determines size of the neighborhood within which the prediction needs to be a local maxima in recovery pixels (CCD pixel/upsampling_factor). A high `neighborhood_size` will make the prediction slower and potentially discard nearby localizations. **DEFAULT: 3**\n",
        "\n",
        "<font size = 4>**`use_local_average`:** This paramter determines whether to locally average the prediction in a 3x3 neighborhood to get the final localizations. If set to **True** it will make inference slightly slower depending on the size of the FOV. **DEFAULT: True**\n",
        "\n",
        "<font size = 4>**`num_patches`:** Determines the number of patches in each row and each column after splitting the frames to patches. The total number of patches will be num_patches<sup>2</sup>. **DEFAULT: 4**\n",
        "\n",
        "<font size = 4>**`batch_size`:** This paramter determines how many frames are processed by any single pass on the GPU. A higher `batch_size` will make the prediction faster but will use more GPU memory. If an OutOfMemory (OOM) error occurs, decrease the `batch_size`. **DEFAULT: 1**\n",
        "\n",
        "<font size = 4>**The following parameters are relevant only if `interpolate_based_on_imaging_parameters` is checked:**\n",
        "\n",
        "<font size = 4> - **`pixel_size` [nm]:** the pixels size of the analyzed video. **DEFAULT: 107**\n",
        "\n",
        "<font size = 4> - **`wavelength` [nm]:** the emission wavelength of the analyzed video. **DEFAULT: 715**\n",
        "\n",
        "<font size = 4> - **`numerical_aperture`:** the optical setup numerical aperture of the analyzed video. **DEFAULT: 1.49**\n",
        "\n",
        "<font size = 4> - **`chunk_size`:** determine the number of patches that will be analyzed in each prediction iteration. This parameter is used for managing compute resources in Google Colab. If you are facing crashes due to RAM memory limitation, decrease the number of patches per chunk. **DEFAULT: 10000**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download testing flie"
      ],
      "metadata": {
        "id": "IZ8Q0-tbf1cB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Data_folder = \"https://github.com/Itamar-Horowitz/real-time-AutoDS/tree/41c8f1af1c8e513cffc314e496424f642f0ddf92/dataset/TOM20_10nM/1\" #@param {type:\"string\"}\n",
        "\n",
        "# ============================================================================\n",
        "# GITHUB DATA DOWNLOAD UTILITIES\n",
        "# ============================================================================\n",
        "def download_github_file(url, destination):\n",
        "    \"\"\"Download a single file from GitHub, handling Git LFS if needed\"\"\"\n",
        "    # Convert GitHub web URL to raw content URL if needed\n",
        "    if 'github.com' in url and '/blob/' in url:\n",
        "        url = url.replace('github.com', 'raw.githubusercontent.com').replace('/blob/', '/')\n",
        "\n",
        "    os.makedirs(os.path.dirname(destination), exist_ok=True)\n",
        "    print(f\"Downloading: {url}\")\n",
        "\n",
        "    # Add headers to avoid GitHub's HTML wrapper\n",
        "    req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "\n",
        "    with urllib.request.urlopen(req) as response:\n",
        "        content = response.read()\n",
        "\n",
        "        # Check if this is a Git LFS pointer file\n",
        "        if b'version https://git-lfs.github.com/spec/' in content[:200]:\n",
        "            print(\"  Detected Git LFS file, extracting download URL...\")\n",
        "            # Parse LFS pointer to get actual file URL\n",
        "            content_str = content.decode('utf-8')\n",
        "            for line in content_str.split('\\n'):\n",
        "                if line.startswith('oid sha256:'):\n",
        "                    oid = line.split(':')[1].strip()\n",
        "                    # Construct LFS download URL\n",
        "                    # Extract user/repo from original URL\n",
        "                    parts = url.split('/')\n",
        "                    user = parts[3]\n",
        "                    repo = parts[4]\n",
        "                    lfs_url = f\"https://media.githubusercontent.com/media/{user}/{repo}/master/{'/'.join(parts[6:])}\"\n",
        "                    print(f\"  LFS URL: {lfs_url}\")\n",
        "\n",
        "                    # Download the actual file\n",
        "                    req_lfs = urllib.request.Request(lfs_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "                    with urllib.request.urlopen(req_lfs) as lfs_response:\n",
        "                        content = lfs_response.read()\n",
        "                    break\n",
        "\n",
        "        # Write content to file\n",
        "        with open(destination, 'wb') as out_file:\n",
        "            out_file.write(content)\n",
        "\n",
        "    # Verify file was downloaded correctly\n",
        "    file_size = os.path.getsize(destination)\n",
        "    if file_size < 1000:  # Files smaller than 1KB are likely error pages\n",
        "        with open(destination, 'rb') as f:\n",
        "            content_check = f.read(100)\n",
        "            if b'<!DOCTYPE' in content_check or b'<html' in content_check:\n",
        "                raise ValueError(f\"Downloaded HTML instead of binary file. URL may be incorrect.\")\n",
        "\n",
        "    print(f\"Saved to: {destination} ({file_size / (1024*1024):.2f} MB)\")\n",
        "\n",
        "\n",
        "def download_tiff_files_fallback(user, repo, commit, dir_path, local_path):\n",
        "    \"\"\"Fallback method to download TIFF files when API fails\"\"\"\n",
        "    os.makedirs(local_path, exist_ok=True)\n",
        "    downloaded = []\n",
        "\n",
        "    # Try common TIFF file numbering patterns\n",
        "    for i in range(1, 100):  # Try up to 100 files\n",
        "        for ext in ['.tif', '.tiff']:\n",
        "            file_name = f\"{i}{ext}\"\n",
        "            raw_url = f\"https://raw.githubusercontent.com/{user}/{repo}/{commit}/{dir_path}/{file_name}\"\n",
        "            dest_path = os.path.join(local_path, file_name)\n",
        "\n",
        "            try:\n",
        "                urllib.request.urlretrieve(raw_url, dest_path)\n",
        "                print(f\"Downloaded: {file_name}\")\n",
        "                downloaded.append(dest_path)\n",
        "                break  # Found file with this number, try next\n",
        "            except:\n",
        "                continue  # File doesn't exist, try next\n",
        "\n",
        "        if i > 10 and len(downloaded) == 0:\n",
        "            break  # Stop if first 10 attempts fail\n",
        "\n",
        "    if len(downloaded) == 0:\n",
        "        print(\"No files found with fallback method.\")\n",
        "    else:\n",
        "        print(f\"Downloaded {len(downloaded)} files using fallback method\")\n",
        "\n",
        "    return downloaded\n",
        "\n",
        "\n",
        "def download_github_directory(repo_url, local_path, branch='main'):\n",
        "    \"\"\"\n",
        "    Download all files from a GitHub directory\n",
        "    Uses git clone with LFS support as primary method\n",
        "\n",
        "    Args:\n",
        "        repo_url: GitHub directory URL (e.g., https://github.com/user/repo/tree/branch/path/to/dir)\n",
        "        local_path: Local directory to save files\n",
        "        branch: Git branch name (default: 'main')\n",
        "    \"\"\"\n",
        "    # Parse the GitHub URL\n",
        "    parts = repo_url.split('github.com/')[-1].split('/')\n",
        "    if len(parts) < 5:\n",
        "        raise ValueError(\"Invalid GitHub directory URL\")\n",
        "\n",
        "    user = parts[0]\n",
        "    repo = parts[1]\n",
        "\n",
        "    # Find where the path starts (after 'tree' and branch/commit)\n",
        "    if 'tree' in parts:\n",
        "        tree_idx = parts.index('tree')\n",
        "        path_parts = parts[tree_idx + 2:]  # Skip 'tree' and branch/commit\n",
        "        dir_path = '/'.join(path_parts)\n",
        "    else:\n",
        "        dir_path = '/'.join(parts[3:])\n",
        "\n",
        "    # Get the commit/branch from URL\n",
        "    if 'tree' in parts:\n",
        "        commit = parts[parts.index('tree') + 1]\n",
        "    else:\n",
        "        commit = branch\n",
        "\n",
        "    # Method 2: Fall back to API-based download\n",
        "    print(\"\\nStart downloading data file\")\n",
        "\n",
        "    api_url = f\"https://api.github.com/repos/{user}/{repo}/contents/{dir_path}?ref={commit}\"\n",
        "\n",
        "    try:\n",
        "        req = urllib.request.Request(api_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "        with urllib.request.urlopen(req) as response:\n",
        "            files_data = json.loads(response.read().decode())\n",
        "    except Exception as e:\n",
        "        return download_tiff_files_fallback(user, repo, commit, dir_path, local_path)\n",
        "\n",
        "    os.makedirs(local_path, exist_ok=True)\n",
        "\n",
        "    downloaded_files = []\n",
        "    for item in files_data:\n",
        "        if item['type'] == 'file':\n",
        "            file_name = item['name']\n",
        "\n",
        "            # Download TIFF, TIF, and ND2 files\n",
        "            if file_name.lower().endswith(('.tif', '.tiff', '.nd2')):\n",
        "                # Use raw.githubusercontent.com for binary files\n",
        "                file_url = f\"https://raw.githubusercontent.com/{user}/{repo}/{commit}/{dir_path}/{file_name}\"\n",
        "                dest_path = os.path.join(local_path, file_name)\n",
        "\n",
        "                try:\n",
        "                    download_github_file(file_url, dest_path, commit_or_branch=commit)\n",
        "                    downloaded_files.append(dest_path)\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to download {file_name}: {e}\")\n",
        "                    # Try using download_url from API as fallback\n",
        "                    if 'download_url' in item and item['download_url']:\n",
        "                        try:\n",
        "                            download_github_file(item['download_url'], dest_path, commit_or_branch=commit)\n",
        "                            downloaded_files.append(dest_path)\n",
        "                        except Exception as e2:\n",
        "                            print(f\"Also failed with API URL: {e2}\")\n",
        "\n",
        "    return downloaded_files\n",
        "\n",
        "# ============================================================================\n",
        "# DOWNLOAD DATA FROM GITHUB\n",
        "# ============================================================================\n",
        "if Data_folder.startswith('http'):\n",
        "    downloaded_files = download_github_directory(Data_folder, Data_folder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiOGc41hefqQ",
        "outputId": "7382f376-0143-487f-c0f3-09cdcc129a7b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start downloading data file\n",
            "No files found with fallback method.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRnQZWSZhArJ"
      },
      "source": [
        "# **V1: Original TensorFlow Version**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kSrZMo3X_NhO",
        "outputId": "7a821716-e4d3-40cd-ed67-ff69e0b475dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You have GPU access\n",
            "[models] found: diff_1\n",
            "[models] found: diff_2\n",
            "[models] found: diff_3\n",
            "[models] found: diff_4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Data_folder' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3789626419.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3418\u001b[0m         \u001b[0mmodel_pixel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3420\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mData_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3421\u001b[0m         \u001b[0;31m# iterate both TIFF and ND2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3422\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_files_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mData_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tif'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tiff'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nd2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Data_folder' is not defined"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import traceback\n",
        "import urllib.request\n",
        "from contextlib import contextmanager\n",
        "\n",
        "import numpy as np\n",
        "import tifffile as tiff\n",
        "import torch\n",
        "from PIL import Image\n",
        "from PIL.TiffTags import TAGS\n",
        "\n",
        "def log(*args, **kwargs):\n",
        "    if not config.QUIET:\n",
        "        print(*args, **kwargs)\n",
        "\n",
        "def list_files_multi(directory, extensions):\n",
        "    exts = {('.' + e.lower()) for e in extensions}\n",
        "    for f in os.listdir(directory):\n",
        "        if os.path.splitext(f)[1].lower() in exts:\n",
        "            yield f\n",
        "\n",
        "def _is_oom(exc: BaseException) -> bool:\n",
        "    msg = (str(exc) or \"\").upper()\n",
        "    return (\n",
        "        isinstance(exc, torch.cuda.OutOfMemoryError) or\n",
        "        isinstance(exc, MemoryError) or\n",
        "        \"OUT OF MEMORY\" in msg or \"OOM\" in msg or\n",
        "        exc.__class__.__name__ in {\"_ArrayMemoryError\", \"OutOfMemoryError\"}\n",
        "    )\n",
        "\n",
        "@contextmanager\n",
        "def catch_oom(phase: str, detail: str = \"\", on_oom=\"continue\"):\n",
        "    \"\"\"\n",
        "    Wrap any memory-heavy block. Prints a friendly message on OOM and continues.\n",
        "    on_oom: \"continue\" (default) just prints and returns; any other value re-raises.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        yield\n",
        "    except Exception as e:\n",
        "        if _is_oom(e):\n",
        "            print(f\"\\n⚠️  OOM while {phase}{(' - ' + detail) if detail else ''}.\")\n",
        "            print(\"   Tip: reduce chunk_size/batch_size/upsampling, or downsample input.\")\n",
        "            if isinstance(e, torch.cuda.OutOfMemoryError):\n",
        "                # PyTorch OOM messages are in str(e) directly\n",
        "                msg_line = str(e).splitlines()[0][:200]\n",
        "                print(\"   PyTorch says:\", msg_line)\n",
        "            else:\n",
        "                traceback.print_exc(limit=1, file=sys.stdout)\n",
        "            if on_oom != \"continue\":\n",
        "                raise\n",
        "        else:\n",
        "            # Non-OOM: re-raise so real bugs are visible\n",
        "            raise\n",
        "\n",
        "# ============================================================================\n",
        "# 1. TIFF File Operations\n",
        "# ============================================================================\n",
        "\n",
        "def getPixelSizeTIFFmetadata(TIFFpath, display=False):\n",
        "    \"\"\"Extract pixel size from TIFF metadata\"\"\"\n",
        "    with Image.open(TIFFpath) as img:\n",
        "        meta_dict = {TAGS[key]: img.tag[key] for key in img.tag.keys()}\n",
        "\n",
        "    ResolutionUnit = meta_dict['ResolutionUnit'][0]\n",
        "    width = meta_dict['ImageWidth'][0]\n",
        "    height = meta_dict['ImageLength'][0]\n",
        "    xResolution = meta_dict['XResolution'][0]\n",
        "\n",
        "    if len(xResolution) == 1:\n",
        "        xResolution = xResolution[0]\n",
        "    elif len(xResolution) == 2:\n",
        "        xResolution = xResolution[0] / xResolution[1]\n",
        "    else:\n",
        "        print('Image resolution not defined.')\n",
        "        xResolution = 1\n",
        "\n",
        "    if ResolutionUnit == 2:\n",
        "        pixel_size = 0.025 * 1e9 / xResolution\n",
        "    elif ResolutionUnit == 3:\n",
        "        pixel_size = 0.01 * 1e9 / xResolution\n",
        "    else:\n",
        "        print('Resolution unit not defined. Assuming: um')\n",
        "        pixel_size = 1e3 / xResolution\n",
        "\n",
        "    if display:\n",
        "        print(f'Pixel size from metadata: {pixel_size} nm')\n",
        "        print(f'Image size: {width}x{height}')\n",
        "\n",
        "    return pixel_size, width, height\n",
        "\n",
        "def saveAsTIF(path, filename, array, pixel_size):\n",
        "    \"\"\"Save array as TIFF with metadata\"\"\"\n",
        "    if array.dtype == np.uint16:\n",
        "        mode = 'I;16'\n",
        "    elif array.dtype == np.uint32:\n",
        "        mode = 'I'\n",
        "    else:\n",
        "        mode = 'F'\n",
        "\n",
        "    if len(array.shape) == 2:\n",
        "        im = Image.fromarray(array)\n",
        "        im.save(os.path.join(path, filename + '.tif'),\n",
        "               mode=mode,\n",
        "               resolution_unit=3,\n",
        "               resolution=0.01 * 1e9 / pixel_size)\n",
        "    elif len(array.shape) == 3:\n",
        "        imlist = []\n",
        "        for frame in array:\n",
        "            imlist.append(Image.fromarray(frame))\n",
        "        imlist[0].save(os.path.join(path, filename + '.tif'),\n",
        "                      save_all=True,\n",
        "                      append_images=imlist[1:],\n",
        "                      mode=mode,\n",
        "                      resolution_unit=3,\n",
        "                      resolution=0.01 * 1e9 / pixel_size)\n",
        "\n",
        "def is_tiff(path):\n",
        "    \"\"\"Check if file is TIFF\"\"\"\n",
        "    return path.lower().endswith(('.tif', '.tiff'))\n",
        "\n",
        "def iter_tiff_frames(path):\n",
        "    \"\"\"Iterate over TIFF frames\"\"\"\n",
        "    with tiff.TiffFile(path) as tif:\n",
        "        for page in tif.pages:\n",
        "            yield page.asarray().astype(np.float32)\n",
        "\n",
        "def count_tiff_frames(path):\n",
        "    \"\"\"Count frames in TIFF file\"\"\"\n",
        "    with tiff.TiffFile(path) as tif:\n",
        "        return len(tif.pages)\n",
        "\n",
        "# ============================================================================\n",
        "# 2. ND2 File Operations\n",
        "# ============================================================================\n",
        "\n",
        "def is_nd2(path):\n",
        "    \"\"\"Check if file is ND2\"\"\"\n",
        "    try:\n",
        "        import nd2\n",
        "        return nd2.is_supported_file(path)\n",
        "    except Exception:\n",
        "        return path.lower().endswith(\".nd2\")\n",
        "\n",
        "def count_nd2_frames(path):\n",
        "    \"\"\"Count frames in ND2 file\"\"\"\n",
        "    import nd2\n",
        "    with nd2.ND2File(path) as f:\n",
        "        try:\n",
        "            return len(f.loop_indices)\n",
        "        except Exception:\n",
        "            sz = getattr(f, \"sizes\", {}) or {}\n",
        "            prod = 1\n",
        "            for ax in (\"T\", \"Z\", \"C\", \"V\"):\n",
        "                prod *= int(sz.get(ax, 1))\n",
        "            return prod\n",
        "\n",
        "def _nd2_to_2d(arr, channel=None):\n",
        "    \"\"\"Convert ND2 frame to 2D\"\"\"\n",
        "    a = np.asarray(arr)\n",
        "    if a.ndim == 2:\n",
        "        return a\n",
        "    if a.ndim == 3:\n",
        "        if a.shape[-1] in (1, 3, 4):\n",
        "            idx = channel if (channel is not None and channel < a.shape[-1]) else 0\n",
        "            return a[..., idx]\n",
        "        if a.shape[0] in (1, 3, 4):\n",
        "            idx = channel if (channel is not None and channel < a.shape[0]) else 0\n",
        "            return a[idx, ...]\n",
        "        return a.mean(axis=0)\n",
        "    a = a.squeeze()\n",
        "    return a if a.ndim == 2 else a.reshape(a.shape[-2], a.shape[-1])\n",
        "\n",
        "def iter_nd2_frames(path, channel=None):\n",
        "    \"\"\"Iterate over ND2 frames\"\"\"\n",
        "    import nd2\n",
        "    n = count_nd2_frames(path)\n",
        "    with nd2.ND2File(path) as f:\n",
        "        for i in range(n):\n",
        "            fr = f.read_frame(i)\n",
        "            fr2d = _nd2_to_2d(fr, channel=channel)\n",
        "            yield fr2d.astype(np.float32, copy=False)\n",
        "\n",
        "def getPixelSizeND2metadata(path, display=False):\n",
        "    \"\"\"Extract pixel size from ND2 metadata\"\"\"\n",
        "    import nd2\n",
        "    with nd2.ND2File(path) as f:\n",
        "        vox_um = getattr(f, \"voxel_size\", None)\n",
        "        if vox_um is None:\n",
        "            return None, None, None\n",
        "        px_nm = vox_um[2] * 1e3\n",
        "        try:\n",
        "            h, w = f.shape[-2], f.shape[-1]\n",
        "        except Exception:\n",
        "            h = w = None\n",
        "        if display:\n",
        "            print(f\"Pixel size (ND2): {px_nm:.2f} nm | image ~ {w}x{h}\")\n",
        "        return px_nm, w, h\n",
        "\n",
        "# ============================================================================\n",
        "# 3. Drift Correction Functions\n",
        "# ============================================================================\n",
        "\n",
        "def correctDriftLocalization(xc_array, yc_array, frames, xDrift, yDrift):\n",
        "    \"\"\"Apply drift correction to localizations\"\"\"\n",
        "    n_locs = xc_array.shape[0]\n",
        "    xc_array_Corr = np.empty(n_locs)\n",
        "    yc_array_Corr = np.empty(n_locs)\n",
        "\n",
        "    for loc in range(n_locs):\n",
        "        xc_array_Corr[loc] = xc_array[loc] - xDrift[frames[loc] - 1]\n",
        "        yc_array_Corr[loc] = yc_array[loc] - yDrift[frames[loc] - 1]\n",
        "\n",
        "    return xc_array_Corr, yc_array_Corr\n",
        "\n",
        "def FromLoc2Image_SimpleHistogram(xc_array, yc_array, image_size=(64, 64), pixel_size=100):\n",
        "    \"\"\"Convert localizations to histogram image\"\"\"\n",
        "    w, h = image_size\n",
        "    locImage = np.zeros(image_size)\n",
        "    n_locs = len(xc_array)\n",
        "\n",
        "    for e in range(n_locs):\n",
        "        y_idx = int(max(min(round(yc_array[e] / pixel_size), w - 1), 0))\n",
        "        x_idx = int(max(min(round(xc_array[e] / pixel_size), h - 1), 0))\n",
        "        locImage[y_idx][x_idx] += 1\n",
        "\n",
        "    return locImage\n",
        "\n",
        "def estimate_drift_com_nm(img1, img2, pixel_size_nm, sigma=1.0, patch_radius=3):\n",
        "    \"\"\"Estimate drift using center of mass of cross-correlation\"\"\"\n",
        "    from scipy.ndimage import gaussian_filter\n",
        "    from scipy.signal import fftconvolve\n",
        "\n",
        "    # Smooth images\n",
        "    img1_smooth = gaussian_filter(img1.astype(np.float32), sigma=sigma)\n",
        "    img2_smooth = gaussian_filter(img2.astype(np.float32), sigma=sigma)\n",
        "\n",
        "    # Cross-correlation\n",
        "    corr = fftconvolve(img1_smooth, img2_smooth, mode='same')\n",
        "\n",
        "    # Center of image\n",
        "    center_y, center_x = np.array(corr.shape) // 2\n",
        "\n",
        "    # Crop around center\n",
        "    y_min = max(0, center_y - patch_radius)\n",
        "    y_max = min(corr.shape[0], center_y + patch_radius + 1)\n",
        "    x_min = max(0, center_x - patch_radius)\n",
        "    x_max = min(corr.shape[1], center_x + patch_radius + 1)\n",
        "\n",
        "    patch = corr[y_min:y_max, x_min:x_max]\n",
        "\n",
        "    # Center of mass\n",
        "    y_grid, x_grid = np.meshgrid(\n",
        "        np.arange(y_min, y_max), np.arange(x_min, x_max), indexing='ij'\n",
        "    )\n",
        "\n",
        "    total = np.sum(patch)\n",
        "    if total == 0:\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    y_com = np.sum(patch * y_grid) / total\n",
        "    x_com = np.sum(patch * x_grid) / total\n",
        "\n",
        "    # Drift in pixels\n",
        "    dy_px = y_com - center_y\n",
        "    dx_px = x_com - center_x\n",
        "\n",
        "    if abs(dy_px) > patch_radius or abs(dx_px) > patch_radius:\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    # Convert to nm\n",
        "    dy_nm = dy_px * pixel_size_nm\n",
        "    dx_nm = dx_px * pixel_size_nm\n",
        "\n",
        "    return dy_nm, dx_nm\n",
        "\n",
        "# ============================================================================\n",
        "# 4. Model Download Utilities\n",
        "# ============================================================================\n",
        "\n",
        "def ensure_models(model_names, target_root=\"/content/AutoDS_models\", model_manifest=None):\n",
        "    if model_manifest is None:\n",
        "        raise ValueError(\"model_manifest must be provided.\")\n",
        "\n",
        "    os.makedirs(target_root, exist_ok=True)\n",
        "\n",
        "    for m in model_names:\n",
        "        cfg = model_manifest[m]\n",
        "        mdir = os.path.join(target_root, m)\n",
        "        need_fetch = False\n",
        "\n",
        "        req = cfg.get(\"contains\", [])\n",
        "        if not os.path.isdir(mdir):\n",
        "            need_fetch = True\n",
        "        else:\n",
        "            for f in req:\n",
        "                if not os.path.exists(os.path.join(mdir, f)):\n",
        "                    need_fetch = True\n",
        "                    break\n",
        "\n",
        "        if not need_fetch:\n",
        "            print(f\"[models] found: {m}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"[models] preparing: {m}\")\n",
        "        os.makedirs(mdir, exist_ok=True)\n",
        "\n",
        "        if \"file_urls\" in cfg:\n",
        "            file_urls = cfg[\"file_urls\"]\n",
        "            for fname, url in file_urls.items():\n",
        "                dst = os.path.join(mdir, fname)\n",
        "                print(f\"[models] downloading: {url}\")\n",
        "                urllib.request.urlretrieve(url, dst)\n",
        "        else:\n",
        "            raise ValueError(f\"Model {m} manifest must have 'file_urls'.\")\n",
        "\n",
        "        for f in req:\n",
        "            if not os.path.exists(os.path.join(mdir, f)):\n",
        "                raise FileNotFoundError(f\"Model {m} missing required file: {f}\")\n",
        "\n",
        "        print(f\"[models] ready: {m}\")\n",
        "\n",
        "    return target_root\n",
        "\n",
        "import numpy as np\n",
        "import scipy.optimize as opt\n",
        "from numpy.lib.stride_tricks import sliding_window_view\n",
        "from scipy.ndimage import gaussian_filter, zoom\n",
        "from scipy.ndimage import gaussian_laplace, maximum_filter, binary_dilation\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ============================================================================\n",
        "# 1. Image Preprocessing Functions\n",
        "# ============================================================================\n",
        "\n",
        "def normalize_im_01(im):\n",
        "    \"\"\"Normalize image to [0, 1]\"\"\"\n",
        "    im = np.squeeze(im)\n",
        "    min_val = im.min()\n",
        "    max_val = im.max()\n",
        "    return (im - min_val) / (max_val - min_val)\n",
        "\n",
        "def normalize_im_01_ret_vals(im):\n",
        "    \"\"\"Normalize and return normalization parameters\"\"\"\n",
        "    im = np.squeeze(im)\n",
        "    min_val = im.min()\n",
        "    max_val = im.max()\n",
        "    return (im - min_val) / (max_val - min_val), min_val, max_val\n",
        "\n",
        "def normalize_im(im, dmean, dstd):\n",
        "    \"\"\"Normalize image with given mean and std\"\"\"\n",
        "    im = np.squeeze(im)\n",
        "    return (im - dmean) / dstd\n",
        "\n",
        "def subtract_smooth_background(im, sigma=3):\n",
        "    \"\"\"Subtract smoothed background\"\"\"\n",
        "    return im - gaussian_filter(im, sigma)\n",
        "\n",
        "def remove_zero_padding(image):\n",
        "    \"\"\"Remove zero padding from image\"\"\"\n",
        "    image_array = np.array(image)\n",
        "    non_zero_rows = np.where(image_array.sum(axis=1) != 0)\n",
        "    non_zero_cols = np.where(image_array.sum(axis=0) != 0)\n",
        "    cropped_image = image_array[non_zero_rows[0][0]:non_zero_rows[0][-1]+1,\n",
        "                                non_zero_cols[0][0]:non_zero_cols[0][-1]+1]\n",
        "    return cropped_image\n",
        "\n",
        "# ============================================================================\n",
        "# 2. Patch Splitting\n",
        "# ============================================================================\n",
        "\n",
        "def split_image_to_patches(img, num_patches, overlap):\n",
        "    \"\"\"\n",
        "    Split image into overlapping patches\n",
        "\n",
        "    Args:\n",
        "        img: Input image (H, W)\n",
        "        num_patches: Number of patches per dimension\n",
        "        overlap: Overlap size in pixels\n",
        "\n",
        "    Returns:\n",
        "        List of patches\n",
        "    \"\"\"\n",
        "    H, W = img.shape\n",
        "    patch_h = H // num_patches\n",
        "    patch_w = W // num_patches\n",
        "\n",
        "    # Pad image for border patches\n",
        "    padded_img = np.pad(img, ((overlap, overlap), (overlap, overlap)), mode='reflect')\n",
        "\n",
        "    # Window shape including overlap\n",
        "    window_shape = (patch_h + 2 * overlap, patch_w + 2 * overlap)\n",
        "\n",
        "    # Create sliding window view\n",
        "    patches_view = sliding_window_view(padded_img, window_shape)\n",
        "\n",
        "    # Sample at regular intervals\n",
        "    patches_array = patches_view[0::patch_h, 0::patch_w, :, :]\n",
        "\n",
        "    # Flatten to list\n",
        "    num_rows, num_cols, ph, pw = patches_array.shape\n",
        "    patches_list = [patches_array[i, j].copy()\n",
        "                   for i in range(num_rows)\n",
        "                   for j in range(num_cols)]\n",
        "\n",
        "    return patches_list\n",
        "\n",
        "# ============================================================================\n",
        "# 3. Interpolation and Scaling\n",
        "# ============================================================================\n",
        "\n",
        "def gaussian_interpolation_batch(data_batch, scale, sigma=1):\n",
        "    \"\"\"Apply Gaussian interpolation to batch of images\"\"\"\n",
        "    upsampled_data_batch = []\n",
        "\n",
        "    for data in data_batch:\n",
        "        smoothed_data = gaussian_filter(data, sigma=sigma)\n",
        "        upsampled_data = zoom(smoothed_data, scale, order=3)\n",
        "        upsampled_data_batch.append(upsampled_data)\n",
        "\n",
        "    return np.array(upsampled_data_batch)\n",
        "\n",
        "def interpolate_frames(tiff_stack, model_pixel_size, current_pixel_size,\n",
        "                      model_wavelength, current_wavelength,\n",
        "                      model_NA, current_NA):\n",
        "    \"\"\"Interpolate frames to match model parameters\"\"\"\n",
        "    # Set defaults\n",
        "    if model_pixel_size is None:\n",
        "        model_pixel_size = current_pixel_size\n",
        "    if model_wavelength is None:\n",
        "        model_wavelength = current_wavelength\n",
        "    if model_NA is None:\n",
        "        model_NA = current_NA\n",
        "    if current_wavelength is None:\n",
        "        current_wavelength = model_wavelength = 1\n",
        "    if current_NA is None:\n",
        "        current_NA = model_NA = 1\n",
        "\n",
        "    if len(tiff_stack.shape) == 2:\n",
        "        tiff_stack = tiff_stack[None, :, :]\n",
        "\n",
        "    # Compute scaling ratio based on optical parameters\n",
        "    scale_ratio_sq = ((0.21 * model_wavelength / model_NA) ** 2 -\n",
        "                     (0.21 * current_wavelength / current_NA) ** 2)\n",
        "\n",
        "    if scale_ratio_sq > 0:\n",
        "        scale_ratio = np.sqrt(scale_ratio_sq) / model_pixel_size\n",
        "        interpolated_stack = np.stack([\n",
        "            gaussian_filter(tiff_stack[i], scale_ratio)\n",
        "            for i in range(tiff_stack.shape[0])\n",
        "        ])\n",
        "    else:\n",
        "        zoom_factors = (1,\n",
        "                       model_pixel_size / current_pixel_size,\n",
        "                       model_pixel_size / current_pixel_size)\n",
        "        interpolated_stack = zoom(tiff_stack.astype(np.float32),\n",
        "                                 zoom_factors, order=3)\n",
        "\n",
        "    return interpolated_stack.astype(np.float32, copy=False)\n",
        "\n",
        "# ============================================================================\n",
        "# 4. Feature Extraction\n",
        "# ============================================================================\n",
        "\n",
        "def gauss2d(xy, offset, amp, x0, y0, sigma):\n",
        "    \"\"\"2D Gaussian function for fitting\"\"\"\n",
        "    x, y = xy\n",
        "    return offset + (amp * np.exp(-((x - x0) ** 2) / (2 * sigma ** 2) -\n",
        "                                  ((y - y0) ** 2) / (2 * sigma ** 2)))\n",
        "\n",
        "def extract_features_frame(OrigImage, pixel_size, psf_sigma, offset=None, verbose=False):\n",
        "    \"\"\"\n",
        "    Extract features from a single frame\n",
        "\n",
        "    Returns:\n",
        "        ADC_offset: Mean background\n",
        "        ReadOutNoise_ADC: Std of background\n",
        "        Signal_amp: Mean signal amplitude\n",
        "        emitter_density: Density of emitters (per μm²)\n",
        "    \"\"\"\n",
        "    M, N = OrigImage.shape\n",
        "\n",
        "    # Subtract smooth background\n",
        "    Image = OrigImage - gaussian_filter(OrigImage, sigma=5)\n",
        "\n",
        "    # Check if SNR is sufficient\n",
        "    if offset is not None:\n",
        "        if (np.percentile(gaussian_filter(Image, 2), 99) < 2 * Image.mean() or\n",
        "            np.percentile(OrigImage, 99) < 2 * offset):\n",
        "            if verbose:\n",
        "                print(\"SNR too low - ignoring patch\")\n",
        "            return np.mean(OrigImage), np.std(OrigImage), 0, 0\n",
        "\n",
        "    # Laplacian of Gaussian for blob detection\n",
        "    log_image = -gaussian_laplace(Image, sigma=psf_sigma)\n",
        "\n",
        "    # Local maxima filtering\n",
        "    neighborhood_size = 3\n",
        "    local_max = (log_image == maximum_filter(log_image, size=neighborhood_size))\n",
        "\n",
        "    # Intensity threshold\n",
        "    amp_threshold = np.mean(Image) + 0.5 * (np.percentile(Image, 99) - np.mean(Image))\n",
        "    pcntl_threshold = np.percentile(Image, 85)\n",
        "\n",
        "    # Binary mask for emitters\n",
        "    binary_mask = np.logical_and(local_max,\n",
        "                                 Image > np.max([amp_threshold, pcntl_threshold]))\n",
        "\n",
        "    # Dilate and create noise mask\n",
        "    dilated_mask = binary_dilation(binary_mask, structure=np.ones((5, 5)))\n",
        "    noise_mask = np.ones_like(binary_mask)\n",
        "    noise_mask[dilated_mask] = 0\n",
        "\n",
        "    if np.sum(binary_mask) > 0:\n",
        "        ADC_offset = np.mean(OrigImage[noise_mask])\n",
        "        ReadOutNoise_ADC = np.std(OrigImage[noise_mask])\n",
        "        Signal_amp = np.mean(OrigImage[binary_mask == 1])\n",
        "        emitter_density = (10 ** 6) * float(np.sum(binary_mask)) / (M * N * pixel_size ** 2)\n",
        "    else:\n",
        "        if verbose:\n",
        "            print(\"Didn't find any emitters\")\n",
        "        return np.mean(OrigImage), np.std(OrigImage), 0, 0\n",
        "\n",
        "    # Additional SNR check\n",
        "    if Signal_amp / ADC_offset < 2.5:\n",
        "        if emitter_density > 2:\n",
        "            if verbose:\n",
        "                print(\"SNR too low for emitter density estimation\")\n",
        "            return ADC_offset, ReadOutNoise_ADC, Signal_amp, 0\n",
        "\n",
        "    return ADC_offset, ReadOutNoise_ADC, Signal_amp, emitter_density\n",
        "\n",
        "# ============================================================================\n",
        "# 5. Model Selection\n",
        "# ============================================================================\n",
        "\n",
        "def ChooseNetByDifficulty_2025(density, SNR):\n",
        "    \"\"\" Choose network based on density and SNR \"\"\"\n",
        "    num_models = 4\n",
        "    norm_density = np.max([np.min([int(np.round(2 * density)), num_models - 1]), 0])\n",
        "    norm_SNR = num_models - 1 - np.max([np.min([SNR // 2, num_models - 1]), 0])\n",
        "    return int(np.round((norm_SNR + norm_density) / 2))\n",
        "\n",
        "# ============================================================================\n",
        "# Module-level kernel cache (shared across all calls)\n",
        "_kernel_cache = {}\n",
        "\n",
        "def _get_gaussian_kernel(sigma, device):\n",
        "    \"\"\"Generate Gaussian kernel for smoothing\"\"\"\n",
        "    key = f'gauss_{sigma}_{device}'\n",
        "    if key not in _kernel_cache:\n",
        "        kernel_size = int(2 * np.ceil(3 * sigma) + 1)\n",
        "        ax = torch.arange(-kernel_size // 2 + 1., kernel_size // 2 + 1., device=device)\n",
        "        xx, yy = torch.meshgrid(ax, ax, indexing='ij')\n",
        "        kernel = torch.exp(-(xx ** 2 + yy ** 2) / (2 * sigma ** 2))\n",
        "        kernel = kernel / kernel.sum()\n",
        "        _kernel_cache[key] = kernel.view(1, 1, kernel_size, kernel_size)\n",
        "    return _kernel_cache[key]\n",
        "\n",
        "\n",
        "def _get_log_kernel(sigma, device):\n",
        "    \"\"\"Generate Laplacian of Gaussian kernel for blob detection\"\"\"\n",
        "    key = f'log_{sigma}_{device}'\n",
        "    if key not in _kernel_cache:\n",
        "        kernel_size = int(2 * np.ceil(3 * sigma) + 1)\n",
        "        ax = torch.arange(-kernel_size // 2 + 1., kernel_size // 2 + 1., device=device)\n",
        "        xx, yy = torch.meshgrid(ax, ax, indexing='ij')\n",
        "        r2 = xx ** 2 + yy ** 2\n",
        "        kernel = -(1 / (np.pi * sigma ** 4)) * (1 - r2 / (2 * sigma ** 2)) * torch.exp(-r2 / (2 * sigma ** 2))\n",
        "        _kernel_cache[key] = kernel.view(1, 1, kernel_size, kernel_size)\n",
        "    return _kernel_cache[key]\n",
        "\n",
        "\n",
        "def percentile_batch(tensor, percentile):\n",
        "    \"\"\"Calculate percentile for batched tensors\"\"\"\n",
        "    flat = tensor.flatten(1)\n",
        "    result = torch.quantile(flat, percentile / 100.0, dim=1)\n",
        "    return result\n",
        "\n",
        "def extract_features_batch(patches_tensor, pixel_size, psf_sigma, offset_array=None,\n",
        "                           verbose=False, device='cuda'):\n",
        "    \"\"\"Fully GPU-accelerated batch feature extraction\"\"\"\n",
        "    B, H, W = patches_tensor.shape\n",
        "    device = patches_tensor.device\n",
        "\n",
        "    # Add channel dimension for conv operations: [B, 1, H, W]\n",
        "    patches_4d = patches_tensor.unsqueeze(1)\n",
        "\n",
        "    # 1. Gaussian filtering\n",
        "    gauss_kernel = _get_gaussian_kernel(5, device)\n",
        "    padding = gauss_kernel.shape[-1] // 2\n",
        "    smooth_bg = F.conv2d(patches_4d, gauss_kernel, padding=padding)\n",
        "    Image = patches_4d - smooth_bg # [B, 1, H, W]\n",
        "\n",
        "    # 2. LoG filtering\n",
        "    log_kernel = _get_log_kernel(psf_sigma, device)\n",
        "    padding = log_kernel.shape[-1] // 2\n",
        "    log_image = -F.conv2d(Image, log_kernel, padding=padding) # [B, 1, H, W]\n",
        "\n",
        "    # 3. Local maxima\n",
        "    local_max = F.max_pool2d(log_image, kernel_size=3, stride=1, padding=1) # [B, 1, H, W]\n",
        "\n",
        "    # 4. Thresholding (all with size [B, 1, 1, 1])\n",
        "    img_mean = Image.mean(dim=(2, 3), keepdim=True)\n",
        "    img_99 = percentile_batch(Image.squeeze(1), 99).view(B, 1, 1, 1)\n",
        "    img_85 = percentile_batch(Image.squeeze(1), 85).view(B, 1, 1, 1)\n",
        "    threshold = torch.max(img_mean + 0.5 * (img_99 - img_mean), img_85)\n",
        "\n",
        "    # 5. Binary masks (batch-wise)\n",
        "    binary_mask = torch.logical_and(log_image == local_max, Image >= threshold)\n",
        "    mask_float = binary_mask.float()\n",
        "    dilated = F.max_pool2d(mask_float, kernel_size=5, stride=1, padding=2)\n",
        "    noise_mask = (dilated < 0.5)\n",
        "\n",
        "    # 6. PRE-COMPUTE SNR check data on GPU as a batch\n",
        "    gauss_kernel_2 = _get_gaussian_kernel(2, device)\n",
        "    padding_2 = (gauss_kernel_2.shape[-1] // 2)\n",
        "    gauss_smooth = F.conv2d(Image, gauss_kernel_2, padding=padding_2)\n",
        "\n",
        "    # Pre-compute percentiles on GPU (batch-wise)\n",
        "    gauss_99 = percentile_batch(gauss_smooth.squeeze(1), 99) #[B]\n",
        "    patch_99 = percentile_batch(patches_tensor, 99)  # [B]\n",
        "    img_mean_flat = img_mean.squeeze()  # [B]\n",
        "\n",
        "    # 7. Statistics on CPU\n",
        "    patches_cpu = patches_tensor.cpu().numpy()\n",
        "    binary_mask_cpu = binary_mask.squeeze(1).cpu().numpy()\n",
        "    noise_mask_cpu = noise_mask.squeeze(1).cpu().numpy()\n",
        "\n",
        "    # Move pre-computed values to CPU\n",
        "    gauss_99_cpu = gauss_99.cpu().numpy()\n",
        "    patch_99_cpu = patch_99.cpu().numpy()\n",
        "    img_mean_cpu = img_mean_flat.cpu().numpy()\n",
        "\n",
        "    results = []\n",
        "    pixel_area = pixel_size * pixel_size\n",
        "\n",
        "    for i in range(B):\n",
        "        patch = patches_cpu[i]\n",
        "        emitter_mask = binary_mask_cpu[i]\n",
        "        noise_m = noise_mask_cpu[i]\n",
        "        patch_offset = offset_array[i]\n",
        "\n",
        "        if patch_offset is not None:\n",
        "            if (gauss_99_cpu[i] < 2 * img_mean_cpu[i] or\n",
        "                patch_99_cpu[i] < 2 * patch_offset):\n",
        "                if verbose:\n",
        "                    print(f\"Patch {i}: SNR too low - ignoring patch\")\n",
        "                results.append((patch.mean(), patch.std(), 0.0, 0.0))\n",
        "                continue\n",
        "\n",
        "        num_emitters = emitter_mask.sum()\n",
        "        if num_emitters == 0:\n",
        "            if verbose:\n",
        "                print(f\"Patch {i}: Didn't find any emitters\")\n",
        "            results.append((patch.mean(), patch.std(), 0.0, 0.0))\n",
        "            continue\n",
        "\n",
        "        ADC_offset = patch[noise_m].mean()\n",
        "        ReadOutNoise_ADC = patch[noise_m].std()\n",
        "        Signal_amp = patch[emitter_mask].mean()\n",
        "        emitter_density = 1e6 * float(num_emitters) / (H * W * pixel_area)\n",
        "\n",
        "        # Additional SNR check\n",
        "        if Signal_amp / (ADC_offset + 1e-8) < 2.5:\n",
        "            if emitter_density > 2:\n",
        "                if verbose:\n",
        "                    print(f\"Patch {i}: SNR too low for emitter density estimation\")\n",
        "                results.append((float(ADC_offset), float(ReadOutNoise_ADC),\n",
        "                                float(Signal_amp), 0.0))\n",
        "                continue\n",
        "\n",
        "        results.append((float(ADC_offset), float(ReadOutNoise_ADC),\n",
        "                        float(Signal_amp), float(emitter_density)))\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def preprocess_frames_batch(frames_batch, device='cuda'):\n",
        "    \"\"\"GPU-accelerated batch preprocessing of frames\"\"\"\n",
        "    B, H, W = frames_batch.shape\n",
        "\n",
        "    # Calculate 35th percentile for each frame (on GPU)\n",
        "    frames_flat = frames_batch.reshape(B, -1)\n",
        "    p35 = torch.quantile(frames_flat, 0.35, dim=1, keepdim=True)\n",
        "    p35 = p35.view(B, 1, 1)\n",
        "\n",
        "    # Subtract 35th percentile\n",
        "    frames_processed = frames_batch - p35\n",
        "\n",
        "    # Subtract minimum\n",
        "    frames_min = frames_processed.reshape(B, -1).min(dim=1, keepdim=True)[0]\n",
        "    frames_min = frames_min.view(B, 1, 1)\n",
        "    frames_processed = frames_processed - frames_min\n",
        "\n",
        "    # Calculate mean and std for normalization\n",
        "    frames_mean = frames_processed.reshape(B, -1).double().mean(dim=1).float()\n",
        "    frames_std = frames_processed.reshape(B, -1).double().std(dim=1).float() + 1e-6\n",
        "    frames_mean_batch = frames_mean.view(B, 1, 1)\n",
        "    frames_std_batch = frames_std.view(B, 1, 1)\n",
        "\n",
        "    # Normalize\n",
        "    frames_processed = (frames_processed - frames_mean_batch) / frames_std_batch\n",
        "\n",
        "    # Calculate offsets\n",
        "    offsets = frames_processed.reshape(B, -1).mean(dim=1)\n",
        "\n",
        "    return frames_processed, offsets\n",
        "\n",
        "\n",
        "def interpolate_frames_batch(frames_batch, model_pixel_size, current_pixel_size,\n",
        "                                  model_wavelength, current_wavelength,\n",
        "                                  model_NA, current_NA, device='cuda'):\n",
        "    \"\"\"GPU-accelerated batch interpolation for multiple frames\"\"\"\n",
        "    # Handle None values\n",
        "    if model_pixel_size is None: model_pixel_size = current_pixel_size\n",
        "    if model_wavelength is None: model_wavelength = current_wavelength\n",
        "    if model_NA is None: model_NA = current_NA\n",
        "    if current_wavelength is None: current_wavelength = model_wavelength = 1\n",
        "    if current_NA is None: current_NA = model_NA = 1\n",
        "\n",
        "    # Calculate scale ratio\n",
        "    scale_ratio_sq = (0.21 * model_wavelength / model_NA) ** 2 - \\\n",
        "                     (0.21 * current_wavelength / current_NA) ** 2\n",
        "\n",
        "    if scale_ratio_sq > 0:\n",
        "        # Gaussian smoothing path\n",
        "        scale_ratio = np.sqrt(scale_ratio_sq) / model_pixel_size\n",
        "        kernel = _get_gaussian_kernel(scale_ratio, device)\n",
        "\n",
        "        # Apply Gaussian filter to all frames at once\n",
        "        frames_4d = frames_batch.unsqueeze(1)  # (B, 1, H, W)\n",
        "        padding = kernel.shape[-1] // 2\n",
        "        interpolated = F.conv2d(frames_4d, kernel, padding=padding).squeeze(1)\n",
        "    else:\n",
        "        # Zoom/resize path\n",
        "        zoom_factor = model_pixel_size / current_pixel_size\n",
        "\n",
        "        if zoom_factor != 1.0:\n",
        "            # Use bilinear interpolation on GPU\n",
        "            new_h = int(frames_batch.shape[1] * zoom_factor)\n",
        "            new_w = int(frames_batch.shape[2] * zoom_factor)\n",
        "\n",
        "            frames_4d = frames_batch.unsqueeze(1)\n",
        "            interpolated = F.interpolate(frames_4d, size=(new_h, new_w),\n",
        "                                        mode='bicubic', align_corners=False).squeeze(1)\n",
        "        else:\n",
        "            interpolated = frames_batch\n",
        "\n",
        "    return interpolated\n",
        "\n",
        "def split_image_to_patches_batch(img_batch, num_patches, overlap, device='cuda'):\n",
        "    \"\"\" Split tensor of images into overlapping patches \"\"\"\n",
        "    # Handle both 2D and 3D input\n",
        "    if img_batch.dim() == 2:\n",
        "        img_batch = img_batch.unsqueeze(0)  # (H, W) -> (1, H, W)\n",
        "\n",
        "    # Determine the non-overlapping patch size\n",
        "    B, H, W = img_batch.shape\n",
        "    patch_h = H // num_patches\n",
        "    patch_w = W // num_patches\n",
        "\n",
        "    # Pad image for border patches (reflection padding as in the original)\n",
        "    padded = F.pad(img_batch.unsqueeze(1), # (B, 1, H, W)\n",
        "                    (overlap, overlap, overlap, overlap),\n",
        "                    mode='reflect').squeeze(1) # (B, H+2*overlap, W+2*overlap)\n",
        "\n",
        "    # Calculate window shape including overlap\n",
        "    window_h = patch_h + 2 * overlap\n",
        "    window_w = patch_w + 2 * overlap\n",
        "\n",
        "    # create sliding windows along height and then along width with the patch_h and patch_w as the step\n",
        "    patches = padded.unfold(1, window_h, patch_h).unfold(2, window_w, patch_w)\n",
        "    # Shape: (B, num_patches, num_patches, window_h, window_w)\n",
        "\n",
        "    # Reshape to (B, num_patches * num_patches, window_h, window_w)\n",
        "    # Flatten the 2D grid of patches for every frame (row-major order).\n",
        "    B, num_rows, num_cols, ph, pw = patches.shape\n",
        "    patches = patches.reshape(B, num_rows * num_cols, ph, pw)\n",
        "\n",
        "    return patches\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ============================================================================\n",
        "# 1. Basic CNN Model (without upsampling)\n",
        "# ============================================================================\n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, in_channels=1):\n",
        "        super(CNNModel, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.features1 = ConvBNReLU(in_channels, 32, 3)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.features2 = ConvBNReLU(32, 64, 3)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.features3 = ConvBNReLU(64, 128, 3)\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.features4 = ConvBNReLU(128, 512, 3)\n",
        "\n",
        "        # Decoder\n",
        "        self.upsample1 = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.features5 = ConvBNReLU(512, 128, 3)\n",
        "\n",
        "        self.upsample2 = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.features6 = ConvBNReLU(128, 64, 3)\n",
        "\n",
        "        self.upsample3 = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.features7 = ConvBNReLU(64, 32, 3)\n",
        "\n",
        "        # Prediction head\n",
        "        self.prediction = nn.Conv2d(32, 1, 1, stride=1, padding=0, bias=False)\n",
        "        nn.init.orthogonal_(self.prediction.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x = self.features1(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.features2(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = self.features3(x)\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        # Bottleneck\n",
        "        x = self.features4(x)\n",
        "\n",
        "        # Decoder\n",
        "        x = self.upsample1(x)\n",
        "        x = self.features5(x)\n",
        "\n",
        "        x = self.upsample2(x)\n",
        "        x = self.features6(x)\n",
        "\n",
        "        x = self.upsample3(x)\n",
        "        x = self.features7(x)\n",
        "\n",
        "        # Prediction\n",
        "        x = self.prediction(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 2. CNN Building Blocks - optimized with fused Conv+BN+ReL operations\n",
        "# ============================================================================\n",
        "\n",
        "class ConvBNReLU(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=None):\n",
        "        super(ConvBNReLU, self).__init__()\n",
        "\n",
        "        if padding is None:\n",
        "            padding = kernel_size // 2\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size,\n",
        "                              stride=stride, padding=padding, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Initialize with Orthogonal (similar to Keras)\n",
        "        nn.init.orthogonal_(self.conv.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 3. CNN Model with Upsampling - optimized with fused Conv+BN+ReL\n",
        "# ============================================================================\n",
        "\n",
        "class CNNUpsample(nn.Module):\n",
        "    def __init__(self, in_channels=1, upsampling_factor=8):\n",
        "        super(CNNUpsample, self).__init__()\n",
        "        self.upsampling_factor = upsampling_factor\n",
        "\n",
        "        # Encoder with fused blocks\n",
        "        self.conv_bn_relu1 = ConvBNReLU(in_channels, 32, 3, 1)\n",
        "        self.conv_bn_relu2 = ConvBNReLU(32, 64, 3, 1)\n",
        "        self.conv_bn_relu3 = ConvBNReLU(64, 128, 3, 1)\n",
        "        self.conv_bn_relu4 = ConvBNReLU(128, 256, 3, 1)\n",
        "\n",
        "        # Decoder with fused blocks\n",
        "        self.conv_bn_relu5 = ConvBNReLU(256, 128, 3, 1)\n",
        "        self.conv_bn_relu6 = ConvBNReLU(128, 64, 3, 1)\n",
        "\n",
        "        # OPTIMIZED: Upsampling blocks with 3x3 kernels + fused Conv+BN+ReLU\n",
        "        num_upsample_blocks = int(np.log2(upsampling_factor))\n",
        "        self.upsample_blocks = nn.ModuleList()\n",
        "\n",
        "        for i in range(num_upsample_blocks):\n",
        "            in_ch = 64 if i == 0 else 32\n",
        "            block = nn.ModuleDict({\n",
        "                'upsample': nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "                'conv_bn_relu': ConvBNReLU(in_ch, 32, 5, 1)\n",
        "            })\n",
        "            self.upsample_blocks.append(block)\n",
        "\n",
        "        # Prediction head\n",
        "        self.prediction = nn.Conv2d(32, 1, 1, stride=1, padding=0, bias=False)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.orthogonal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x = self.conv_bn_relu1(x)\n",
        "        x = self.conv_bn_relu2(x)\n",
        "        x = self.conv_bn_relu3(x)\n",
        "        x = self.conv_bn_relu4(x)\n",
        "\n",
        "        # Decoder\n",
        "        x = self.conv_bn_relu5(x)\n",
        "        x = self.conv_bn_relu6(x)\n",
        "\n",
        "        # Upsampling\n",
        "        for block in self.upsample_blocks:\n",
        "            x = block['upsample'](x)\n",
        "            x = block['conv_bn_relu'](x)\n",
        "\n",
        "        # Prediction\n",
        "        x = self.prediction(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 1. Gaussian Filter for Loss Computation\n",
        "# ============================================================================\n",
        "\n",
        "def matlab_style_gauss2D(shape=(7, 7), sigma=1):\n",
        "    \"\"\"Create 2D Gaussian kernel matching MATLAB style\"\"\"\n",
        "    m, n = [(ss - 1.) / 2. for ss in shape]\n",
        "    y, x = np.ogrid[-m:m+1, -n:n+1]\n",
        "    h = np.exp(-(x*x + y*y) / (2. * sigma * sigma))\n",
        "    h[h < np.finfo(h.dtype).eps * h.max()] = 0\n",
        "    sumh = h.sum()\n",
        "    if sumh != 0:\n",
        "        h /= sumh\n",
        "    h = h * 2.0\n",
        "    return h.astype(np.float32)\n",
        "\n",
        "# Create Gaussian filter as a tensor\n",
        "psf_heatmap = matlab_style_gauss2D(shape=(7, 7), sigma=1)\n",
        "# Shape: [out_channels, in_channels, height, width] -> [1, 1, 7, 7]\n",
        "gfilter = torch.from_numpy(psf_heatmap).view(1, 1, 7, 7)\n",
        "\n",
        "# ============================================================================\n",
        "# 2. Custom Loss Functions\n",
        "# ============================================================================\n",
        "\n",
        "class L1L2Loss(nn.Module):\n",
        "    \"\"\"Combined L1 + L2 loss with Gaussian filtering\"\"\"\n",
        "    def __init__(self, input_shape):\n",
        "        super(L1L2Loss, self).__init__()\n",
        "        self.input_shape = input_shape\n",
        "        # Register Gaussian filter as buffer (moves with model to GPU)\n",
        "        self.register_buffer('gfilter', gfilter)\n",
        "\n",
        "    def forward(self, spikes_pred, heatmap_true):\n",
        "        # Apply Gaussian convolution to predictions\n",
        "        heatmap_pred = F.conv2d(spikes_pred, self.gfilter, padding=3)\n",
        "\n",
        "        # MSE loss on heatmaps\n",
        "        loss_heatmaps = F.mse_loss(heatmap_pred, heatmap_true)\n",
        "\n",
        "        # L1 loss on spikes (sparsity)\n",
        "        loss_spikes = torch.mean(torch.abs(spikes_pred))\n",
        "\n",
        "        return loss_heatmaps + loss_spikes\n",
        "\n",
        "class CustomLoss(nn.Module):\n",
        "    \"\"\"Custom loss for upsampling model\"\"\"\n",
        "    def __init__(self, input_shape):\n",
        "        super(CustomLoss, self).__init__()\n",
        "        self.input_shape = input_shape\n",
        "        self.register_buffer('gfilter', gfilter)\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        # Apply Gaussian convolution\n",
        "        heatmap_pred = F.conv2d(y_pred, self.gfilter, padding=3)\n",
        "\n",
        "        # MSE on heatmaps\n",
        "        loss_heatmaps = torch.mean((y_true - heatmap_pred) ** 2)\n",
        "\n",
        "        # L1 on predictions (sparsity)\n",
        "        loss_spikes = torch.mean(torch.abs(y_pred))\n",
        "\n",
        "        return loss_heatmaps + loss_spikes\n",
        "\n",
        "# ============================================================================\n",
        "# 3. Maxima Finder Layer (Peak Detection)\n",
        "# ============================================================================\n",
        "\n",
        "class MaximaFinder(nn.Module):\n",
        "    \"\"\"Find local maxima in predicted density maps\"\"\"\n",
        "    def __init__(self, thresh=0.1, neighborhood_size=3, use_local_avg=False):\n",
        "        super(MaximaFinder, self).__init__()\n",
        "        self.thresh = thresh\n",
        "        self.nhood = neighborhood_size\n",
        "        self.use_local_avg = use_local_avg\n",
        "\n",
        "        if use_local_avg:\n",
        "            # Sobel-like kernels for local averaging\n",
        "            kernel_x = torch.tensor([[[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]],\n",
        "                                    dtype=torch.float32).view(1, 1, 3, 3)\n",
        "            kernel_y = torch.tensor([[[-1, -1, -1], [0, 0, 0], [1, 1, 1]]],\n",
        "                                    dtype=torch.float32).view(1, 1, 3, 3)\n",
        "            kernel_sum = torch.ones(1, 1, 3, 3, dtype=torch.float32)\n",
        "\n",
        "            self.register_buffer('kernel_x', kernel_x)\n",
        "            self.register_buffer('kernel_y', kernel_y)\n",
        "            self.register_buffer('kernel_sum', kernel_sum)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Max pooling to find local maxima\n",
        "        max_pool = F.max_pool2d(inputs, kernel_size=self.nhood,\n",
        "                               stride=1, padding=self.nhood//2)\n",
        "\n",
        "        # Condition: value is local max AND above threshold\n",
        "        cond = (max_pool > self.thresh) & (max_pool == inputs)\n",
        "\n",
        "        # Get indices where condition is True\n",
        "        indices = torch.nonzero(cond, as_tuple=False)  # (N, 4): [batch, channel, y, x]\n",
        "\n",
        "        bind = indices[:, 0]  # batch indices\n",
        "        yind = indices[:, 2]  # y coordinates\n",
        "        xind = indices[:, 3]  # x coordinates\n",
        "\n",
        "        # Gather confidence values\n",
        "        confidence = inputs[bind, indices[:, 1], yind, xind]\n",
        "\n",
        "        # Convert to float for potential subpixel refinement\n",
        "        xind = xind.float()\n",
        "        yind = yind.float()\n",
        "\n",
        "        # Subpixel refinement using local averaging\n",
        "        if self.use_local_avg:\n",
        "            # Ensure kernels match input dtype\n",
        "            kernel_x = self.kernel_x.to(inputs.dtype)\n",
        "            kernel_y = self.kernel_y.to(inputs.dtype)\n",
        "            kernel_sum = self.kernel_sum.to(dtype=inputs.dtype)\n",
        "\n",
        "            # Compute gradients\n",
        "            # Sobel-like kernels for local averaging\n",
        "            x_image = F.conv2d(inputs, kernel_x, padding=1)\n",
        "            y_image = F.conv2d(inputs, kernel_y, padding=1)\n",
        "            sum_image = F.conv2d(inputs, kernel_sum, padding=1)\n",
        "\n",
        "            # Gather at detected locations\n",
        "            gathered_sum = sum_image[bind, indices[:, 1], yind.long(), xind.long()]\n",
        "            gathered_x = x_image[bind, indices[:, 1], yind.long(), xind.long()]\n",
        "            gathered_y = y_image[bind, indices[:, 1], yind.long(), xind.long()]\n",
        "\n",
        "            # Compute local offsets\n",
        "            x_local = gathered_x / (gathered_sum + 1e-6)\n",
        "            y_local = gathered_y / (gathered_sum + 1e-6)\n",
        "\n",
        "            # Update positions and confidence\n",
        "            xind = xind + x_local\n",
        "            yind = yind + y_local\n",
        "            confidence = gathered_sum\n",
        "\n",
        "        return bind, xind, yind, confidence\n",
        "\n",
        "# ============================================================================\n",
        "# 6. Maxima Finder Layer (Peak Detection)\n",
        "# ============================================================================\n",
        "\n",
        "class MaximaFinder(nn.Module):\n",
        "    \"\"\"Find local maxima in predicted density maps\"\"\"\n",
        "    def __init__(self, thresh=0.1, neighborhood_size=3, use_local_avg=False):\n",
        "        super(MaximaFinder, self).__init__()\n",
        "        self.thresh = thresh\n",
        "        self.nhood = neighborhood_size\n",
        "        self.use_local_avg = use_local_avg\n",
        "\n",
        "        if use_local_avg:\n",
        "            # Sobel-like kernels for local averaging\n",
        "            kernel_x = torch.tensor([[[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]],\n",
        "                                    dtype=torch.float32).view(1, 1, 3, 3)\n",
        "            kernel_y = torch.tensor([[[-1, -1, -1], [0, 0, 0], [1, 1, 1]]],\n",
        "                                    dtype=torch.float32).view(1, 1, 3, 3)\n",
        "            kernel_sum = torch.ones(1, 1, 3, 3, dtype=torch.float32)\n",
        "\n",
        "            self.register_buffer('kernel_x', kernel_x)\n",
        "            self.register_buffer('kernel_y', kernel_y)\n",
        "            self.register_buffer('kernel_sum', kernel_sum)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Max pooling to find local maxima\n",
        "        max_pool = F.max_pool2d(inputs, kernel_size=self.nhood,\n",
        "                               stride=1, padding=self.nhood//2)\n",
        "\n",
        "        # Condition: value is local max AND above threshold\n",
        "        cond = (max_pool > self.thresh) & (max_pool == inputs)\n",
        "\n",
        "        # Get indices where condition is True\n",
        "        indices = torch.nonzero(cond, as_tuple=False)  # (N, 4): [batch, channel, y, x]\n",
        "\n",
        "        bind = indices[:, 0]  # batch indices\n",
        "        yind = indices[:, 2]  # y coordinates\n",
        "        xind = indices[:, 3]  # x coordinates\n",
        "\n",
        "        # Gather confidence values\n",
        "        confidence = inputs[bind, indices[:, 1], yind, xind]\n",
        "\n",
        "        # Convert to float for potential subpixel refinement\n",
        "        xind = xind.float()\n",
        "        yind = yind.float()\n",
        "\n",
        "        # Subpixel refinement using local averaging\n",
        "        if self.use_local_avg:\n",
        "            # Ensure kernels match input dtype\n",
        "            kernel_x = self.kernel_x.to(inputs.dtype)\n",
        "            kernel_y = self.kernel_y.to(inputs.dtype)\n",
        "            kernel_sum = self.kernel_sum.to(dtype=inputs.dtype)\n",
        "\n",
        "            # Compute gradients\n",
        "            x_image = F.conv2d(inputs, kernel_x, padding=1)\n",
        "            y_image = F.conv2d(inputs, kernel_y, padding=1)\n",
        "            sum_image = F.conv2d(inputs, kernel_sum, padding=1)\n",
        "\n",
        "            # Gather at detected locations\n",
        "            gathered_sum = sum_image[bind, indices[:, 1], yind.long(), xind.long()]\n",
        "            gathered_x = x_image[bind, indices[:, 1], yind.long(), xind.long()]\n",
        "            gathered_y = y_image[bind, indices[:, 1], yind.long(), xind.long()]\n",
        "\n",
        "            # Compute local offsets\n",
        "            x_local = gathered_x / (gathered_sum + 1e-6)\n",
        "            y_local = gathered_y / (gathered_sum + 1e-6)\n",
        "\n",
        "            # Update positions and confidence\n",
        "            xind = xind + x_local\n",
        "            yind = yind + y_local\n",
        "            confidence = gathered_sum\n",
        "\n",
        "        return bind, xind, yind, confidence\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import h5py\n",
        "import re\n",
        "import os\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 1. Model Builder Function\n",
        "# ============================================================================\n",
        "\n",
        "def build_model_upsample(input_shape, lr=0.001, upsampling_factor=8):\n",
        "    \"\"\"\n",
        "    Build upsampling model for PyTorch\n",
        "\n",
        "    Args:\n",
        "        input_shape: Tuple (H, W, C) - note: will be converted to (C, H, W)\n",
        "        lr: Learning rate\n",
        "        upsampling_factor: Upsampling factor\n",
        "\n",
        "    Returns:\n",
        "        model: PyTorch model\n",
        "        optimizer: Adam optimizer\n",
        "        criterion: Loss function\n",
        "    \"\"\"\n",
        "    from c_models_and_layers import CNNUpsample, CustomLoss\n",
        "\n",
        "    # Convert from (H, W, C) to (C, H, W)\n",
        "    in_channels = input_shape[2] if len(input_shape) == 3 else 1\n",
        "\n",
        "    model = CNNUpsample(in_channels=in_channels,\n",
        "                        upsampling_factor=upsampling_factor)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = CustomLoss(input_shape)\n",
        "\n",
        "    return model, optimizer, criterion\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 2. Weight Loading - Support both PyTorch and Keras formats\n",
        "# ============================================================================\n",
        "\n",
        "def load_model_weights(model, weights_path, verbose=True):\n",
        "    \"\"\"\n",
        "    Load model weights from either PyTorch (.pth) or Keras (.h5) format\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "        weights_path: Path to weights file (.pth or .h5)\n",
        "        verbose: Print loading progress\n",
        "    \"\"\"\n",
        "    if weights_path.endswith('.pth'):\n",
        "        load_pytorch_weights(model, weights_path, verbose=verbose)\n",
        "    elif weights_path.endswith('.h5'):\n",
        "        load_keras_weights_to_pytorch(model, weights_path, verbose=verbose)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported weights format: {weights_path}. \"\n",
        "                         f\"Expected .pth or .h5 file\")\n",
        "\n",
        "\n",
        "def load_pytorch_weights(model, pth_path, verbose=True):\n",
        "    \"\"\"\n",
        "    Load PyTorch native weights from .pth file\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "        pth_path: Path to .pth weights file\n",
        "        verbose: Print loading progress\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(f\"Loading PyTorch weights from {pth_path}\")\n",
        "\n",
        "    # Get device from model\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Load state dict\n",
        "    state_dict = torch.load(pth_path, map_location=device)\n",
        "\n",
        "    # Load weights into model\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"✓ PyTorch weights loaded successfully!\")\n",
        "\n",
        "\n",
        "def load_keras_weights_to_pytorch(model, h5_path, verbose=True):\n",
        "    \"\"\"\n",
        "    Load Keras weights from H5 file to PyTorch model\n",
        "\n",
        "    Supports fused Conv+BN+ReLU blocks while maintaining compatibility.\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model (CNNUpsample with fused blocks)\n",
        "        h5_path: Path to Keras H5 weights file\n",
        "        verbose: Print loading progress\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(f\"Loading Keras weights from {h5_path}\")\n",
        "\n",
        "    # Get device from model\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    with h5py.File(h5_path, 'r') as f:\n",
        "        # Get all layer names from the H5 file\n",
        "        if 'model_weights' in f:\n",
        "            weight_group = f['model_weights']\n",
        "        else:\n",
        "            weight_group = f\n",
        "\n",
        "        # Extract layer names\n",
        "        if hasattr(weight_group, 'attrs') and 'layer_names' in weight_group.attrs:\n",
        "            layer_names = [n.decode('utf8') if isinstance(n, bytes) else n\n",
        "                           for n in weight_group.attrs['layer_names']]\n",
        "        else:\n",
        "            layer_names = list(weight_group.keys())\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Found {len(layer_names)} layers in H5 file\")\n",
        "\n",
        "        # Create a dictionary to store weights\n",
        "        keras_weights = {}\n",
        "\n",
        "        for layer_name in layer_names:\n",
        "            if layer_name not in weight_group:\n",
        "                continue\n",
        "\n",
        "            layer_group = weight_group[layer_name]\n",
        "\n",
        "            if not hasattr(layer_group, 'keys'):\n",
        "                continue\n",
        "\n",
        "            # Get weight names for this layer\n",
        "            if hasattr(layer_group, 'attrs') and 'weight_names' in layer_group.attrs:\n",
        "                weight_names = [n.decode('utf8') if isinstance(n, bytes) else n\n",
        "                                for n in layer_group.attrs['weight_names']]\n",
        "            else:\n",
        "                weight_names = list(layer_group.keys())\n",
        "\n",
        "            # Extract weights\n",
        "            layer_weights = {}\n",
        "            for weight_name in weight_names:\n",
        "                if '/' in weight_name:\n",
        "                    weight_key = weight_name.split('/')[-1]\n",
        "                else:\n",
        "                    weight_key = weight_name\n",
        "\n",
        "                try:\n",
        "                    weight_value = layer_group[weight_name][()]\n",
        "                    layer_weights[weight_key] = weight_value\n",
        "                except:\n",
        "                    try:\n",
        "                        weight_value = layer_group[weight_key][()]\n",
        "                        layer_weights[weight_key] = weight_value\n",
        "                    except:\n",
        "                        if verbose:\n",
        "                            print(f\"  Warning: Could not load {weight_name} from {layer_name}\")\n",
        "\n",
        "            if layer_weights:\n",
        "                keras_weights[layer_name] = layer_weights\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Extracted weights from {len(keras_weights)} layers\")\n",
        "\n",
        "        # Assign to PyTorch model with fused blocks\n",
        "        _assign_weights_to_model(model, keras_weights, device, verbose=verbose)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"✓ Keras weights loaded successfully!\")\n",
        "\n",
        "\n",
        "def _assign_weights_to_model(model, keras_weights, device, verbose=True):\n",
        "    \"\"\"Helper function to assign Keras weights to PyTorch model with fused blocks\"\"\"\n",
        "\n",
        "    # Mapping from Keras layer names to PyTorch fused block names\n",
        "    name_mapping = {\n",
        "        'F1': 'conv_bn_relu1',\n",
        "        'BN_1': 'conv_bn_relu1',\n",
        "        'F2': 'conv_bn_relu2',\n",
        "        'BN_2': 'conv_bn_relu2',\n",
        "        'F3': 'conv_bn_relu3',\n",
        "        'BN_3': 'conv_bn_relu3',\n",
        "        'F4': 'conv_bn_relu4',\n",
        "        'BN_4': 'conv_bn_relu4',\n",
        "        'F5': 'conv_bn_relu5',\n",
        "        'BN_5': 'conv_bn_relu5',\n",
        "        'F6': 'conv_bn_relu6',\n",
        "        'BN_6': 'conv_bn_relu6',\n",
        "        'Prediction': 'prediction',\n",
        "    }\n",
        "\n",
        "    model_dict = dict(model.named_modules())\n",
        "    loaded_count = 0\n",
        "\n",
        "    # Load encoder and decoder layers (now fused blocks)\n",
        "    for keras_name, pytorch_name in name_mapping.items():\n",
        "        if keras_name not in keras_weights:\n",
        "            continue\n",
        "\n",
        "        if pytorch_name not in model_dict:\n",
        "            continue\n",
        "\n",
        "        module = model_dict[pytorch_name]\n",
        "        weights = keras_weights[keras_name]\n",
        "\n",
        "        # Check if this is a fused ConvBNReLU block\n",
        "        if hasattr(module, 'conv') and hasattr(module, 'bn'):\n",
        "            # This is a fused block - load into conv and bn sub-modules\n",
        "\n",
        "            # Load Conv2d weights\n",
        "            if 'kernel:0' in weights:\n",
        "                kernel = weights['kernel:0']\n",
        "                kernel_torch = np.transpose(kernel, (3, 2, 0, 1))\n",
        "                module.conv.weight.data = torch.from_numpy(kernel_torch).float().to(device)\n",
        "                loaded_count += 1\n",
        "                if verbose:\n",
        "                    print(f\"  ✓ Loaded {keras_name} -> {pytorch_name}.conv (Conv2d)\")\n",
        "\n",
        "            # Load BatchNorm weights\n",
        "            if 'gamma:0' in weights:\n",
        "                module.bn.weight.data = torch.from_numpy(weights['gamma:0']).float().to(device)\n",
        "            if 'beta:0' in weights:\n",
        "                module.bn.bias.data = torch.from_numpy(weights['beta:0']).float().to(device)\n",
        "            if 'moving_mean:0' in weights:\n",
        "                module.bn.running_mean.data = torch.from_numpy(weights['moving_mean:0']).float().to(device)\n",
        "            if 'moving_variance:0' in weights:\n",
        "                module.bn.running_var.data = torch.from_numpy(weights['moving_variance:0']).float().to(device)\n",
        "\n",
        "            if any(k in weights for k in ['gamma:0', 'beta:0']):\n",
        "                if verbose:\n",
        "                    print(f\"  ✓ Loaded {keras_name} -> {pytorch_name}.bn (BatchNorm)\")\n",
        "\n",
        "        # Load prediction layer (not fused)\n",
        "        elif isinstance(module, nn.Conv2d):\n",
        "            if 'kernel:0' in weights:\n",
        "                kernel = weights['kernel:0']\n",
        "                kernel_torch = np.transpose(kernel, (3, 2, 0, 1))\n",
        "                module.weight.data = torch.from_numpy(kernel_torch).float().to(device)\n",
        "                loaded_count += 1\n",
        "                if verbose:\n",
        "                    print(f\"  ✓ Loaded {keras_name} -> {pytorch_name} (Conv2d)\")\n",
        "\n",
        "            if 'bias:0' in weights and module.bias is not None:\n",
        "                bias = weights['bias:0']\n",
        "                module.bias.data = torch.from_numpy(bias).float().to(device)\n",
        "\n",
        "    # Load upsampling blocks (now with fused conv_bn_relu)\n",
        "    for keras_name in keras_weights.keys():\n",
        "        if 'conv_upsample' in keras_name or 'BN_upsample' in keras_name:\n",
        "            match = re.search(r'(\\d+)', keras_name)\n",
        "            if match:\n",
        "                idx = int(match.group(1)) - 1\n",
        "\n",
        "                if idx >= len(model.upsample_blocks):\n",
        "                    continue\n",
        "\n",
        "                weights = keras_weights[keras_name]\n",
        "\n",
        "                if 'conv_upsample' in keras_name:\n",
        "                    # Access the fused block's conv layer\n",
        "                    fused_block = model.upsample_blocks[idx]['conv_bn_relu']\n",
        "\n",
        "                    if 'kernel:0' in weights and hasattr(fused_block, 'conv'):\n",
        "                        kernel = weights['kernel:0']\n",
        "                        kernel_torch = np.transpose(kernel, (3, 2, 0, 1))\n",
        "                        fused_block.conv.weight.data = torch.from_numpy(kernel_torch).float().to(device)\n",
        "                        loaded_count += 1\n",
        "                        if verbose:\n",
        "                            print(f\"  ✓ Loaded {keras_name} -> upsample_blocks[{idx}]['conv_bn_relu'].conv\")\n",
        "\n",
        "                elif 'BN_upsample' in keras_name:\n",
        "                    # Access the fused block's bn layer\n",
        "                    fused_block = model.upsample_blocks[idx]['conv_bn_relu']\n",
        "\n",
        "                    if hasattr(fused_block, 'bn'):\n",
        "                        if 'gamma:0' in weights:\n",
        "                            fused_block.bn.weight.data = torch.from_numpy(weights['gamma:0']).float().to(device)\n",
        "                        if 'beta:0' in weights:\n",
        "                            fused_block.bn.bias.data = torch.from_numpy(weights['beta:0']).float().to(device)\n",
        "                        if 'moving_mean:0' in weights:\n",
        "                            fused_block.bn.running_mean.data = torch.from_numpy(weights['moving_mean:0']).float().to(\n",
        "                                device)\n",
        "                        if 'moving_variance:0' in weights:\n",
        "                            fused_block.bn.running_var.data = torch.from_numpy(weights['moving_variance:0']).float().to(\n",
        "                                device)\n",
        "                        loaded_count += 1\n",
        "                        if verbose:\n",
        "                            print(f\"  ✓ Loaded {keras_name} -> upsample_blocks[{idx}]['conv_bn_relu'].bn\")\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n✓ Successfully loaded {loaded_count} layer weights\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 3. Main Reconstruction Function with Global Profiling\n",
        "# ============================================================================\n",
        "def reconstruct_patches_2025_pytorch(\n",
        "        patches, patch_indices, frame_numbers,\n",
        "        model_num,\n",
        "        num_patches, overlap,\n",
        "        number_of_frames, threshold, neighborhood_size=3,\n",
        "        use_local_avg=True, upsampling_factor=8,\n",
        "        pixel_size=233, batch_size=32, L2_weighting_factor=100,\n",
        "        profiler=None, precision_mode=\"float32\", use_metadata = False):\n",
        "\n",
        "    pixel_size_hr = pixel_size / upsampling_factor\n",
        "\n",
        "    # Get device and precision mode from cache\n",
        "    device = get_device()\n",
        "\n",
        "    patches = patches.float().to(device)\n",
        "\n",
        "     # Convert patches to appropriate precision\n",
        "    if isinstance(patches, np.ndarray):\n",
        "        patches = torch.from_numpy(patches).float().to(device)\n",
        "    else:\n",
        "        patches = patches.to(device)\n",
        "\n",
        "    # Apply precision conversion\n",
        "    if precision_mode == 'fp16' and device.type == 'cuda':\n",
        "        patches = patches.half()\n",
        "    elif precision_mode == 'fp8' and device.type == 'cuda':\n",
        "        patches = patches.to(dtype=torch.float8_e4m3fn)\n",
        "\n",
        "    if patches.ndim == 2:\n",
        "        patches = patches.unsqueeze(0)  # Ensure 3D shape\n",
        "    K_frames, M, N = patches.shape\n",
        "\n",
        "    # Determine dimensions of each predicted (cropped) patch\n",
        "    upsampled_patch_h = M * upsampling_factor - 2 * overlap\n",
        "    upsampled_patch_w = N * upsampling_factor - 2 * overlap\n",
        "\n",
        "    # Create full image tensor on GPU\n",
        "    #dtype = torch.float16 if (precision_mode == \"fp16\" and device.type == 'cuda') else torch.float32\n",
        "    reconstructed_image = torch.zeros((upsampled_patch_h * num_patches, upsampled_patch_w * num_patches),\n",
        "                                      dtype=torch.float32, device=device)\n",
        "\n",
        "    # Prepare lists for detections\n",
        "    recon_xind, recon_yind, frame_index, confidence_list = [], [], [], []\n",
        "\n",
        "    # Store predicted patches for each input patch\n",
        "    all_predicted_patches = []\n",
        "\n",
        "    with torch.cuda.device(0):\n",
        "        # Get model from cache\n",
        "        model = get_model(model_num)\n",
        "        model.eval()\n",
        "\n",
        "        # Create the post-processing layer\n",
        "        max_layer = MaximaFinder(threshold, neighborhood_size, use_local_avg).to(device)\n",
        "\n",
        "        '''\n",
        "        # Convert maxima finder to appropriate precision\n",
        "        if precision_mode == 'fp16' and device.type == 'cuda':\n",
        "            max_layer = max_layer.half()\n",
        "        elif precision_mode == 'fp8' and device.type == 'cuda':\n",
        "            max_layer = max_layer.to(dtype=torch.float8_e4m3fn)\n",
        "        '''\n",
        "\n",
        "        # Process in batches\n",
        "        n_batches = int(np.ceil(K_frames / batch_size))\n",
        "\n",
        "        for batch_idx in range(n_batches):\n",
        "            start_idx = batch_idx * batch_size\n",
        "            end_idx = min(K_frames, start_idx + batch_size)\n",
        "            nF = end_idx - start_idx\n",
        "\n",
        "            # --- Move input batch to GPU ---\n",
        "            batch_imgs = patches[start_idx:end_idx].to(device)  # Shape: (nF, M, N)\n",
        "\n",
        "            # add channel dim to match conv2D\n",
        "            batch_imgs = batch_imgs.unsqueeze(1)  # Shape: (nF, 1, M, N)\n",
        "\n",
        "            # --- Run prediction on GPU ---\n",
        "            profiler.start_timer(\"model forward\")\n",
        "            with torch.no_grad():\n",
        "                if precision_mode == 'fp16' and device.type == 'cuda':\n",
        "                    # Use automatic mixed precision for FP16\n",
        "                    with torch.amp.autocast('cuda', dtype=torch.float16):\n",
        "                        predicted_density = model(batch_imgs)\n",
        "\n",
        "                elif precision_mode == 'int8' and device.type == 'cuda':\n",
        "                    # INT8 weight-only quantization doesn't need special autocast\n",
        "                    # The model handles quantization internally\n",
        "                    predicted_density = model(batch_imgs)\n",
        "\n",
        "                else:\n",
        "                    # Float32 with optional autocast for better performance\n",
        "                    with torch.amp.autocast('cuda', enabled=(device.type == 'cuda')):\n",
        "                        predicted_density = model(batch_imgs)\n",
        "\n",
        "            profiler.stop_timer(\"model forward\")\n",
        "\n",
        "            # Post-processing\n",
        "            predicted_density = torch.relu(predicted_density - 0.5)\n",
        "\n",
        "            # Crop off extra overlap\n",
        "            cropped_pred = predicted_density[:, 0, overlap:-overlap, overlap:-overlap]\n",
        "\n",
        "            profiler.start_timer(\"localizations detection\")\n",
        "            # --- Post-processing on GPU ---\n",
        "            # Maxima detection\n",
        "            bind, xind, yind, conf = max_layer(predicted_density[:, :, overlap:-overlap, overlap:-overlap])\n",
        "\n",
        "            # Convert tensors to NumPy (only when needed)\n",
        "            bind_np = bind.cpu().numpy()\n",
        "            xind_np = xind.cpu().numpy()\n",
        "            yind_np = yind.cpu().numpy()\n",
        "            conf_np = conf.cpu().numpy() / L2_weighting_factor\n",
        "            profiler.stop_timer(\"localizations detection\")\n",
        "\n",
        "            profiler.start_timer(\"reconstruction image building\")\n",
        "            # --- Place each patch in reconstructed image ---\n",
        "            for i in range(nF):\n",
        "                p_ind = patch_indices[start_idx + i]\n",
        "                y1 = upsampled_patch_h * (p_ind // num_patches)\n",
        "                x1 = upsampled_patch_w * (p_ind % num_patches)\n",
        "\n",
        "                # Use PyTorch addition instead of NumPy\n",
        "                reconstructed_image[y1:y1 + upsampled_patch_h,\n",
        "                x1:x1 + upsampled_patch_w].add_(cropped_pred[i] / number_of_frames)\n",
        "\n",
        "                # Collect detections (CPU operations)\n",
        "                det_idx = np.where(bind_np == i)[0]\n",
        "                if det_idx.size:\n",
        "                    recon_xind.extend((x1 + xind_np[det_idx]).tolist())\n",
        "                    recon_yind.extend((y1 + yind_np[det_idx]).tolist())\n",
        "                    frame_index.extend([frame_numbers[start_idx + i] + 1] * det_idx.size)\n",
        "                    confidence_list.extend(conf_np[det_idx].tolist())\n",
        "\n",
        "                all_predicted_patches.append(cropped_pred[i].cpu().numpy())\n",
        "\n",
        "            profiler.stop_timer(\"reconstruction image building\")\n",
        "\n",
        "    # Convert coordinates to physical units\n",
        "    xind_final = (np.array(recon_xind) * pixel_size_hr).tolist()\n",
        "    yind_final = (np.array(recon_yind) * pixel_size_hr).tolist()\n",
        "\n",
        "    # Return reconstructed image, localizations, and predicted patches\n",
        "    return reconstructed_image, [frame_index, xind_final, yind_final, confidence_list], all_predicted_patches\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 4. Weight Validation Function\n",
        "# ============================================================================\n",
        "\n",
        "def validate_model_weights(model, verbose=True):\n",
        "    \"\"\"\n",
        "    Validate that model weights are loaded correctly\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model with loaded weights\n",
        "        verbose: Print validation details\n",
        "\n",
        "    Returns:\n",
        "        bool: True if weights appear valid\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"VALIDATING MODEL WEIGHTS\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "    issues = []\n",
        "\n",
        "    # Check encoder/decoder fused blocks\n",
        "    for i in range(1, 7):\n",
        "        block_name = f'conv_bn_relu{i}'\n",
        "        if hasattr(model, block_name):\n",
        "            block = getattr(model, block_name)\n",
        "\n",
        "            # Check conv weights\n",
        "            conv_weights = block.conv.weight.data\n",
        "            if torch.all(conv_weights == 0):\n",
        "                issues.append(f\"{block_name}.conv weights are all zeros\")\n",
        "            elif torch.isnan(conv_weights).any():\n",
        "                issues.append(f\"{block_name}.conv weights contain NaN\")\n",
        "\n",
        "            # Check BN parameters\n",
        "            if torch.all(block.bn.weight.data == 1) and torch.all(block.bn.bias.data == 0):\n",
        "                issues.append(f\"{block_name}.bn parameters are uninitialized (gamma=1, beta=0)\")\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"  {block_name}.conv: shape={tuple(conv_weights.shape)}, \"\n",
        "                      f\"mean={conv_weights.mean().item():.6f}, std={conv_weights.std().item():.6f}\")\n",
        "                print(f\"  {block_name}.bn: gamma_mean={block.bn.weight.mean().item():.6f}, \"\n",
        "                      f\"beta_mean={block.bn.bias.mean().item():.6f}\")\n",
        "\n",
        "    # Check upsampling blocks\n",
        "    if verbose:\n",
        "        print(f\"\\n  Upsampling blocks: {len(model.upsample_blocks)} blocks\")\n",
        "\n",
        "    for idx, block_dict in enumerate(model.upsample_blocks):\n",
        "        fused_block = block_dict['conv_bn_relu']\n",
        "\n",
        "        conv_weights = fused_block.conv.weight.data\n",
        "        expected_kernel_size = 5\n",
        "        actual_kernel_size = conv_weights.shape[2]\n",
        "\n",
        "        if actual_kernel_size != expected_kernel_size:\n",
        "            issues.append(f\"upsample_blocks[{idx}] has {actual_kernel_size}x{actual_kernel_size} kernel, \"\n",
        "                          f\"expected {expected_kernel_size}x{expected_kernel_size}\")\n",
        "\n",
        "        if torch.all(conv_weights == 0):\n",
        "            issues.append(f\"upsample_blocks[{idx}].conv weights are all zeros\")\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"  upsample_blocks[{idx}].conv: shape={tuple(conv_weights.shape)}, \"\n",
        "                  f\"kernel_size={actual_kernel_size}x{actual_kernel_size}, \"\n",
        "                  f\"mean={conv_weights.mean().item():.6f}\")\n",
        "\n",
        "    # Check prediction layer\n",
        "    pred_weights = model.prediction.weight.data\n",
        "    if torch.all(pred_weights == 0):\n",
        "        issues.append(\"prediction layer weights are all zeros\")\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n  prediction: shape={tuple(pred_weights.shape)}, \"\n",
        "              f\"mean={pred_weights.mean().item():.6f}\")\n",
        "\n",
        "    # Report results\n",
        "    if verbose:\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        if issues:\n",
        "            print(\"⚠️ VALIDATION WARNINGS:\")\n",
        "            for issue in issues:\n",
        "                print(f\"  - {issue}\")\n",
        "        else:\n",
        "            print(\"✓ ALL WEIGHTS VALIDATED SUCCESSFULLY\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "import time\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "\n",
        "class timing_profiler:\n",
        "    def __init__(self, enabled=False):\n",
        "        self.enabled = enabled\n",
        "        self.accu_timing: Dict[str, List[float]] = defaultdict(list)\n",
        "        self.active_timers: Dict[str, float] = {}\n",
        "\n",
        "    def start_timer(self, name):\n",
        "        if not self.enabled:\n",
        "            return\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        self.active_timers[name] = time.perf_counter()\n",
        "\n",
        "    def stop_timer(self, name):\n",
        "        if not self.enabled:\n",
        "            return\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        run_time = time.perf_counter() - self.active_timers[name]\n",
        "        self.accu_timing[name].append(run_time)\n",
        "        del self.active_timers[name]\n",
        "\n",
        "    def get_stats(self, name):\n",
        "        times = self.accu_timing[name]\n",
        "        return {\n",
        "            'total': sum(times),\n",
        "            'average': sum(times) / len(times),\n",
        "            'count': len(times),\n",
        "            'min': min(times),\n",
        "            'max': max(times)\n",
        "        }\n",
        "\n",
        "    def print_timing_summary(self):\n",
        "        if not self.enabled:\n",
        "            return\n",
        "\n",
        "        #print(\"\\n\" + \"=\" * 100)\n",
        "        #print(\"TIMING SUMMARY\")\n",
        "        #print(\"=\" * 100)\n",
        "\n",
        "        sub_sections_timers = {}\n",
        "\n",
        "        for name, times in self.accu_timing.items():\n",
        "            sub_sections_timers[name] = times\n",
        "\n",
        "        # Print reconstruction section\n",
        "        self._print_section(sub_sections_timers, \"\")\n",
        "\n",
        "    def _print_section(self, timers, prefix):\n",
        "        # Group by hierarchy level\n",
        "        hierarchy = {}\n",
        "        for name, times in timers.items():\n",
        "            # Remove prefix\n",
        "            relative_name = name[len(prefix) + 1:] if name.startswith(prefix + '.') else name\n",
        "            hierarchy[relative_name] = times\n",
        "\n",
        "        if not hierarchy:\n",
        "            return\n",
        "\n",
        "        if 'total' in hierarchy:\n",
        "            total_time = sum(hierarchy['total'])\n",
        "        else:\n",
        "            total_time = sum(sum(times) for times in hierarchy.values())\n",
        "        print(\"-\" * 84)\n",
        "        print(f\"{'Step':<40} {'total time':<12} {'avg (per call)':<8} {'calls':<8} {'% of total':<12}\")\n",
        "        print(\"-\" * 84)\n",
        "\n",
        "        # Sort by total time (descending)\n",
        "        sorted_items = sorted(hierarchy.items(), key=lambda x: sum(x[1]), reverse=True)\n",
        "\n",
        "        for section_name, times in sorted_items:\n",
        "            total = sum(times)\n",
        "            avg_per_call = (total / len(times))\n",
        "            count = len(times)\n",
        "            percentage = (total / total_time * 100) if total_time > 0 else 0\n",
        "\n",
        "            print(f\"{section_name:<40} {total:>11.3f} {avg_per_call:>11.2f} {count:>7} {percentage:>10.1f}%\")\n",
        "\n",
        "    def reset(self):\n",
        "        self.accu_timing.clear()\n",
        "        self.active_timers.clear()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from torchao.quantization import quantize_, Int8WeightOnlyConfig\n",
        "\n",
        "# ============================================================================\n",
        "# Global state - simple module-level variables\n",
        "# ============================================================================\n",
        "\n",
        "_models = {}  # Dictionary to store loaded models\n",
        "_device = None  # Device (CPU or CUDA)\n",
        "_is_initialized = False  # Track if we've loaded models\n",
        "_precision_mode = \"float32\"  # Track current precision mode\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Simple functions to manage the cache\n",
        "# ============================================================================\n",
        "\n",
        "def initialize_model_cache(config, upsampling_factor, device=None,\n",
        "                           use_pytorch_weights=False, precision_mode=\"float32\"):\n",
        "    \"\"\"\n",
        "    Load all models once and store them in memory\n",
        "\n",
        "    Args:\n",
        "        config: Configuration object with model paths and names\n",
        "        upsampling_factor: Upsampling factor for the models\n",
        "        device: Device to load models on (CPU or CUDA)\n",
        "        use_pytorch_weights: If True, load .pth weights. If False, load .h5 weights\n",
        "        precision_mode: \"float32\", \"fp16\", or \"int8\"\n",
        "    \"\"\"\n",
        "    global _models, _device, _is_initialized, _precision_mode\n",
        "\n",
        "    # Skip if already initialized\n",
        "    if _is_initialized:\n",
        "        #print(\"⚠️ Models already loaded, skipping initialization\")\n",
        "        return\n",
        "\n",
        "    # Setup device\n",
        "    if device is None:\n",
        "        _device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    else:\n",
        "        _device = device\n",
        "\n",
        "    # Store precision mode\n",
        "    _precision_mode = precision_mode\n",
        "\n",
        "    #print(\"\\n\" + \"=\" * 70)\n",
        "    #print(\"LOADING ALL MODELS INTO CACHE\")\n",
        "    #print(\"=\" * 70)\n",
        "    #print(f\"Device: {_device}\")\n",
        "    #print(f\"Upsampling Factor: {upsampling_factor}\")\n",
        "    #print(f\"Weight Format: {'PyTorch (.pth)' if use_pytorch_weights else 'Keras (.h5)'}\")\n",
        "    #print(f\"Precision Mode: {precision_mode}\")\n",
        "    #print(f\"Number of models: {len(config.model_names)}\")\n",
        "\n",
        "    # Import model classes\n",
        "    from c_models_and_layers import CNNUpsample\n",
        "    from d_reconstruction import load_model_weights\n",
        "\n",
        "    # Determine weight file extension\n",
        "    weight_extension = 'best_weights.pth' if use_pytorch_weights else 'best_weights.h5'\n",
        "    weight_type = \"PyTorch\" if use_pytorch_weights else \"Keras\"\n",
        "\n",
        "    # Load each model\n",
        "    for model_num, model_name in enumerate(config.model_names):\n",
        "        model_path = os.path.join(\n",
        "            config.prediction_model_path,\n",
        "            model_name,\n",
        "            weight_extension\n",
        "        )\n",
        "\n",
        "        if not os.path.exists(model_path):\n",
        "            raise FileNotFoundError(\n",
        "                f\"Model weights not found: {model_path}\\n\"\n",
        "                f\"Expected {weight_type} weights for model: {model_name}\"\n",
        "            )\n",
        "\n",
        "        #print(f\"\\nLoading model {model_num + 1}/{len(config.model_names)}: {model_name} ({weight_type} weights)\")\n",
        "\n",
        "        # Create model\n",
        "        model = CNNUpsample(in_channels=1, upsampling_factor=upsampling_factor)\n",
        "        model = model.to(_device)\n",
        "        model.eval()\n",
        "\n",
        "        # Load weights\n",
        "        load_model_weights(model, model_path, verbose=False)\n",
        "\n",
        "        # Apply precision mode\n",
        "        if precision_mode == \"fp16\":\n",
        "            if _device.type == 'cuda':\n",
        "                model = model.half()  # Convert to FP16\n",
        "                print(f\"→ Converted to FP16 (half precision)\")\n",
        "            #else:\n",
        "            #    print(f\"⚠️ FP16 requested but CUDA not available, using float32\")\n",
        "\n",
        "        elif precision_mode == 'int8':\n",
        "            if _device.type == 'cuda':\n",
        "                quantize_(model, Int8WeightOnlyConfig())\n",
        "                print(\"→ Converted to INT8 quantization\")\n",
        "\n",
        "        elif precision_mode == \"float32\":\n",
        "            # Keep as float32 (default)\n",
        "            print(f\"→ Using float32 (full precision)\")\n",
        "\n",
        "        ## Optimize for inference - channels last\n",
        "        #if torch.cuda.is_available() and precision_mode in ['float32', 'fp16']:\n",
        "        #    try:\n",
        "        #        model = model.to(memory_format=torch.channels_last)\n",
        "        #    except:\n",
        "        #        pass\n",
        "\n",
        "        # Store in cache\n",
        "        _models[model_num] = model\n",
        "\n",
        "        # Print memory usage\n",
        "        if torch.cuda.is_available():\n",
        "            allocated = torch.cuda.memory_allocated() / 1024 ** 2\n",
        "            print(f\"✓ Loaded {model_name} (GPU Memory: {allocated:.1f} MB)\")\n",
        "        else:\n",
        "            print(f\"  ✓ Loaded {model_name}\")\n",
        "\n",
        "    _is_initialized = True\n",
        "\n",
        "    print(f\"✓ ALL {len(_models)} models loaded and cached\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        total_allocated = torch.cuda.memory_allocated() / 1024 ** 2\n",
        "        #print(f\"Total GPU Memory Used: {total_allocated:.1f} MB\")\n",
        "        #print(\"=\" * 70)\n",
        "\n",
        "\n",
        "def get_model(model_num):\n",
        "    \"\"\"Get a cached model by its number\"\"\"\n",
        "    if not _is_initialized:\n",
        "        raise RuntimeError(\"Models not loaded. Call initialize_model_cache() first.\")\n",
        "\n",
        "    if model_num not in _models:\n",
        "        raise KeyError(f\"Model {model_num} not found. Available: {list(_models.keys())}\")\n",
        "\n",
        "    return _models[model_num]\n",
        "\n",
        "\n",
        "def get_device():\n",
        "    \"\"\"Get the device being used (CPU or CUDA)\"\"\"\n",
        "    if _device is None:\n",
        "        raise RuntimeError(\"Model cache not initialized. Call initialize_model_cache() first.\")\n",
        "\n",
        "    return _device\n",
        "\n",
        "\n",
        "def get_precision_mode():\n",
        "    \"\"\"Get the current precision mode\"\"\"\n",
        "    return _precision_mode\n",
        "\n",
        "\n",
        "def clear_cache():\n",
        "    \"\"\"Clear all cached models from memory\"\"\"\n",
        "    global _models, _device, _is_initialized, _precision_mode\n",
        "\n",
        "    print(\"\\n⚠️ Clearing model cache...\")\n",
        "\n",
        "    # Move models to CPU and delete\n",
        "    for model in _models.values():\n",
        "        model.cpu()\n",
        "\n",
        "    _models.clear()\n",
        "    _device = None\n",
        "    _is_initialized = False\n",
        "    _precision_mode = \"float32\"\n",
        "\n",
        "    # Clear GPU memory\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"✓ Model cache cleared\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Compatibility wrapper (optional - for backwards compatibility)\n",
        "# ============================================================================\n",
        "\n",
        "class ModelCacheManager:\n",
        "    \"\"\"Simple wrapper class for compatibility with existing code\"\"\"\n",
        "\n",
        "    def initialize(self, config, upsampling_factor, device=None,\n",
        "                   use_pytorch_weights=False, precision_mode=\"float32\"):\n",
        "        initialize_model_cache(config, upsampling_factor, device,\n",
        "                               use_pytorch_weights, precision_mode)\n",
        "\n",
        "    def get_model(self, model_num):\n",
        "        return get_model(model_num)\n",
        "\n",
        "    def get_device(self):\n",
        "        return get_device()\n",
        "\n",
        "    def get_precision_mode(self):\n",
        "        return get_precision_mode()\n",
        "\n",
        "    def clear_cache(self):\n",
        "        clear_cache()\n",
        "\n",
        "\n",
        "def get_model_cache():\n",
        "    \"\"\"Return a simple manager instance for compatibility\"\"\"\n",
        "    return ModelCacheManager()\n",
        "\n",
        "\n",
        "import pickle\n",
        "import gzip\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "from typing import Optional, Tuple, Dict, Any\n",
        "import threading\n",
        "import queue\n",
        "\n",
        "\n",
        "class MetadataManager:\n",
        "    \"\"\"\n",
        "    Manages patch metadata with efficient asynchronous saving to disk.\n",
        "    Stores all metadata for future analysis without slowing down processing.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, save_path: str, filename: str):\n",
        "        \"\"\"\n",
        "        Initialize metadata manager\n",
        "\n",
        "        Args:\n",
        "            save_path: Directory to save metadata\n",
        "            filename: Base filename for metadata files\n",
        "        \"\"\"\n",
        "        self.save_path = save_path\n",
        "        self.filename = filename\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "        # In-memory storage (will be written to disk asynchronously)\n",
        "        self._metadata = defaultdict(dict)\n",
        "        self._original_frames = {}  # Store original frames for visualization\n",
        "\n",
        "        # Asynchronous saving queue\n",
        "        self._save_queue = queue.Queue()\n",
        "        self._saver_thread = None\n",
        "        self._stop_saving = threading.Event()\n",
        "\n",
        "        # Statistics\n",
        "        self.total_patches = 0\n",
        "\n",
        "    def add_patch_metadata(self, frame_idx: int, patch_idx: int, metadata: Dict[str, Any]):\n",
        "        \"\"\"\n",
        "        Add metadata for a patch (non-blocking)\n",
        "\n",
        "        Args:\n",
        "            frame_idx: Frame index\n",
        "            patch_idx: Patch index within frame\n",
        "            metadata: Dictionary containing all patch information:\n",
        "                - valid_patch: torch.Tensor (valid area of the patch)\n",
        "                - predicted_patch: np.ndarray (reconstructed patch)\n",
        "                - curr_mean_noise: float\n",
        "                - curr_std_noise: float\n",
        "                - signal_amp: float\n",
        "                - curr_emitter_density: float\n",
        "                - difficulty: int (model selection)\n",
        "        \"\"\"\n",
        "        # Store metadata in memory (lightweight operation)\n",
        "        self._metadata[frame_idx][patch_idx] = metadata\n",
        "        self.total_patches += 1\n",
        "\n",
        "    def add_original_frame(self, frame_idx: int, frame_data: np.ndarray):\n",
        "        \"\"\"\n",
        "        Store original frame for future visualization\n",
        "\n",
        "        Args:\n",
        "            frame_idx: Frame index\n",
        "            frame_data: Original frame data (2D numpy array)\n",
        "        \"\"\"\n",
        "        # Store as compressed format to save memory\n",
        "        self._original_frames[frame_idx] = frame_data.astype(np.float32)\n",
        "\n",
        "    def save_to_disk_async(self):\n",
        "        \"\"\"\n",
        "        Start asynchronous background thread to save metadata to disk.\n",
        "        This doesn't block the main processing pipeline.\n",
        "        \"\"\"\n",
        "        if self._saver_thread is None or not self._saver_thread.is_alive():\n",
        "            self._stop_saving.clear()\n",
        "            self._saver_thread = threading.Thread(target=self._background_saver, daemon=True)\n",
        "            self._saver_thread.start()\n",
        "\n",
        "    def _background_saver(self):\n",
        "        \"\"\"Background thread function to save data to disk\"\"\"\n",
        "        while not self._stop_saving.is_set():\n",
        "            try:\n",
        "                # Get data from queue with timeout\n",
        "                save_task = self._save_queue.get(timeout=1.0)\n",
        "\n",
        "                if save_task is None:  # Poison pill to stop thread\n",
        "                    break\n",
        "\n",
        "                # Perform the actual save operation\n",
        "                filepath, data = save_task\n",
        "                with gzip.open(filepath, 'wb', compresslevel=6) as f:\n",
        "                    pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "                self._save_queue.task_done()\n",
        "\n",
        "            except queue.Empty:\n",
        "                continue\n",
        "\n",
        "    def save_all_metadata(self, wait_for_completion: bool = True):\n",
        "        \"\"\"\n",
        "        Save all metadata to disk (can be done asynchronously)\n",
        "\n",
        "        Args:\n",
        "            wait_for_completion: If True, blocks until save is complete\n",
        "        \"\"\"\n",
        "        base_name = os.path.splitext(self.filename)[0]\n",
        "\n",
        "        # Prepare data for saving\n",
        "        metadata_path = os.path.join(self.save_path, f'metadata_{base_name}.pkl.gz')\n",
        "        frames_path = os.path.join(self.save_path, f'original_frames_{base_name}.pkl.gz')\n",
        "\n",
        "        # Convert metadata to serializable format\n",
        "        serializable_metadata = {}\n",
        "        for frame_idx, patches in self._metadata.items():\n",
        "            serializable_metadata[frame_idx] = {}\n",
        "            for patch_idx, data in patches.items():\n",
        "                # Convert tensors to numpy for serialization\n",
        "                serialized_data = {}\n",
        "                for key, value in data.items():\n",
        "                    if isinstance(value, torch.Tensor):\n",
        "                        serialized_data[key] = value.cpu().numpy()\n",
        "                    else:\n",
        "                        serialized_data[key] = value\n",
        "                serializable_metadata[frame_idx][patch_idx] = serialized_data\n",
        "\n",
        "        if wait_for_completion:\n",
        "            # Synchronous save (blocking)\n",
        "            print(f\"\\n start saving metadata:\")\n",
        "            with gzip.open(metadata_path, 'wb', compresslevel=6) as f:\n",
        "                pickle.dump(serializable_metadata, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "            with gzip.open(frames_path, 'wb', compresslevel=6) as f:\n",
        "                pickle.dump(self._original_frames, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "            print(f\"✓ metadata saved: {metadata_path}\")\n",
        "            print(f\"✓ original frames saved: {frames_path}\")\n",
        "        else:\n",
        "            # Asynchronous save (non-blocking)\n",
        "            self._save_queue.put((metadata_path, serializable_metadata))\n",
        "            self._save_queue.put((frames_path, self._original_frames))\n",
        "\n",
        "    def finalize(self):\n",
        "        \"\"\"Wait for all async saves to complete and cleanup\"\"\"\n",
        "        if self._saver_thread is not None and self._saver_thread.is_alive():\n",
        "            self._save_queue.put(None)  # Poison pill\n",
        "            self._saver_thread.join(timeout=30)\n",
        "            self._stop_saving.set()\n",
        "\n",
        "    def clear_memory(self):\n",
        "        \"\"\"Clear in-memory data after saving to disk\"\"\"\n",
        "        self._metadata.clear()\n",
        "        self._original_frames.clear()\n",
        "        self.total_patches = 0\n",
        "\n",
        "    @staticmethod\n",
        "    def load_metadata(save_path: str, filename: str) -> Tuple[Dict, Dict]:\n",
        "        \"\"\"\n",
        "        Load saved metadata from disk\n",
        "\n",
        "        Args:\n",
        "            save_path: Directory containing metadata\n",
        "            filename: Base filename\n",
        "\n",
        "        Returns:\n",
        "            metadata: Dictionary of all patch metadata\n",
        "            original_frames: Dictionary of original frames\n",
        "        \"\"\"\n",
        "        base_name = os.path.splitext(filename)[0]\n",
        "        metadata_path = os.path.join(save_path, f'metadata_{base_name}.pkl.gz')\n",
        "        frames_path = os.path.join(save_path, f'original_frames_{base_name}.pkl.gz')\n",
        "\n",
        "        print(f\"Loading metadata from {metadata_path}...\")\n",
        "        with gzip.open(metadata_path, 'rb') as f:\n",
        "            metadata = pickle.load(f)\n",
        "\n",
        "        print(f\"Loading original frames from {frames_path}...\")\n",
        "        with gzip.open(frames_path, 'rb') as f:\n",
        "            original_frames = pickle.load(f)\n",
        "\n",
        "        print(\"✓ Metadata loaded successfully\")\n",
        "        return metadata, original_frames\n",
        "\n",
        "\n",
        "class FrameReconstructor:\n",
        "    \"\"\"\n",
        "    Reconstructs individual frames and patches from saved metadata.\n",
        "    Only computes reconstructions when requested (lazy evaluation).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, metadata: Dict, original_frames: Dict,\n",
        "                 num_patches: int, upsampling_factor: int):\n",
        "        \"\"\"\n",
        "        Initialize frame reconstructor\n",
        "\n",
        "        Args:\n",
        "            metadata: Loaded metadata dictionary\n",
        "            original_frames: Loaded original frames dictionary\n",
        "            num_patches: Number of patches per dimension\n",
        "            upsampling_factor: Upsampling factor used in reconstruction\n",
        "        \"\"\"\n",
        "        self.metadata = metadata\n",
        "        self.original_frames = original_frames\n",
        "        self.num_patches = num_patches\n",
        "        self.upsampling_factor = upsampling_factor\n",
        "\n",
        "    def reconstruct_frame(self, frame_idx: int, overlap: int = 0,\n",
        "                          save_path: Optional[str] = None) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Reconstruct a complete frame from individual patch predictions\n",
        "\n",
        "        Args:\n",
        "            frame_idx: Frame index to reconstruct\n",
        "            overlap: Overlap size used during patching\n",
        "            save_path: If provided, save as TIFF file\n",
        "\n",
        "        Returns:\n",
        "            reconstructed_frame: Full reconstructed frame\n",
        "        \"\"\"\n",
        "        if frame_idx not in self.metadata:\n",
        "            raise ValueError(f\"Frame {frame_idx} not found in metadata\")\n",
        "\n",
        "        patches_dict = self.metadata[frame_idx]\n",
        "\n",
        "        if not patches_dict:\n",
        "            raise ValueError(f\"No patches found for frame {frame_idx}\")\n",
        "\n",
        "        # Get dimensions from first patch\n",
        "        first_patch = list(patches_dict.values())[0]['predicted_patch']\n",
        "        patch_h, patch_w = first_patch.shape\n",
        "\n",
        "        # Calculate full frame dimensions\n",
        "        frame_h = patch_h * self.num_patches\n",
        "        frame_w = patch_w * self.num_patches\n",
        "\n",
        "        # Initialize reconstruction\n",
        "        reconstructed = np.zeros((frame_h, frame_w), dtype=np.float32)\n",
        "\n",
        "        # Place each patch\n",
        "        for patch_idx, patch_data in patches_dict.items():\n",
        "            if patch_data['predicted_patch'] is None:\n",
        "                continue\n",
        "\n",
        "            predicted_patch = patch_data['predicted_patch']\n",
        "\n",
        "            # Calculate position in full frame\n",
        "            row = patch_idx // self.num_patches\n",
        "            col = patch_idx % self.num_patches\n",
        "\n",
        "            y1 = row * patch_h\n",
        "            x1 = col * patch_w\n",
        "\n",
        "            # Place patch (handle potential size mismatches)\n",
        "            y2 = min(y1 + predicted_patch.shape[0], frame_h)\n",
        "            x2 = min(x1 + predicted_patch.shape[1], frame_w)\n",
        "\n",
        "            reconstructed[y1:y2, x1:x2] = predicted_patch[:y2 - y1, :x2 - x1]\n",
        "\n",
        "        # Save if requested\n",
        "        if save_path is not None:\n",
        "            from a_file_loader import saveAsTIF\n",
        "            pixel_size = 233 / self.upsampling_factor  # Default from config\n",
        "            saveAsTIF(save_path, f'reconstructed_frame_{frame_idx}',\n",
        "                      reconstructed, pixel_size)\n",
        "            print(f\"✓ Saved reconstructed frame {frame_idx} to {save_path}\")\n",
        "\n",
        "        return reconstructed\n",
        "\n",
        "    def get_original_frame(self, frame_idx: int,\n",
        "                           save_path: Optional[str] = None) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Get original (pre-reconstruction) frame\n",
        "\n",
        "        Args:\n",
        "            frame_idx: Frame index\n",
        "            save_path: If provided, save as TIFF file\n",
        "\n",
        "        Returns:\n",
        "            original_frame: Original frame data\n",
        "        \"\"\"\n",
        "        if frame_idx not in self.original_frames:\n",
        "            raise ValueError(f\"Original frame {frame_idx} not found\")\n",
        "\n",
        "        original = self.original_frames[frame_idx]\n",
        "\n",
        "        # Save if requested\n",
        "        if save_path is not None:\n",
        "            from a_file_loader import saveAsTIF\n",
        "            pixel_size = 233  # Default from config\n",
        "            saveAsTIF(save_path, f'original_frame_{frame_idx}',\n",
        "                      original, pixel_size)\n",
        "            print(f\"✓ Saved original frame {frame_idx} to {save_path}\")\n",
        "\n",
        "        return original\n",
        "\n",
        "    def get_patch_data(self, frame_idx: int, patch_idx: int) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Get all data for a specific patch\n",
        "\n",
        "        Args:\n",
        "            frame_idx: Frame index\n",
        "            patch_idx: Patch index\n",
        "\n",
        "        Returns:\n",
        "            patch_data: Dictionary with all patch information\n",
        "        \"\"\"\n",
        "        if frame_idx not in self.metadata:\n",
        "            raise ValueError(f\"Frame {frame_idx} not found\")\n",
        "\n",
        "        if patch_idx not in self.metadata[frame_idx]:\n",
        "            raise ValueError(f\"Patch {patch_idx} not found in frame {frame_idx}\")\n",
        "\n",
        "        return self.metadata[frame_idx][patch_idx]\n",
        "\n",
        "    def compare_frames(self, frame_idx: int, save_path: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Create side-by-side comparison of original and reconstructed frame\n",
        "\n",
        "        Args:\n",
        "            frame_idx: Frame index\n",
        "            save_path: If provided, save comparison figure\n",
        "        \"\"\"\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        original = self.get_original_frame(frame_idx)\n",
        "        reconstructed = self.reconstruct_frame(frame_idx)\n",
        "\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "        axes[0].imshow(original, cmap='gray')\n",
        "        axes[0].set_title(f'Original Frame {frame_idx}', fontsize=14)\n",
        "        axes[0].axis('off')\n",
        "\n",
        "        axes[1].imshow(reconstructed, cmap='gray')\n",
        "        axes[1].set_title(f'Reconstructed Frame {frame_idx}', fontsize=14)\n",
        "        axes[1].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path is not None:\n",
        "            plt.savefig(os.path.join(save_path, f'comparison_frame_{frame_idx}.png'),\n",
        "                        dpi=150, bbox_inches='tight')\n",
        "            print(f\"✓ Saved comparison for frame {frame_idx}\")\n",
        "\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Convenience Functions\n",
        "# ============================================================================\n",
        "\n",
        "def visualize_frame_from_file(metadata_path: str, filename: str,\n",
        "                              frame_idx: int, num_patches: int = 8,\n",
        "                              upsampling_factor: int = 8,\n",
        "                              save_flag: bool = False,\n",
        "                              output_path: Optional[str] = None) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Load metadata and visualize a specific frame\n",
        "\n",
        "    Args:\n",
        "        metadata_path: Path to metadata directory\n",
        "        filename: Original filename\n",
        "        frame_idx: Frame to visualize\n",
        "        num_patches: Number of patches per dimension\n",
        "        upsampling_factor: Upsampling factor\n",
        "        save_flag: Whether to save output\n",
        "        output_path: Where to save (if save_flag=True)\n",
        "\n",
        "    Returns:\n",
        "        original_frame: Original frame data\n",
        "        reconstructed_frame: Reconstructed frame data\n",
        "    \"\"\"\n",
        "    # Load metadata\n",
        "    metadata, original_frames = MetadataManager.load_metadata(metadata_path, filename)\n",
        "\n",
        "    # Create reconstructor\n",
        "    reconstructor = FrameReconstructor(metadata, original_frames,\n",
        "                                       num_patches, upsampling_factor)\n",
        "\n",
        "    # Get frames\n",
        "    original = reconstructor.get_original_frame(\n",
        "        frame_idx,\n",
        "        save_path=output_path if save_flag else None\n",
        "    )\n",
        "\n",
        "    reconstructed = reconstructor.reconstruct_frame(\n",
        "        frame_idx,\n",
        "        save_path=output_path if save_flag else None\n",
        "    )\n",
        "\n",
        "    # Create comparison if saving\n",
        "    if save_flag and output_path is not None:\n",
        "        reconstructor.compare_frames(frame_idx, output_path)\n",
        "\n",
        "    return original, reconstructed\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import subprocess\n",
        "from typing import Optional, List, Tuple\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def compare_trio_finals(image_path1: str, image_path2: str, image_path3: str,\n",
        "                   heading1: str, heading2: str, heading3: str,\n",
        "                   result_path: str):\n",
        "    \"\"\"\n",
        "    Compare three final images side by side with headings\n",
        "\n",
        "    Args:\n",
        "        image_path1: Path to first image\n",
        "        image_path2: Path to second image\n",
        "        image_path3: Path to third image\n",
        "        heading1: Heading for first image\n",
        "        heading2: Heading for second image\n",
        "        heading3: Heading for third image\n",
        "        result_path: Path where to save the comparison figure\n",
        "    \"\"\"\n",
        "    # Read all three images\n",
        "    from PIL import Image\n",
        "    img1 = np.array(Image.open(image_path1))\n",
        "    img2 = np.array(Image.open(image_path2))\n",
        "    img3 = np.array(Image.open(image_path3))\n",
        "\n",
        "    # Create figure with 3 subplots side by side\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(21, 7))\n",
        "\n",
        "    # Display first image\n",
        "    axes[0].imshow(img1, cmap='gray' if len(img1.shape) == 2 else None)\n",
        "    axes[0].set_title(heading1, fontsize=14, fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Display second image\n",
        "    axes[1].imshow(img2, cmap='gray' if len(img2.shape) == 2 else None)\n",
        "    axes[1].set_title(heading2, fontsize=14, fontweight='bold')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    # Display third image\n",
        "    axes[2].imshow(img3, cmap='gray' if len(img3.shape) == 2 else None)\n",
        "    axes[2].set_title(heading3, fontsize=14, fontweight='bold')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    # Adjust spacing between subplots\n",
        "    plt.tight_layout(pad=3.0)\n",
        "\n",
        "    # Save the figure\n",
        "    plt.savefig(result_path, dpi=150, bbox_inches='tight')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def analyze_patch_statistics(result_folder: str, filename: str,\n",
        "                             frame_idx: Optional[int] = None):\n",
        "    \"\"\"\n",
        "    Analyze statistics of patches (difficulty distribution, SNR, etc.)\n",
        "\n",
        "    Args:\n",
        "        result_folder: Path to results folder\n",
        "        filename: Original filename\n",
        "        frame_idx: If provided, analyze only this frame. Otherwise, all frames.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'=' * 70}\")\n",
        "    print(f\"Analyzing patch statistics for {filename}\")\n",
        "    print(f\"{'=' * 70}\")\n",
        "\n",
        "    # Load metadata\n",
        "    metadata, _ = MetadataManager.load_metadata(result_folder, filename)\n",
        "\n",
        "    # Collect statistics\n",
        "    difficulties = []\n",
        "    snr_values = []\n",
        "    densities = []\n",
        "\n",
        "    frames_to_analyze = [frame_idx] if frame_idx is not None else metadata.keys()\n",
        "\n",
        "    for fid in frames_to_analyze:\n",
        "        if fid not in metadata:\n",
        "            continue\n",
        "\n",
        "        for patch_data in metadata[fid].values():\n",
        "            difficulties.append(patch_data['difficulty'])\n",
        "\n",
        "            snr = patch_data['signal_amp'] / (patch_data['curr_mean_noise'] + 1e-10)\n",
        "            snr_values.append(snr)\n",
        "\n",
        "            densities.append(patch_data['curr_emitter_density'])\n",
        "\n",
        "    # Create visualization\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "    # Difficulty distribution\n",
        "    unique, counts = np.unique(difficulties, return_counts=True)\n",
        "    axes[0, 0].bar(unique, counts, color='steelblue', edgecolor='black')\n",
        "    axes[0, 0].set_xlabel('Difficulty Level (Model)')\n",
        "    axes[0, 0].set_ylabel('Number of Patches')\n",
        "    axes[0, 0].set_title('Model Selection Distribution')\n",
        "    axes[0, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # SNR distribution\n",
        "    axes[0, 1].hist(snr_values, bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
        "    axes[0, 1].set_xlabel('Signal-to-Noise Ratio')\n",
        "    axes[0, 1].set_ylabel('Frequency')\n",
        "    axes[0, 1].set_title('SNR Distribution')\n",
        "    axes[0, 1].axvline(np.median(snr_values), color='red', linestyle='--',\n",
        "                       label=f'Median: {np.median(snr_values):.2f}')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Density distribution\n",
        "    axes[1, 0].hist(densities, bins=50, color='lightgreen', edgecolor='black', alpha=0.7)\n",
        "    axes[1, 0].set_xlabel('Emitter Density (per μm²)')\n",
        "    axes[1, 0].set_ylabel('Frequency')\n",
        "    axes[1, 0].set_title('Emitter Density Distribution')\n",
        "    axes[1, 0].axvline(np.median(densities), color='darkgreen', linestyle='--',\n",
        "                       label=f'Median: {np.median(densities):.2f}')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Difficulty vs SNR scatter\n",
        "    axes[1, 1].scatter(snr_values, difficulties, alpha=0.3, s=10, color='purple')\n",
        "    axes[1, 1].set_xlabel('Signal-to-Noise Ratio')\n",
        "    axes[1, 1].set_ylabel('Difficulty Level')\n",
        "    axes[1, 1].set_title('Difficulty vs SNR')\n",
        "    axes[1, 1].grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    output_path = os.path.join(result_folder, f'patch_statistics_{os.path.splitext(filename)[0]}.png')\n",
        "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"✓ Saved statistics to: {output_path}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"\\n{'=' * 70}\")\n",
        "    print(f\"Total patches analyzed: {len(difficulties)}\")\n",
        "    print(f\"\\nDifficulty distribution:\")\n",
        "    for diff, count in zip(unique, counts):\n",
        "        print(f\"  Model {diff}: {count} patches ({100 * count / len(difficulties):.1f}%)\")\n",
        "    print(f\"\\nSNR statistics:\")\n",
        "    print(f\"  Mean: {np.mean(snr_values):.2f}\")\n",
        "    print(f\"  Median: {np.median(snr_values):.2f}\")\n",
        "    print(f\"  Range: [{np.min(snr_values):.2f}, {np.max(snr_values):.2f}]\")\n",
        "    print(f\"\\nDensity statistics:\")\n",
        "    print(f\"  Mean: {np.mean(densities):.2f} per μm²\")\n",
        "    print(f\"  Median: {np.median(densities):.2f} per μm²\")\n",
        "    print(f\"  Range: [{np.min(densities):.2f}, {np.max(densities):.2f}]\")\n",
        "    print(f\"{'=' * 70}\\n\")  # ============================================================================\n",
        "\n",
        "\n",
        "# Visualization Utilities - Easy-to-use functions for analyzing saved metadata\n",
        "# ============================================================================\n",
        "\n",
        "def patch_consistency_heatmap(result_folder: str, filename: str,\n",
        "                              num_patches: int = 8,\n",
        "                              model_names: List[str] = ['diff_1', 'diff_2', 'diff_3', 'diff_4'],\n",
        "                              visualization_folder: str = \"visualizations\"):\n",
        "    \"\"\"\n",
        "    Create heatmap showing consistency of patch-to-model assignments across all frames\n",
        "\n",
        "    For each patch position, shows:\n",
        "    - Which model was most frequently selected\n",
        "    - What percentage of frames used that model (consistency %)\n",
        "\n",
        "    Args:\n",
        "        result_folder: Path to results folder\n",
        "        filename: Original filename\n",
        "        num_patches: Number of patches per dimension (creates num_patches x num_patches grid)\n",
        "        model_names: List of model names\n",
        "        visualization_folder: Subfolder name for visualizations\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'=' * 70}\")\n",
        "    print(f\"Analyzing patch consistency across all frames\")\n",
        "    print(f\"{'=' * 70}\")\n",
        "\n",
        "    # Create visualization folder\n",
        "    vis_path = os.path.join(result_folder, visualization_folder)\n",
        "    os.makedirs(vis_path, exist_ok=True)\n",
        "\n",
        "    # Load metadata\n",
        "    metadata, _ = MetadataManager.load_metadata(result_folder, filename)\n",
        "\n",
        "    total_patches = num_patches * num_patches\n",
        "    num_models = len(model_names)\n",
        "\n",
        "    # Count model assignments for each patch position across all frames\n",
        "    patch_model_counts = defaultdict(lambda: np.zeros(num_models, dtype=int))\n",
        "\n",
        "    for frame_idx, patches in metadata.items():\n",
        "        for patch_idx, patch_data in patches.items():\n",
        "            difficulty = patch_data['difficulty']\n",
        "            patch_model_counts[patch_idx][difficulty] += 1\n",
        "\n",
        "    # Calculate consistency metrics\n",
        "    dominant_models = np.zeros((num_patches, num_patches), dtype=int)\n",
        "    consistency_percentages = np.zeros((num_patches, num_patches), dtype=float)\n",
        "\n",
        "    for patch_idx in range(total_patches):\n",
        "        row = patch_idx // num_patches\n",
        "        col = patch_idx % num_patches\n",
        "\n",
        "        counts = patch_model_counts[patch_idx]\n",
        "        total_count = counts.sum()\n",
        "\n",
        "        if total_count > 0:\n",
        "            dominant_model = np.argmax(counts)\n",
        "            consistency = (counts[dominant_model] / total_count) * 100\n",
        "\n",
        "            dominant_models[row, col] = dominant_model\n",
        "            consistency_percentages[row, col] = consistency\n",
        "\n",
        "    # Create visualization\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
        "\n",
        "    # Left: Dominant model per patch\n",
        "    from matplotlib.colors import ListedColormap\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'][:num_models]\n",
        "    cmap = ListedColormap(colors)\n",
        "\n",
        "    im1 = axes[0].imshow(dominant_models, cmap=cmap, vmin=0, vmax=num_models - 1)\n",
        "    axes[0].set_title('Dominant Model per Patch Position', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_xlabel('Patch Column', fontsize=11)\n",
        "    axes[0].set_ylabel('Patch Row', fontsize=11)\n",
        "\n",
        "    # Add grid\n",
        "    for i in range(num_patches + 1):\n",
        "        axes[0].axhline(y=i - 0.5, color='white', linewidth=2)\n",
        "        axes[0].axvline(x=i - 0.5, color='white', linewidth=2)\n",
        "\n",
        "    # Add annotations\n",
        "    for i in range(num_patches):\n",
        "        for j in range(num_patches):\n",
        "            patch_idx = i * num_patches + j\n",
        "            model_idx = dominant_models[i, j]\n",
        "            consistency = consistency_percentages[i, j]\n",
        "\n",
        "            axes[0].text(j, i, f'{model_names[model_idx]}\\n{consistency:.0f}%',\n",
        "                         ha='center', va='center', fontsize=8,\n",
        "                         color='white', fontweight='bold')\n",
        "\n",
        "    # Colorbar\n",
        "    cbar1 = plt.colorbar(im1, ax=axes[0], ticks=range(num_models))\n",
        "    cbar1.set_ticklabels(model_names)\n",
        "    cbar1.set_label('Model', rotation=270, labelpad=20, fontsize=11)\n",
        "\n",
        "    # Right: Consistency percentage heatmap\n",
        "    im2 = axes[1].imshow(consistency_percentages, cmap='RdYlGn', vmin=0, vmax=100)\n",
        "    axes[1].set_title('Consistency Percentage', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_xlabel('Patch Column', fontsize=11)\n",
        "    axes[1].set_ylabel('Patch Row', fontsize=11)\n",
        "\n",
        "    # Add grid\n",
        "    for i in range(num_patches + 1):\n",
        "        axes[1].axhline(y=i - 0.5, color='gray', linewidth=1)\n",
        "        axes[1].axvline(x=i - 0.5, color='gray', linewidth=1)\n",
        "\n",
        "    # Add annotations\n",
        "    for i in range(num_patches):\n",
        "        for j in range(num_patches):\n",
        "            pct = consistency_percentages[i, j]\n",
        "            text_color = 'black' if pct > 50 else 'white'\n",
        "            axes[1].text(j, i, f'{pct:.0f}%',\n",
        "                         ha='center', va='center', fontsize=9,\n",
        "                         color=text_color, fontweight='bold')\n",
        "\n",
        "    # Colorbar\n",
        "    cbar2 = plt.colorbar(im2, ax=axes[1])\n",
        "    cbar2.set_label('Consistency (%)', rotation=270, labelpad=20, fontsize=11)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    output_path = os.path.join(vis_path, 'patch_consistency_heatmap.png')\n",
        "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"✓ Saved to: {output_path}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Print statistics\n",
        "    print(f\"\\n{'=' * 70}\")\n",
        "    print(f\"Patch Consistency Statistics\")\n",
        "    print(f\"{'=' * 70}\")\n",
        "\n",
        "    avg_consistency = np.mean(consistency_percentages)\n",
        "    print(f\"\\nOverall average consistency: {avg_consistency:.1f}%\")\n",
        "\n",
        "    # Find most and least consistent patches\n",
        "    flat_idx = np.argsort(consistency_percentages.ravel())\n",
        "\n",
        "    print(f\"\\nMost consistent patches (always use same model):\")\n",
        "    for i in range(min(5, len(flat_idx))):\n",
        "        idx = flat_idx[-(i + 1)]\n",
        "        row, col = idx // num_patches, idx % num_patches\n",
        "        patch_idx = row * num_patches + col\n",
        "        consistency = consistency_percentages[row, col]\n",
        "        model = model_names[dominant_models[row, col]]\n",
        "        print(f\"  Patch ({row}, {col}) [idx={patch_idx}]: {model} - {consistency:.1f}%\")\n",
        "\n",
        "    print(f\"\\nLeast consistent patches (split between models):\")\n",
        "    for i in range(min(5, len(flat_idx))):\n",
        "        idx = flat_idx[i]\n",
        "        row, col = idx // num_patches, idx % num_patches\n",
        "        patch_idx = row * num_patches + col\n",
        "        consistency = consistency_percentages[row, col]\n",
        "        model = model_names[dominant_models[row, col]]\n",
        "\n",
        "        # Show distribution\n",
        "        counts = patch_model_counts[patch_idx]\n",
        "        total = counts.sum()\n",
        "        print(f\"  Patch ({row}, {col}) [idx={patch_idx}]: {model} - {consistency:.1f}%\")\n",
        "        for m_idx, count in enumerate(counts):\n",
        "            if count > 0:\n",
        "                pct = (count / total) * 100\n",
        "                print(f\"    {model_names[m_idx]}: {pct:.1f}%\")\n",
        "\n",
        "    print(f\"{'=' * 70}\\n\")\n",
        "\n",
        "\n",
        "def compare_patches_across_frames(result_folder: str, filename: str,\n",
        "                                  patch_idx: int,\n",
        "                                  frame_indices: List[int],\n",
        "                                  num_patches: int = 8,\n",
        "                                  upsampling_factor: int = 8,\n",
        "                                  visualization_folder: str = \"visualizations\"):\n",
        "    \"\"\"\n",
        "    Compare a specific patch position across multiple frames\n",
        "\n",
        "    Shows how the same spatial location evolves across different frames\n",
        "\n",
        "    Args:\n",
        "        result_folder: Path to results folder\n",
        "        filename: Original filename\n",
        "        patch_idx: Patch index to track (0 to num_patches^2 - 1)\n",
        "        frame_indices: List of frame indices to compare\n",
        "        num_patches: Number of patches per dimension\n",
        "        upsampling_factor: Upsampling factor\n",
        "        visualization_folder: Subfolder name for visualizations\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'=' * 70}\")\n",
        "    print(f\"Comparing patch {patch_idx} across {len(frame_indices)} frames\")\n",
        "    print(f\"{'=' * 70}\")\n",
        "\n",
        "    # Create visualization folder\n",
        "    vis_path = os.path.join(result_folder, visualization_folder)\n",
        "    os.makedirs(vis_path, exist_ok=True)\n",
        "\n",
        "    # Load metadata\n",
        "    metadata, _ = MetadataManager.load_metadata(result_folder, filename)\n",
        "\n",
        "    # Calculate patch position\n",
        "    row = patch_idx // num_patches\n",
        "    col = patch_idx % num_patches\n",
        "    print(f\"Patch position: Row {row}, Column {col}\")\n",
        "\n",
        "    # Collect patch data\n",
        "    n_frames = len(frame_indices)\n",
        "    fig, axes = plt.subplots(2, n_frames, figsize=(4 * n_frames, 8))\n",
        "\n",
        "    if n_frames == 1:\n",
        "        axes = axes.reshape(-1, 1)\n",
        "\n",
        "    for i, frame_idx in enumerate(frame_indices):\n",
        "        if frame_idx not in metadata:\n",
        "            print(f\"Warning: Frame {frame_idx} not found in metadata\")\n",
        "            continue\n",
        "\n",
        "        if patch_idx not in metadata[frame_idx]:\n",
        "            print(f\"Warning: Patch {patch_idx} not found in frame {frame_idx}\")\n",
        "            continue\n",
        "\n",
        "        patch_data = metadata[frame_idx][patch_idx]\n",
        "\n",
        "        # Get original patch (from full_patch tensor)\n",
        "        original_patch = patch_data['full_patch']\n",
        "        if isinstance(original_patch, np.ndarray):\n",
        "            pass  # Already numpy\n",
        "        else:\n",
        "            original_patch = original_patch.numpy() if hasattr(original_patch, 'numpy') else np.array(original_patch)\n",
        "\n",
        "        # Get predicted patch\n",
        "        predicted_patch = patch_data['predicted_patch']\n",
        "        if predicted_patch is None:\n",
        "            print(f\"Warning: No prediction for patch {patch_idx} in frame {frame_idx}\")\n",
        "            continue\n",
        "\n",
        "        # Display original patch\n",
        "        axes[0, i].imshow(original_patch, cmap='gray')\n",
        "        axes[0, i].set_title(f'Frame {frame_idx}\\nOriginal', fontsize=10)\n",
        "        axes[0, i].axis('off')\n",
        "\n",
        "        # Display predicted patch\n",
        "        vmin, vmax = np.percentile(predicted_patch, [1, 99])\n",
        "        axes[1, i].imshow(np.clip(predicted_patch, vmin, vmax), cmap='hot')\n",
        "\n",
        "        # Add metadata to title\n",
        "        difficulty = patch_data['difficulty']\n",
        "        snr = patch_data['signal_amp'] / (patch_data['curr_mean_noise'] + 1e-10)\n",
        "        density = patch_data['curr_emitter_density']\n",
        "\n",
        "        axes[1, i].set_title(f'Reconstructed\\nModel: {difficulty} | SNR: {snr:.1f}\\nDensity: {density:.1f}',\n",
        "                             fontsize=9)\n",
        "        axes[1, i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    output_path = os.path.join(vis_path, f'patch_{patch_idx}_comparison.png')\n",
        "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"✓ Saved to: {output_path}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Print statistics\n",
        "    print(f\"\\n{'=' * 70}\")\n",
        "    print(f\"Patch {patch_idx} Statistics Across Frames\")\n",
        "    print(f\"{'=' * 70}\")\n",
        "\n",
        "    difficulties = []\n",
        "    snrs = []\n",
        "    densities = []\n",
        "\n",
        "    for frame_idx in frame_indices:\n",
        "        if frame_idx in metadata and patch_idx in metadata[frame_idx]:\n",
        "            patch_data = metadata[frame_idx][patch_idx]\n",
        "            difficulties.append(patch_data['difficulty'])\n",
        "            snr = patch_data['signal_amp'] / (patch_data['curr_mean_noise'] + 1e-10)\n",
        "            snrs.append(snr)\n",
        "            densities.append(patch_data['curr_emitter_density'])\n",
        "\n",
        "    print(f\"\\nModel selection:\")\n",
        "    unique, counts = np.unique(difficulties, return_counts=True)\n",
        "    for model, count in zip(unique, counts):\n",
        "        print(f\"  Model {model}: {count} times ({100 * count / len(difficulties):.1f}%)\")\n",
        "\n",
        "    print(f\"\\nSNR range: [{np.min(snrs):.2f}, {np.max(snrs):.2f}], mean: {np.mean(snrs):.2f}\")\n",
        "    print(f\"Density range: [{np.min(densities):.2f}, {np.max(densities):.2f}], mean: {np.mean(densities):.2f}\")\n",
        "    print(f\"{'=' * 70}\\n\")\n",
        "\n",
        "\n",
        "def quick_visualize_frame(result_folder: str, filename: str, frame_idx: int,\n",
        "                          num_patches: int = 8, upsampling_factor: int = 8,\n",
        "                          save_output: bool = True, visualization_folder: str = \"visualizations\"):\n",
        "    \"\"\"\n",
        "    Quick visualization of a single frame (original vs reconstructed)\n",
        "\n",
        "    Args:\n",
        "        result_folder: Path to results folder containing metadata\n",
        "        filename: Original filename (e.g., 'data.tif')\n",
        "        frame_idx: Frame index to visualize\n",
        "        num_patches: Number of patches per dimension (default: 8)\n",
        "        upsampling_factor: Upsampling factor (default: 8)\n",
        "        save_output: Whether to save the visualization\n",
        "        visualization_folder: Subfolder name for visualizations (default: \"visualizations\")\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'=' * 70}\")\n",
        "    print(f\"Loading frame {frame_idx} from {filename}\")\n",
        "    print(f\"{'=' * 70}\")\n",
        "\n",
        "    # Create visualization folder\n",
        "    vis_path = os.path.join(result_folder, visualization_folder)\n",
        "    os.makedirs(vis_path, exist_ok=True)\n",
        "\n",
        "    # Load metadata\n",
        "    metadata, original_frames = MetadataManager.load_metadata(result_folder, filename)\n",
        "\n",
        "    # Create reconstructor\n",
        "    reconstructor = FrameReconstructor(metadata, original_frames,\n",
        "                                       num_patches, upsampling_factor)\n",
        "\n",
        "    # Get frames\n",
        "    original = reconstructor.get_original_frame(frame_idx)\n",
        "    reconstructed = reconstructor.reconstruct_frame(frame_idx)\n",
        "\n",
        "    # Create visualization\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(30, 12))\n",
        "\n",
        "    # Original\n",
        "    im0 = axes[0,0].imshow(original, cmap='gray')\n",
        "    axes[0,0].set_title(f'Original Frame {frame_idx}', fontsize=14, fontweight='bold')\n",
        "    axes[0,0].axis('off')\n",
        "    fig.colorbar(im0, ax=axes[0,0])\n",
        "\n",
        "    # Histogram of non-zero values for original\n",
        "    nonzero_vals_original = original[original > 0]\n",
        "    axes[1,0].hist(nonzero_vals_original, bins=50, color='orange', edgecolor='black', alpha=0.7)\n",
        "    axes[1,0].set_title(f'Original Values Histogram', fontsize=14, fontweight='bold')\n",
        "    axes[1,0].set_xlabel('Pixel Value')\n",
        "    axes[1,0].set_ylabel('Frequency')\n",
        "    axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Reconstructed\n",
        "    num_nonzero = np.count_nonzero(reconstructed)\n",
        "    total_pixels = reconstructed.size\n",
        "    percentage_nonzero = (num_nonzero / total_pixels) * 100\n",
        "\n",
        "    im1 = axes[0,1].imshow(reconstructed, cmap='hot')\n",
        "    axes[0,1].set_title(f'Reconstructed Frame {frame_idx} | Non-zero: {num_nonzero} ({percentage_nonzero:.2f}%)',\n",
        "                      fontsize=14, fontweight='bold')\n",
        "    axes[0,1].axis('off')\n",
        "    fig.colorbar(im1, ax=axes[0, 1])\n",
        "\n",
        "    # Add histogram of non-zero values\n",
        "    nonzero_vals = reconstructed[reconstructed > 0]\n",
        "    axes[1,1].hist(nonzero_vals, bins=50, color='orange', edgecolor='black', alpha=0.7)\n",
        "    axes[1,1].set_title(f'Reconstructed Values Histogram', fontsize=14, fontweight='bold')\n",
        "    axes[1,1].set_xlabel('Pixel Value')\n",
        "    axes[1,1].set_ylabel('Frequency')\n",
        "    axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Normalized with gamma correction\n",
        "    gamma = 0.25  # Adjust between 0.3-0.7 for different brightening levels\n",
        "    reconstructed_normalized = reconstructed / reconstructed.max()  # Normalize to [0, 1]\n",
        "    reconstructed_gamma = np.power(reconstructed_normalized, gamma) * reconstructed.max()  # Apply gamma and scale back\n",
        "\n",
        "    im2 = axes[0,2].imshow(reconstructed_gamma, cmap='hot')\n",
        "    axes[0,2].set_title(f'Normalized (γ={gamma})', fontsize=14, fontweight='bold')\n",
        "    axes[0,2].axis('off')\n",
        "    fig.colorbar(im2, ax=axes[0,2])\n",
        "\n",
        "    # Add histogram of non-zero values for gamma-corrected\n",
        "    nonzero_vals_gamma = reconstructed_gamma[reconstructed_gamma > 0]\n",
        "    axes[1,2].hist(nonzero_vals_gamma, bins=50, color='orange', edgecolor='black', alpha=0.7)\n",
        "    axes[1,2].set_title(f'Gamma-corrected Histogram', fontsize=14, fontweight='bold')\n",
        "    axes[1,2].set_xlabel('Pixel Value')\n",
        "    axes[1,2].set_ylabel('Frequency')\n",
        "    axes[1,2].grid(True, alpha=0.3)\n",
        "\n",
        "    # Normalized\n",
        "    vmin, vmax = np.percentile(reconstructed, [0, 99.9])\n",
        "    im4 = axes[0,3].imshow(np.clip(reconstructed, vmin, vmax), cmap='hot')\n",
        "    axes[0,3].set_title(f'Normalized (1-99%)', fontsize=14, fontweight='bold')\n",
        "    axes[0,3].axis('off')\n",
        "    fig.colorbar(im4, ax=axes[0,3])\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Add histogram of non-zero values for reconstruction percentile correction\n",
        "    reconstructed_clipped = np.clip(reconstructed, vmin, vmax)\n",
        "    nonzero_vals_clipped = reconstructed_clipped[reconstructed_clipped > 0]\n",
        "\n",
        "    axes[1,3].hist(nonzero_vals_clipped, bins=50, color='orange', edgecolor='black', alpha=0.7)\n",
        "    axes[1,3].set_title(f'percentile corrected Histogram', fontsize=14, fontweight='bold')\n",
        "    axes[1,3].set_xlabel('Pixel Value')\n",
        "    axes[1,3].set_ylabel('Frequency')\n",
        "    axes[1,3].grid(True, alpha=0.3)\n",
        "\n",
        "    # NEW: Classic Min-Max Normalization\n",
        "    reconstructed_min = reconstructed.min()\n",
        "    reconstructed_max = reconstructed.max()\n",
        "    reconstructed_classic = (reconstructed - reconstructed_min) / (reconstructed_max - reconstructed_min)\n",
        "    reconstructed_classic = reconstructed_classic * 255  # Scale to 0-255 range\n",
        "\n",
        "    im4 = axes[0, 4].imshow(reconstructed_classic, cmap='hot')\n",
        "    axes[0, 4].set_title(f'Classic Min-Max Normalized', fontsize=14, fontweight='bold')\n",
        "    axes[0, 4].axis('off')\n",
        "    fig.colorbar(im4, ax=axes[0, 4])\n",
        "\n",
        "    # Add histogram of non-zero values for classic normalization\n",
        "    nonzero_vals_classic = reconstructed_classic[reconstructed_classic > 0]\n",
        "    axes[1, 4].hist(nonzero_vals_classic, bins=50, color='orange', edgecolor='black', alpha=0.7)\n",
        "    axes[1, 4].set_title(f'Classic Normalized Histogram', fontsize=14, fontweight='bold')\n",
        "    axes[1, 4].set_xlabel('Pixel Value')\n",
        "    axes[1, 4].set_ylabel('Frequency')\n",
        "    axes[1, 4].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_output:\n",
        "        output_path = os.path.join(vis_path, f'frame_{frame_idx}_comparison.png')\n",
        "        plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "        print(f\"✓ Saved to: {output_path}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\n{'=' * 70}\")\n",
        "    print(f\"Original shape: {original.shape}\")\n",
        "    print(f\"Reconstructed shape: {reconstructed.shape}\")\n",
        "    print(f\"Reconstruction range: [{reconstructed.min():.2f}, {reconstructed.max():.2f}]\")\n",
        "    print(f\"{'=' * 70}\\n\")\n",
        "\n",
        "\n",
        "def analyze_patch_statistics(result_folder: str, filename: str,\n",
        "                             frame_idx: Optional[int] = None):\n",
        "    \"\"\"\n",
        "    Analyze statistics of patches (difficulty distribution, SNR, etc.)\n",
        "\n",
        "    Args:\n",
        "        result_folder: Path to results folder\n",
        "        filename: Original filename\n",
        "        frame_idx: If provided, analyze only this frame. Otherwise, all frames.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'=' * 70}\")\n",
        "    print(f\"Analyzing patch statistics for {filename}\")\n",
        "    print(f\"{'=' * 70}\")\n",
        "\n",
        "    # Load metadata\n",
        "    metadata, _ = MetadataManager.load_metadata(result_folder, filename)\n",
        "\n",
        "    # Collect statistics\n",
        "    difficulties = []\n",
        "    snr_values = []\n",
        "    densities = []\n",
        "\n",
        "    frames_to_analyze = [frame_idx] if frame_idx is not None else metadata.keys()\n",
        "\n",
        "    for fid in frames_to_analyze:\n",
        "        if fid not in metadata:\n",
        "            continue\n",
        "\n",
        "        for patch_data in metadata[fid].values():\n",
        "            difficulties.append(patch_data['difficulty'])\n",
        "\n",
        "            snr = patch_data['signal_amp'] / (patch_data['curr_mean_noise'] + 1e-10)\n",
        "            snr_values.append(snr)\n",
        "\n",
        "            densities.append(patch_data['curr_emitter_density'])\n",
        "\n",
        "    # Create visualization\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "    # Difficulty distribution\n",
        "    unique, counts = np.unique(difficulties, return_counts=True)\n",
        "    axes[0, 0].bar(unique, counts, color='steelblue', edgecolor='black')\n",
        "    axes[0, 0].set_xlabel('Difficulty Level (Model)')\n",
        "    axes[0, 0].set_ylabel('Number of Patches')\n",
        "    axes[0, 0].set_title('Model Selection Distribution')\n",
        "    axes[0, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # SNR distribution\n",
        "    axes[0, 1].hist(snr_values, bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
        "    axes[0, 1].set_xlabel('Signal-to-Noise Ratio')\n",
        "    axes[0, 1].set_ylabel('Frequency')\n",
        "    axes[0, 1].set_title('SNR Distribution')\n",
        "    axes[0, 1].axvline(np.median(snr_values), color='red', linestyle='--',\n",
        "                       label=f'Median: {np.median(snr_values):.2f}')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Density distribution\n",
        "    axes[1, 0].hist(densities, bins=50, color='lightgreen', edgecolor='black', alpha=0.7)\n",
        "    axes[1, 0].set_xlabel('Emitter Density (per μm²)')\n",
        "    axes[1, 0].set_ylabel('Frequency')\n",
        "    axes[1, 0].set_title('Emitter Density Distribution')\n",
        "    axes[1, 0].axvline(np.median(densities), color='darkgreen', linestyle='--',\n",
        "                       label=f'Median: {np.median(densities):.2f}')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Difficulty vs SNR scatter\n",
        "    axes[1, 1].scatter(snr_values, difficulties, alpha=0.3, s=10, color='purple')\n",
        "    axes[1, 1].set_xlabel('Signal-to-Noise Ratio')\n",
        "    axes[1, 1].set_ylabel('Difficulty Level')\n",
        "    axes[1, 1].set_title('Difficulty vs SNR')\n",
        "    axes[1, 1].grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    output_path = os.path.join(result_folder, f'patch_statistics_{os.path.splitext(filename)[0]}.png')\n",
        "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"✓ Saved statistics to: {output_path}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"\\n{'=' * 70}\")\n",
        "    print(f\"Total patches analyzed: {len(difficulties)}\")\n",
        "    print(f\"\\nDifficulty distribution:\")\n",
        "    for diff, count in zip(unique, counts):\n",
        "        print(f\"  Model {diff}: {count} patches ({100 * count / len(difficulties):.1f}%)\")\n",
        "    print(f\"\\nSNR statistics:\")\n",
        "    print(f\"  Mean: {np.mean(snr_values):.2f}\")\n",
        "    print(f\"  Median: {np.median(snr_values):.2f}\")\n",
        "    print(f\"  Range: [{np.min(snr_values):.2f}, {np.max(snr_values):.2f}]\")\n",
        "    print(f\"\\nDensity statistics:\")\n",
        "    print(f\"  Mean: {np.mean(densities):.2f} per μm²\")\n",
        "    print(f\"  Median: {np.median(densities):.2f} per μm²\")\n",
        "    print(f\"  Range: [{np.min(densities):.2f}, {np.max(densities):.2f}]\")\n",
        "    print(f\"{'=' * 70}\\n\")\n",
        "\n",
        "\n",
        "def create_movie_frames(result_folder: str, filename: str,\n",
        "                        frame_range: Optional[Tuple[int, int]] = None,\n",
        "                        num_patches: int = 8, upsampling_factor: int = 8,\n",
        "                        visualization_folder: str = \"visualizations\",\n",
        "                        fps: int = 10):\n",
        "    \"\"\"\n",
        "    Create MP4 movie showing cumulative reconstruction (high quality)\n",
        "\n",
        "    Args:\n",
        "        result_folder: Path to results folder\n",
        "        filename: Original filename\n",
        "        frame_range: Tuple (start, end) or None for all frames\n",
        "        num_patches: Number of patches per dimension\n",
        "        upsampling_factor: Upsampling factor\n",
        "        visualization_folder: Subfolder name for visualizations\n",
        "        fps: Frames per second for the video\n",
        "    \"\"\"\n",
        "    # Create movie folder inside visualizations\n",
        "    vis_path = os.path.join(result_folder, visualization_folder)\n",
        "    movie_folder = os.path.join(vis_path, \"movie\")\n",
        "    os.makedirs(movie_folder, exist_ok=True)\n",
        "\n",
        "    print(f\"\\n{'=' * 70}\")\n",
        "    print(f\"Creating movie frames for {filename}\")\n",
        "    print(f\"{'=' * 70}\")\n",
        "\n",
        "    # Load metadata\n",
        "    metadata, original_frames = MetadataManager.load_metadata(result_folder, filename)\n",
        "\n",
        "    # Create reconstructor\n",
        "    reconstructor = FrameReconstructor(metadata, original_frames,\n",
        "                                       num_patches, upsampling_factor)\n",
        "\n",
        "    # Determine frame range\n",
        "    all_frame_indices = sorted(original_frames.keys())\n",
        "    if frame_range is not None:\n",
        "        start, end = frame_range\n",
        "        frame_indices = [f for f in all_frame_indices if start <= f < end]\n",
        "    else:\n",
        "        frame_indices = all_frame_indices\n",
        "\n",
        "    print(f\"Processing {len(frame_indices)} frames...\")\n",
        "\n",
        "    # Initialize cumulative reconstruction\n",
        "    cumulative_reconstruction = None\n",
        "\n",
        "    # Process each frame\n",
        "    for i, frame_idx in enumerate(frame_indices):\n",
        "        if i % 10 == 0:\n",
        "            print(f\"  Frame {i + 1}/{len(frame_indices)}\")\n",
        "\n",
        "        try:\n",
        "            original = reconstructor.get_original_frame(frame_idx)\n",
        "            frame_reconstruction = reconstructor.reconstruct_frame(frame_idx)\n",
        "\n",
        "            # Accumulate reconstruction\n",
        "            if cumulative_reconstruction is None:\n",
        "                original_reconstruction = original\n",
        "                cumulative_reconstruction = frame_reconstruction.copy()\n",
        "            else:\n",
        "                cumulative_reconstruction += frame_reconstruction\n",
        "                original_reconstruction += original\n",
        "\n",
        "            # Create side-by-side image with cumulative reconstruction\n",
        "            fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "            axes[0].imshow(original_reconstruction, cmap='gray')\n",
        "            axes[0].set_title(f'Original - Frame {frame_idx}', fontsize=14)\n",
        "            axes[0].axis('off')\n",
        "\n",
        "            # Show cumulative reconstruction (normalized)\n",
        "            vmin, vmax = np.percentile(cumulative_reconstruction, [1, 99])\n",
        "            axes[1].imshow(np.clip(cumulative_reconstruction, vmin, vmax), cmap='hot')\n",
        "            axes[1].set_title(f'Cumulative Reconstruction - Frame {frame_idx} ({i + 1}/{len(frame_indices)})',\n",
        "                              fontsize=14)\n",
        "            axes[1].axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            output_path = os.path.join(movie_folder, f'frame_{i:05d}.png')\n",
        "            plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Warning: Failed to process frame {frame_idx}: {e}\")\n",
        "\n",
        "    print(f\"\\n✓ Saved {len(frame_indices)} frames to: {movie_folder}\")\n",
        "\n",
        "    # Create MP4 using ffmpeg\n",
        "    print(f\"\\nCreating high-quality MP4 video...\")\n",
        "    base_name = os.path.splitext(filename)[0]\n",
        "    output_video = os.path.join(vis_path, f\"{base_name}_reconstruction_movie.mp4\")\n",
        "\n",
        "    # High-quality ffmpeg command\n",
        "    ffmpeg_cmd = [\n",
        "        'ffmpeg',\n",
        "        '-y',  # Overwrite output file\n",
        "        '-framerate', str(fps),\n",
        "        '-i', os.path.join(movie_folder, 'frame_%05d.png'),\n",
        "        '-c:v', 'libx264',\n",
        "        '-preset', 'slow',  # Better compression\n",
        "        '-crf', '18',  # High quality (18 is visually lossless)\n",
        "        '-pix_fmt', 'yuv420p',\n",
        "        '-vf', 'scale=trunc(iw/2)*2:trunc(ih/2)*2',  # Ensure even dimensions\n",
        "        output_video\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        subprocess.run(ffmpeg_cmd, check=True, capture_output=True)\n",
        "        print(f\"✓ Video created successfully: {output_video}\")\n",
        "        print(f\"\\nVideo details:\")\n",
        "        print(f\"  - Quality: High (CRF 18, visually lossless)\")\n",
        "        print(f\"  - Framerate: {fps} fps\")\n",
        "        print(f\"  - Codec: H.264\")\n",
        "        print(f\"  - Total frames: {len(frame_indices)}\")\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"✗ Error creating video: {e}\")\n",
        "        print(f\"  stderr: {e.stderr.decode()}\")\n",
        "        print(f\"\\nYou can manually create the video with:\")\n",
        "        print(\n",
        "            f\"  ffmpeg -framerate {fps} -i {movie_folder}/frame_%05d.png -c:v libx264 -crf 18 -pix_fmt yuv420p {output_video}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"✗ ffmpeg not found. Please install ffmpeg to create videos.\")\n",
        "        print(f\"  Frames saved in: {movie_folder}\")\n",
        "        print(f\"\\nAfter installing ffmpeg, run:\")\n",
        "        print(\n",
        "            f\"  ffmpeg -framerate {fps} -i {movie_folder}/frame_%05d.png -c:v libx264 -crf 18 -pix_fmt yuv420p {output_video}\")\n",
        "\n",
        "    print(f\"{'=' * 70}\\n\")\n",
        "\n",
        "\n",
        "def compare_multiple_frames(result_folder: str, filename: str,\n",
        "                            frame_indices: List[int],\n",
        "                            num_patches: int = 8, upsampling_factor: int = 8,\n",
        "                            visualization_folder: str = \"visualizations\"):\n",
        "    \"\"\"\n",
        "    Create a grid comparison of multiple frames\n",
        "\n",
        "    Args:\n",
        "        result_folder: Path to results folder\n",
        "        filename: Original filename\n",
        "        frame_indices: List of frame indices to compare\n",
        "        num_patches: Number of patches per dimension\n",
        "        upsampling_factor: Upsampling factor\n",
        "        visualization_folder: Subfolder name for visualizations\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'=' * 70}\")\n",
        "    print(f\"Comparing {len(frame_indices)} frames\")\n",
        "    print(f\"{'=' * 70}\")\n",
        "\n",
        "    # Create visualization folder\n",
        "    vis_path = os.path.join(result_folder, visualization_folder)\n",
        "    os.makedirs(vis_path, exist_ok=True)\n",
        "\n",
        "    # Load metadata\n",
        "    metadata, original_frames = MetadataManager.load_metadata(result_folder, filename)\n",
        "\n",
        "    # Create reconstructor\n",
        "    reconstructor = FrameReconstructor(metadata, original_frames,\n",
        "                                       num_patches, upsampling_factor)\n",
        "\n",
        "    # Create grid\n",
        "    n_frames = len(frame_indices)\n",
        "    fig, axes = plt.subplots(2, n_frames, figsize=(5 * n_frames, 10))\n",
        "\n",
        "    if n_frames == 1:\n",
        "        axes = axes.reshape(-1, 1)\n",
        "\n",
        "    for i, frame_idx in enumerate(frame_indices):\n",
        "        original = reconstructor.get_original_frame(frame_idx)\n",
        "        reconstructed = reconstructor.reconstruct_frame(frame_idx)\n",
        "\n",
        "        # Original\n",
        "        axes[0, i].imshow(original, cmap='gray')\n",
        "        axes[0, i].set_title(f'Original {frame_idx}', fontsize=12)\n",
        "        axes[0, i].axis('off')\n",
        "\n",
        "        # Reconstructed\n",
        "        cap = np.percentile(reconstructed, 99.5)\n",
        "        reconstructed[reconstructed > cap] = cap\n",
        "\n",
        "        #vmin, vmax = np.percentile(reconstructed, [1, 99])\n",
        "        #axes[1, i].imshow(np.clip(reconstructed, vmin, vmax), cmap='hot')\n",
        "        axes[1, i].imshow(reconstructed, cmap='hot', vmin=0, vmax=cap)\n",
        "        axes[1, i].set_title(f'Reconstructed {frame_idx}', fontsize=12)\n",
        "        axes[1, i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    output_path = os.path.join(vis_path, f'comparison_grid.png')\n",
        "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"✓ Saved to: {output_path}\")\n",
        "\n",
        "    plt.show()\n",
        "    print(f\"{'=' * 70}\\n\")\n",
        "\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "# ============================================================================\n",
        "# Configuration\n",
        "# ============================================================================\n",
        "class Config:\n",
        "    \"\"\"Configuration for AutoDS inference\"\"\"\n",
        "    # Data paths\n",
        "    Result_folder = \"../Results/TOM20_10nM/v26/fp16\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "    # --- Quiet/Preview flags ------------------------------------------------------\n",
        "    QUIET = False  # no training/inference chatter unless set to False\n",
        "    HEADLESS_PREVIEW = True  # set True if you want to see the preview figures\n",
        "\n",
        "    # --- Metadata collection flag -------------------------------------------------\n",
        "    use_patch_metadata = False  # Set to False to disable metadata collection\n",
        "\n",
        "    PRECISION_MODE = 'fp16'  # Options: 'float32', 'fp16', 'int8'\n",
        "\n",
        "    # Detection parameters\n",
        "    threshold = 10 #@param {type:\"number\"}\n",
        "    neighborhood_size = 3 #@param {type:\"integer\"}\n",
        "    use_local_average = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "    # Patch parameters\n",
        "    num_patches = 8 #@param {type:\"number\"}\n",
        "    overlap = 4 #@param {type:\"number\"}\n",
        "    patch_batch_size = 32 #@param {type:\"number\"}\n",
        "    frame_batch_size = 10 #@param {type:\"number\"}\n",
        "\n",
        "    # Imaging parameters\n",
        "    interpolate_based_on_imaging_parameters = True #@param {type:\"boolean\"}\n",
        "    get_pixel_size_from_file = False #@param {type:\"boolean\"}\n",
        "    pixel_size = 233 #@param {type:\"number\"}\n",
        "    wavelength = 233 #@param {type:\"number\"}\n",
        "    numerical_aperture = 1.49 #@param {type:\"number\"}\n",
        "\n",
        "\n",
        "    # Processing parameters\n",
        "    chunk_size = 10000 - 16\n",
        "\n",
        "    # Timing parameters\n",
        "    enable_timing = True  # Set to True to enable detailed timing profiling #@param {type:\"boolean\"}\n",
        "\n",
        "    # Model parameters\n",
        "    use_pytorch_weights = False  # Set to True to use .pth weights, False to use .h5 weights #@param {type:\"boolean\"}\n",
        "\n",
        "    # Model paths\n",
        "    prediction_model_path = \"/content/AutoDS_models\"\n",
        "    model_names = ['diff_1', 'diff_2', 'diff_3', 'diff_4']\n",
        "\n",
        "    # Model manifest for downloading\n",
        "    MODEL_MANIFEST = {\n",
        "        \"diff_1\": {\n",
        "            \"file_urls\": {\n",
        "                # \"best_weights.pth\": \"https://your-url/diff_1/best_weights.pth\",\n",
        "                \"best_weights.h5\": \"https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/raw/main/AutoDS/models/diff_1/best_weights.h5\",\n",
        "                \"model_metadata.mat\": \"https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/raw/main/AutoDS/models/diff_1/model_metadata.mat\",\n",
        "            },\n",
        "            \"contains\": [\"model_metadata.mat\"]\n",
        "        },\n",
        "        \"diff_2\": {\n",
        "            \"file_urls\": {\n",
        "                # \"best_weights.pth\": \"https://your-url/diff_2/best_weights.pth\",\n",
        "                \"best_weights.h5\": \"https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/raw/main/AutoDS/models/diff_2/best_weights.h5\",\n",
        "                \"model_metadata.mat\": \"https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/raw/main/AutoDS/models/diff_2/model_metadata.mat\",\n",
        "            },\n",
        "            \"contains\": [\"model_metadata.mat\"]\n",
        "        },\n",
        "        \"diff_3\": {\n",
        "            \"file_urls\": {\n",
        "                # \"best_weights.pth\": \"https://your-url/diff_3/best_weights.pth\",\n",
        "                \"best_weights.h5\": \"https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/raw/main/AutoDS/models/diff_3/best_weights.h5\",\n",
        "                \"model_metadata.mat\": \"https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/raw/main/AutoDS/models/diff_3/model_metadata.mat\",\n",
        "            },\n",
        "            \"contains\": [\"model_metadata.mat\"]\n",
        "        },\n",
        "        \"diff_4\": {\n",
        "            \"file_urls\": {\n",
        "                # \"best_weights.pth\": \"https://your-url/diff_4/best_weights.pth\",\n",
        "                \"best_weights.h5\": \"https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/raw/main/AutoDS/models/diff_4/best_weights.h5\",\n",
        "                \"model_metadata.mat\": \"https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/raw/main/AutoDS/models/diff_4/model_metadata.mat\",\n",
        "            },\n",
        "            \"contains\": [\"model_metadata.mat\"]\n",
        "        },\n",
        "    }\n",
        "\n",
        "\n",
        "# Setup\n",
        "config = Config()\n",
        "\n",
        "# ============================================================================\n",
        "# Entry Point\n",
        "# ============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    if device.type != 'cuda':\n",
        "        log('You do not have GPU access.')\n",
        "        log('Did you change your runtime?')\n",
        "        log('If the runtime settings are correct then GPU might not be allocated to your session.')\n",
        "        log('Expect slow performance. To access GPU try reconnecting later.')\n",
        "    else:\n",
        "        log('You have GPU access')\n",
        "        #if config.PRECISION_MODE != \"float32\":\n",
        "        #    log(f'Precision mode: {config.PRECISION_MODE}')\n",
        "        #if config.use_patch_metadata:\n",
        "        #    log(f'Metadata collection: Enabled')\n",
        "\n",
        "    # Initialize timing profiler\n",
        "    profiler = timing_profiler(enabled=config.enable_timing)\n",
        "\n",
        "    config.prediction_model_path = ensure_models(config.model_names, target_root=config.prediction_model_path,\n",
        "                                                 model_manifest=config.MODEL_MANIFEST)\n",
        "\n",
        "    MAX_FILE_GB = 5.0  # warn & skip when file is larger than this\n",
        "\n",
        "    # PSF parameters\n",
        "    psf_sigma_nm = 0.21 * config.wavelength / config.numerical_aperture\n",
        "    psf_sigma_pixels = psf_sigma_nm / config.pixel_size\n",
        "\n",
        "    if config.get_pixel_size_from_file:\n",
        "        pixel_size = None\n",
        "\n",
        "    # Load model metadata\n",
        "    matfile = sio.loadmat(os.path.join(config.prediction_model_path, config.model_names[0], 'model_metadata.mat'))\n",
        "    try:\n",
        "        model_wavelength = np.array(matfile['wavelength'].item())\n",
        "    except:\n",
        "        model_wavelength = None\n",
        "    try:\n",
        "        model_NA = np.array(matfile['numerical_aperture'].item())\n",
        "    except:\n",
        "        model_NA = None\n",
        "    try:\n",
        "        model_pixel_size = np.array(matfile['pixel_size'].item())\n",
        "    except:\n",
        "        model_pixel_size = None\n",
        "\n",
        "    if os.path.isdir(Data_folder):\n",
        "        # iterate both TIFF and ND2\n",
        "        for filename in list_files_multi(Data_folder, extensions=['tif', 'tiff', 'nd2']):\n",
        "            print(f\"\\nStart processing file: {filename}\")\n",
        "\n",
        "            # Install nd2 reader only when needed (optional)\n",
        "            if filename.lower().endswith('.nd2'):\n",
        "                try:\n",
        "                    import nd2\n",
        "                except Exception:\n",
        "                    import subprocess\n",
        "                    import sys\n",
        "\n",
        "                    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'nd2'])\n",
        "                    import nd2\n",
        "\n",
        "            in_path = os.path.join(config.Data_folder, filename)\n",
        "\n",
        "            # --------- file size guard ----------\n",
        "            try:\n",
        "                file_size_gb = os.path.getsize(in_path) / 1e9\n",
        "                if file_size_gb > MAX_FILE_GB:\n",
        "                    print(f\"\\n⚠️  {filename}: {file_size_gb:.2f} GB > {MAX_FILE_GB:.2f} GB.\")\n",
        "                    print(\"   Video size is too big, please use Google Colab Pro or run locally.\")\n",
        "                    continue\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            # --- Resolve pixel size if requested ---\n",
        "            if config.get_pixel_size_from_file:\n",
        "                if is_tiff(in_path):\n",
        "                    with catch_oom(\"reading TIFF pixel size\", filename):\n",
        "                        pixel_size, _, _ = getPixelSizeTIFFmetadata(in_path, True)\n",
        "                elif is_nd2(in_path):\n",
        "                    with catch_oom(\"reading ND2 pixel size\", filename):\n",
        "                        px_nm, _, _ = getPixelSizeND2metadata(in_path, True)\n",
        "                        pixel_size = px_nm if px_nm is not None else pixel_size\n",
        "\n",
        "            # --- Common model params ---\n",
        "            upsampling_factor = np.array(matfile['upsampling_factor']).item()\n",
        "            try:\n",
        "                L2_weighting_factor = np.array(matfile['Normalization factor']).item()\n",
        "            except:\n",
        "                L2_weighting_factor = 100\n",
        "\n",
        "            # save all models to cache\n",
        "            initialize_model_cache(config, upsampling_factor, device,\n",
        "                                   use_pytorch_weights=config.use_pytorch_weights,\n",
        "                                   precision_mode=config.PRECISION_MODE)\n",
        "\n",
        "            # --- Choose reader & frame count ---\n",
        "            number_of_frames, frame_iter = None, None\n",
        "            with catch_oom(\"opening stack\", filename):\n",
        "                if is_tiff(in_path):\n",
        "                    number_of_frames = count_tiff_frames(in_path)\n",
        "                    frame_iter = iter_tiff_frames(in_path)\n",
        "                    log(f'\\nLoaded tiff stack with {number_of_frames} frames')\n",
        "                elif is_nd2(in_path):\n",
        "                    number_of_frames = count_nd2_frames(in_path)\n",
        "                    frame_iter = iter_nd2_frames(in_path)\n",
        "                    log(f'\\nLoaded ND2 stack with ~{number_of_frames} planes (T*Z*C)')\n",
        "                else:\n",
        "                    log(f\"Skipping unsupported file: {filename}\")\n",
        "\n",
        "            if frame_iter is None:\n",
        "                print(f\"⚠️  Skipping {filename} due to earlier error.\")\n",
        "                continue\n",
        "\n",
        "            # Initialize patch lists for each model (like v18)\n",
        "            patches_list = [[] for _ in config.model_names]\n",
        "            patch_indices_list = [[] for _ in config.model_names]\n",
        "            frame_numbers = [[] for _ in config.model_names]\n",
        "\n",
        "            # Initialize patch manager (only if metadata is enabled)\n",
        "            if config.use_patch_metadata:\n",
        "                metadata_manager = MetadataManager(config.Result_folder, filename)\n",
        "\n",
        "            # Initialize accumulator variables\n",
        "            M, N = None, None\n",
        "            sum_image = None\n",
        "            patchwise_recon = None\n",
        "            frame_number_list, x_nm_list, y_nm_list, confidence_au_list = [], [], [], []\n",
        "            total_selected_model_hist = np.zeros(len(config.model_names), dtype=int)\n",
        "\n",
        "            # Progress bar for overall process\n",
        "            pbar = tqdm(total=number_of_frames, desc=\"Processing frames\")\n",
        "\n",
        "            for frame_start in range(0, number_of_frames, config.frame_batch_size):\n",
        "                frame_end = min(frame_start + config.frame_batch_size, number_of_frames)\n",
        "                frame_batch_size = frame_end - frame_start\n",
        "\n",
        "                # Collect patches from multiple frames\n",
        "                all_valid_patches = []\n",
        "                all_full_patches = []\n",
        "                all_patches_local_indices = []\n",
        "                all_frames_numbers = []\n",
        "                all_patches_offset = []\n",
        "\n",
        "                profiler.start_timer(\"frame reading and splitting\")\n",
        "\n",
        "                # Collect frames\n",
        "                frames_list = []\n",
        "                for frame_idx in range(frame_start, frame_end):\n",
        "                    frame_i = next(frame_iter)\n",
        "\n",
        "                    # Store original frame for future visualization\n",
        "                    if config.use_patch_metadata:\n",
        "                        metadata_manager.add_original_frame(frame_idx, frame_i.copy())\n",
        "\n",
        "                    # Initialize sum_image and dimensions on first frame\n",
        "                    if sum_image is None:\n",
        "                        sum_image = np.zeros_like(frame_i, dtype=np.float32)\n",
        "\n",
        "                        # Interpolate first to get actual dimensions\n",
        "                        if config.interpolate_based_on_imaging_parameters:\n",
        "                            temp_frame = interpolate_frames(\n",
        "                                frame_i,\n",
        "                                model_pixel_size, config.pixel_size,\n",
        "                                model_wavelength, config.wavelength,\n",
        "                                model_NA, config.numerical_aperture\n",
        "                            )[0]\n",
        "                            M, N = temp_frame.shape\n",
        "                        else:\n",
        "                            M, N = frame_i.shape\n",
        "\n",
        "                    # Accumulate for preview\n",
        "                    sum_image += frame_i.astype(np.float32) / number_of_frames\n",
        "\n",
        "                    # Interpolate frame\n",
        "                    if config.interpolate_based_on_imaging_parameters:\n",
        "                        frame_i = interpolate_frames(\n",
        "                            frame_i,\n",
        "                            model_pixel_size, config.pixel_size,\n",
        "                            model_wavelength, config.wavelength,\n",
        "                            model_NA, config.numerical_aperture\n",
        "                        )[0]\n",
        "                    frames_list.append(frame_i)\n",
        "\n",
        "                # Preprocess on GPU\n",
        "                frames_torch = torch.from_numpy(np.array(frames_list)).float().to(device)\n",
        "                fproc_tensor, frames_offsets = preprocess_frames_batch(frames_torch, device)\n",
        "\n",
        "                # Split all frames to patches (GPU)\n",
        "                all_patches_tensor = split_image_to_patches_batch(\n",
        "                    fproc_tensor,\n",
        "                    config.num_patches,\n",
        "                    config.overlap,\n",
        "                    device=device\n",
        "                )\n",
        "\n",
        "                for frame_idx in range(frame_start, frame_end):\n",
        "                    offset = frames_offsets[frame_idx - frame_start].cpu().item()\n",
        "                    patches = all_patches_tensor[frame_idx - frame_start]\n",
        "\n",
        "                    # Process each patch\n",
        "                    for m in range(config.num_patches):\n",
        "                        for n in range(config.num_patches):\n",
        "                            down = config.overlap if m == 0 else 0\n",
        "                            up = (M // config.num_patches) - config.overlap if m == config.num_patches - 1 else (\n",
        "                                    M // config.num_patches)\n",
        "                            left = config.overlap if n == 0 else 0\n",
        "                            right = (N // config.num_patches) - config.overlap if n == config.num_patches - 1 else (\n",
        "                                    N // config.num_patches)\n",
        "\n",
        "                            local_patch_idx = m * config.num_patches + n\n",
        "                            full_patch = patches[local_patch_idx]\n",
        "                            valid_patch = full_patch[down:up, left:right]\n",
        "\n",
        "                            all_full_patches.append(full_patch)\n",
        "                            all_valid_patches.append(valid_patch)\n",
        "                            all_patches_local_indices.append(local_patch_idx)\n",
        "                            all_patches_offset.append(offset)\n",
        "                            all_frames_numbers.append(frame_idx)\n",
        "\n",
        "                profiler.stop_timer(\"frame reading and splitting\")\n",
        "\n",
        "                profiler.start_timer(\"patch features extraction\")\n",
        "                # Group patches by size and extract features\n",
        "                shape_groups = defaultdict(lambda: {'patches': [], 'indices': [], 'offsets': []})\n",
        "\n",
        "                for idx, patch in enumerate(all_valid_patches):\n",
        "                    shape = patch.shape\n",
        "                    shape_groups[shape]['patches'].append(patch)\n",
        "                    shape_groups[shape]['indices'].append(idx)\n",
        "                    shape_groups[shape]['offsets'].append(all_patches_offset[idx])\n",
        "\n",
        "                # Process each size group\n",
        "                all_features = []\n",
        "\n",
        "                for shape, group_data in shape_groups.items():\n",
        "                    patches_tensor = torch.stack(group_data['patches'])\n",
        "                    offsets_array = np.array(group_data['offsets'])\n",
        "\n",
        "                    features_batch = extract_features_batch(\n",
        "                        patches_tensor,\n",
        "                        config.pixel_size,\n",
        "                        psf_sigma_pixels,\n",
        "                        offsets_array,\n",
        "                        verbose=False,\n",
        "                        device=device\n",
        "                    )\n",
        "\n",
        "                    for feat, idx in zip(features_batch, group_data['indices']):\n",
        "                        all_features.append((feat, idx))\n",
        "\n",
        "                profiler.stop_timer(\"patch features extraction\")\n",
        "                profiler.start_timer(\"patch classification\")\n",
        "\n",
        "                # Classify and accumulate patches for reconstruction\n",
        "                for features, idx in all_features:\n",
        "                    curr_mean_noise, curr_std_noise, signal_amp, curr_emitter_density = features\n",
        "\n",
        "                    # Skip invalid patches\n",
        "                    if signal_amp == 0 or curr_mean_noise == 0:\n",
        "                        continue\n",
        "                    if any(np.isnan(v) for v in (signal_amp, curr_mean_noise, curr_std_noise, curr_emitter_density)):\n",
        "                        continue\n",
        "\n",
        "                    # Choose difficulty level\n",
        "                    difficulty_choice = ChooseNetByDifficulty_2025(\n",
        "                        curr_emitter_density,\n",
        "                        signal_amp / curr_mean_noise\n",
        "                    )\n",
        "                    total_selected_model_hist[difficulty_choice] += 1\n",
        "\n",
        "                    # Store patch data\n",
        "                    patches_list[difficulty_choice].append(all_full_patches[idx])\n",
        "                    patch_indices_list[difficulty_choice].append(all_patches_local_indices[idx])\n",
        "                    frame_numbers[difficulty_choice].append(all_frames_numbers[idx])\n",
        "\n",
        "                    # Add to patch manager (if metadata enabled)\n",
        "                    if config.use_patch_metadata:\n",
        "                        # Store metadata for future analysis\n",
        "                        metadata_manager.add_patch_metadata(\n",
        "                            frame_idx=all_frames_numbers[idx],\n",
        "                            patch_idx=all_patches_local_indices[idx],\n",
        "                            metadata={\n",
        "                                'valid_patch': all_valid_patches[idx].cpu().numpy(),  # Move to CPU to save GPU memory\n",
        "                                'curr_mean_noise': curr_mean_noise,\n",
        "                                'curr_std_noise': curr_std_noise,\n",
        "                                'signal_amp': signal_amp,\n",
        "                                'curr_emitter_density': curr_emitter_density,\n",
        "                                'difficulty': difficulty_choice,\n",
        "                                'predicted_patch': None  # Will be filled after reconstruction\n",
        "                            }\n",
        "                        )\n",
        "\n",
        "                profiler.stop_timer(\"patch classification\")\n",
        "\n",
        "                # Initialize reconstruction array on first batch and move it to the GPU\n",
        "                if patchwise_recon is None:\n",
        "                    M, N = fproc_tensor.shape[1], fproc_tensor.shape[2]\n",
        "                    patchwise_recon = torch.zeros(M * upsampling_factor, N * upsampling_factor,\n",
        "                                                  dtype=torch.float32, device=device)\n",
        "\n",
        "                # Process with each model\n",
        "                for model_num, model_name in enumerate(config.model_names):\n",
        "                    if not patches_list[model_num]:\n",
        "                        continue\n",
        "\n",
        "                    # Reconstruct using CACHED model\n",
        "                    pw_recon, loc_list, predicted_patches = reconstruct_patches_2025_pytorch(\n",
        "                        torch.stack(patches_list[model_num]),\n",
        "                        patch_indices_list[model_num],\n",
        "                        frame_numbers[model_num],\n",
        "                        model_num,\n",
        "                        config.num_patches,\n",
        "                        config.overlap * upsampling_factor,\n",
        "                        number_of_frames,\n",
        "                        config.threshold,\n",
        "                        neighborhood_size=config.neighborhood_size,\n",
        "                        use_local_avg=config.use_local_average,\n",
        "                        upsampling_factor=upsampling_factor,\n",
        "                        pixel_size=config.pixel_size,\n",
        "                        batch_size=config.patch_batch_size,\n",
        "                        L2_weighting_factor=L2_weighting_factor,\n",
        "                        profiler=profiler,\n",
        "                        precision_mode=config.PRECISION_MODE,\n",
        "                        use_metadata=config.use_patch_metadata\n",
        "                    )\n",
        "\n",
        "                    if config.use_patch_metadata:\n",
        "                        # Store predicted patches back into patch_manager\n",
        "                        for i, (frame_idx, patch_idx) in enumerate(zip(frame_numbers[model_num], patch_indices_list[model_num])):\n",
        "                            predicted_patch = predicted_patches[i]\n",
        "                            # Update metadata manager with prediction\n",
        "                            metadata_dict = metadata_manager._metadata[frame_idx][patch_idx]\n",
        "                            metadata_dict['predicted_patch'] = predicted_patch\n",
        "\n",
        "                    # Accumulate results\n",
        "                    frame_number_list.extend(loc_list[0])\n",
        "                    x_nm_list.extend(loc_list[1])\n",
        "                    y_nm_list.extend(loc_list[2])\n",
        "                    confidence_au_list.extend(loc_list[3])\n",
        "\n",
        "                    patchwise_recon[:M // config.num_patches * upsampling_factor * config.num_patches,\n",
        "                                    :N // config.num_patches * upsampling_factor * config.num_patches] += pw_recon\n",
        "\n",
        "                # Clear patches lists after processing to free memory (like v18)\n",
        "                for i in range(len(patches_list)):\n",
        "                    patches_list[i].clear()\n",
        "                    patch_indices_list[i].clear()\n",
        "                    frame_numbers[i].clear()\n",
        "\n",
        "                # Update process bar\n",
        "                pbar.update(frame_batch_size)\n",
        "\n",
        "            # close progress bar\n",
        "            pbar.close()\n",
        "\n",
        "            # Create output folder if needed\n",
        "            if not os.path.exists(config.Result_folder):\n",
        "                print('Result folder was created.')\n",
        "                os.makedirs(config.Result_folder, exist_ok=True)\n",
        "\n",
        "            # Save results\n",
        "            os.makedirs(config.Result_folder, exist_ok=True)\n",
        "            ext = '_avg' if config.use_local_average else '_max'\n",
        "            base = os.path.splitext(filename)[0]\n",
        "\n",
        "            # Histogram of overall patches distribution by models\n",
        "            selected_model_hist = total_selected_model_hist\n",
        "\n",
        "            # Histogram 1: Overall patches distribution by models\n",
        "            print(f\"selected_model_hist = {total_selected_model_hist}\")\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.bar(np.arange(len(config.model_names)), selected_model_hist, width=0.8)\n",
        "            plt.xticks(np.arange(len(config.model_names)), config.model_names)\n",
        "            plt.xlabel('Selected Model')\n",
        "            plt.ylabel('Number of Patches')\n",
        "            plt.title('Model Selection Distribution')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(config.Result_folder, f'model_selection_{os.path.splitext(filename)[0]}.png'))\n",
        "            plt.close()\n",
        "\n",
        "            #print(f\"\\n{'=' * 70}\")\n",
        "            #print(f\"Total localizations found: {len(frame_number_list)}\")\n",
        "            #print(f\"{'=' * 70}\")\n",
        "\n",
        "            # Save localizations\n",
        "            with open(os.path.join(config.Result_folder, f'Localizations_{base}{ext}.csv'), \"w\", newline='') as file:\n",
        "                writer = csv.writer(file)\n",
        "                writer.writerow(['frame', 'x [nm]', 'y [nm]', 'confidence [a.u]'])\n",
        "                sort_ind = np.argsort(frame_number_list)\n",
        "                locs = list(zip(\n",
        "                    list(np.array(frame_number_list)[sort_ind]),\n",
        "                    list(np.array(x_nm_list)[sort_ind]),\n",
        "                    list(np.array(y_nm_list)[sort_ind]),\n",
        "                    list(np.array(confidence_au_list)[sort_ind])\n",
        "                ))\n",
        "                writer.writerows(locs)\n",
        "\n",
        "            #print(f\"Saved {len(frame_number_list)} localizations\")\n",
        "\n",
        "            # move the reconstructed image to CPU for saving\n",
        "            patchwise_recon = patchwise_recon.cpu().numpy()\n",
        "\n",
        "            # Save reconstruction\n",
        "            pw_recon_tif = np.copy(patchwise_recon)\n",
        "            cap = np.percentile(pw_recon_tif, 99.5)\n",
        "            pw_recon_tif[pw_recon_tif > cap] = cap\n",
        "            saveAsTIF(config.Result_folder, f\"Predicted_patchwise_{base}\", pw_recon_tif,\n",
        "                      config.pixel_size / upsampling_factor)\n",
        "\n",
        "            # Create preview\n",
        "            fig, axes = plt.subplots(1, 3, figsize=(20, 16))\n",
        "            axes[0].axis('off')\n",
        "            axes[0].imshow(sum_image)\n",
        "            axes[0].set_title('Original', fontsize=15)\n",
        "            axes[1].axis('off')\n",
        "            axes[1].imshow(patchwise_recon)\n",
        "            axes[1].set_title('Prediction', fontsize=15)\n",
        "            axes[2].axis('off')\n",
        "            axes[2].imshow(np.clip(patchwise_recon,\n",
        "                                   np.percentile(patchwise_recon, 1),\n",
        "                                   np.percentile(patchwise_recon, 99)))\n",
        "            axes[2].set_title('Normalized Prediction', fontsize=15)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(config.Result_folder, f'preview_{base}.png'), dpi=150)\n",
        "            plt.close()\n",
        "\n",
        "            if config.enable_timing:\n",
        "                # Print timing summary\n",
        "                profiler.print_timing_summary()\n",
        "                # reset timing for next file\n",
        "                profiler.reset()\n",
        "\n",
        "            if config.use_patch_metadata:\n",
        "                # Save all metadata to disk (asynchronously, won't block)\n",
        "                metadata_manager.save_all_metadata(wait_for_completion=True)\n",
        "                metadata_manager.clear_memory()\n",
        "\n",
        "                # Start async saver thread (won't slow down processing)\n",
        "                metadata_manager.save_to_disk_async()\n",
        "\n",
        "            print(f\"\\nCompleted processing file: {filename}\")\n",
        "\n",
        "\n",
        "\n",
        "        # METADATA: At the very end, wait for all async saves to complete\n",
        "        if config.use_patch_metadata:\n",
        "            metadata_manager.finalize()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **V2: PyTorch Version**\n",
        "1. full pyTorch compatibility\n",
        "2. frame-wize preprocessing (instead of model-wize)"
      ],
      "metadata": {
        "id": "5UW6KRIzcVz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import urllib.request\n",
        "from contextlib import contextmanager\n",
        "\n",
        "import numpy as np\n",
        "import tifffile as tiff\n",
        "from PIL import Image\n",
        "from PIL.TiffTags import TAGS\n",
        "\n",
        "def log(*args, **kwargs):\n",
        "    if not config.QUIET:\n",
        "        print(*args, **kwargs)\n",
        "\n",
        "def list_files_multi(directory, extensions):\n",
        "    exts = {('.' + e.lower()) for e in extensions}\n",
        "    for f in os.listdir(directory):\n",
        "        if os.path.splitext(f)[1].lower() in exts:\n",
        "            yield f\n",
        "\n",
        "@contextmanager\n",
        "def catch_oom(phase: str, detail: str = \"\", on_oom=\"continue\"):\n",
        "    \"\"\"\n",
        "    Wrap any memory-heavy block. Prints a friendly message on OOM and continues.\n",
        "    on_oom: \"continue\" (default) just prints and returns; any other value re-raises.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        yield\n",
        "    except Exception as e:\n",
        "        if _is_oom(e):\n",
        "            print(f\"\\n⚠️  OOM while {phase}{(' - ' + detail) if detail else ''}.\")\n",
        "            print(\"   Tip: reduce chunk_size/batch_size/upsampling, or downsample input.\")\n",
        "            if isinstance(e, torch.cuda.OutOfMemoryError):\n",
        "                # PyTorch OOM messages are in str(e) directly\n",
        "                msg_line = str(e).splitlines()[0][:200]\n",
        "                print(\"   PyTorch says:\", msg_line)\n",
        "            else:\n",
        "                traceback.print_exc(limit=1, file=sys.stdout)\n",
        "            if on_oom != \"continue\":\n",
        "                raise\n",
        "        else:\n",
        "            # Non-OOM: re-raise so real bugs are visible\n",
        "            raise\n",
        "\n",
        "# ============================================================================\n",
        "# 1. TIFF File Operations\n",
        "# ============================================================================\n",
        "\n",
        "def getPixelSizeTIFFmetadata(TIFFpath, display=False):\n",
        "    \"\"\"Extract pixel size from TIFF metadata\"\"\"\n",
        "    with Image.open(TIFFpath) as img:\n",
        "        meta_dict = {TAGS[key]: img.tag[key] for key in img.tag.keys()}\n",
        "\n",
        "    ResolutionUnit = meta_dict['ResolutionUnit'][0]\n",
        "    width = meta_dict['ImageWidth'][0]\n",
        "    height = meta_dict['ImageLength'][0]\n",
        "    xResolution = meta_dict['XResolution'][0]\n",
        "\n",
        "    if len(xResolution) == 1:\n",
        "        xResolution = xResolution[0]\n",
        "    elif len(xResolution) == 2:\n",
        "        xResolution = xResolution[0] / xResolution[1]\n",
        "    else:\n",
        "        print('Image resolution not defined.')\n",
        "        xResolution = 1\n",
        "\n",
        "    if ResolutionUnit == 2:\n",
        "        pixel_size = 0.025 * 1e9 / xResolution\n",
        "    elif ResolutionUnit == 3:\n",
        "        pixel_size = 0.01 * 1e9 / xResolution\n",
        "    else:\n",
        "        print('Resolution unit not defined. Assuming: um')\n",
        "        pixel_size = 1e3 / xResolution\n",
        "\n",
        "    if display:\n",
        "        print(f'Pixel size from metadata: {pixel_size} nm')\n",
        "        print(f'Image size: {width}x{height}')\n",
        "\n",
        "    return pixel_size, width, height\n",
        "\n",
        "def saveAsTIF(path, filename, array, pixel_size):\n",
        "    \"\"\"Save array as TIFF with metadata\"\"\"\n",
        "    if array.dtype == np.uint16:\n",
        "        mode = 'I;16'\n",
        "    elif array.dtype == np.uint32:\n",
        "        mode = 'I'\n",
        "    else:\n",
        "        mode = 'F'\n",
        "\n",
        "    if len(array.shape) == 2:\n",
        "        im = Image.fromarray(array)\n",
        "        im.save(os.path.join(path, filename + '.tif'),\n",
        "               mode=mode,\n",
        "               resolution_unit=3,\n",
        "               resolution=0.01 * 1e9 / pixel_size)\n",
        "    elif len(array.shape) == 3:\n",
        "        imlist = []\n",
        "        for frame in array:\n",
        "            imlist.append(Image.fromarray(frame))\n",
        "        imlist[0].save(os.path.join(path, filename + '.tif'),\n",
        "                      save_all=True,\n",
        "                      append_images=imlist[1:],\n",
        "                      mode=mode,\n",
        "                      resolution_unit=3,\n",
        "                      resolution=0.01 * 1e9 / pixel_size)\n",
        "\n",
        "def is_tiff(path):\n",
        "    \"\"\"Check if file is TIFF\"\"\"\n",
        "    return path.lower().endswith(('.tif', '.tiff'))\n",
        "\n",
        "def iter_tiff_frames(path):\n",
        "    \"\"\"Iterate over TIFF frames\"\"\"\n",
        "    with tiff.TiffFile(path) as tif:\n",
        "        for page in tif.pages:\n",
        "            yield page.asarray().astype(np.float32)\n",
        "\n",
        "def count_tiff_frames(path):\n",
        "    \"\"\"Count frames in TIFF file\"\"\"\n",
        "    with tiff.TiffFile(path) as tif:\n",
        "        return len(tif.pages)\n",
        "\n",
        "# ============================================================================\n",
        "# 2. ND2 File Operations\n",
        "# ============================================================================\n",
        "\n",
        "def is_nd2(path):\n",
        "    \"\"\"Check if file is ND2\"\"\"\n",
        "    try:\n",
        "        import nd2\n",
        "        return nd2.is_supported_file(path)\n",
        "    except Exception:\n",
        "        return path.lower().endswith(\".nd2\")\n",
        "\n",
        "def count_nd2_frames(path):\n",
        "    \"\"\"Count frames in ND2 file\"\"\"\n",
        "    import nd2\n",
        "    with nd2.ND2File(path) as f:\n",
        "        try:\n",
        "            return len(f.loop_indices)\n",
        "        except Exception:\n",
        "            sz = getattr(f, \"sizes\", {}) or {}\n",
        "            prod = 1\n",
        "            for ax in (\"T\", \"Z\", \"C\", \"V\"):\n",
        "                prod *= int(sz.get(ax, 1))\n",
        "            return prod\n",
        "\n",
        "def _nd2_to_2d(arr, channel=None):\n",
        "    \"\"\"Convert ND2 frame to 2D\"\"\"\n",
        "    a = np.asarray(arr)\n",
        "    if a.ndim == 2:\n",
        "        return a\n",
        "    if a.ndim == 3:\n",
        "        if a.shape[-1] in (1, 3, 4):\n",
        "            idx = channel if (channel is not None and channel < a.shape[-1]) else 0\n",
        "            return a[..., idx]\n",
        "        if a.shape[0] in (1, 3, 4):\n",
        "            idx = channel if (channel is not None and channel < a.shape[0]) else 0\n",
        "            return a[idx, ...]\n",
        "        return a.mean(axis=0)\n",
        "    a = a.squeeze()\n",
        "    return a if a.ndim == 2 else a.reshape(a.shape[-2], a.shape[-1])\n",
        "\n",
        "def iter_nd2_frames(path, channel=None):\n",
        "    \"\"\"Iterate over ND2 frames\"\"\"\n",
        "    import nd2\n",
        "    n = count_nd2_frames(path)\n",
        "    with nd2.ND2File(path) as f:\n",
        "        for i in range(n):\n",
        "            fr = f.read_frame(i)\n",
        "            fr2d = _nd2_to_2d(fr, channel=channel)\n",
        "            yield fr2d.astype(np.float32, copy=False)\n",
        "\n",
        "def getPixelSizeND2metadata(path, display=False):\n",
        "    \"\"\"Extract pixel size from ND2 metadata\"\"\"\n",
        "    import nd2\n",
        "    with nd2.ND2File(path) as f:\n",
        "        vox_um = getattr(f, \"voxel_size\", None)\n",
        "        if vox_um is None:\n",
        "            return None, None, None\n",
        "        px_nm = vox_um[2] * 1e3\n",
        "        try:\n",
        "            h, w = f.shape[-2], f.shape[-1]\n",
        "        except Exception:\n",
        "            h = w = None\n",
        "        if display:\n",
        "            print(f\"Pixel size (ND2): {px_nm:.2f} nm | image ~ {w}x{h}\")\n",
        "        return px_nm, w, h\n",
        "\n",
        "# ============================================================================\n",
        "# 3. Drift Correction Functions\n",
        "# ============================================================================\n",
        "\n",
        "def correctDriftLocalization(xc_array, yc_array, frames, xDrift, yDrift):\n",
        "    \"\"\"Apply drift correction to localizations\"\"\"\n",
        "    n_locs = xc_array.shape[0]\n",
        "    xc_array_Corr = np.empty(n_locs)\n",
        "    yc_array_Corr = np.empty(n_locs)\n",
        "\n",
        "    for loc in range(n_locs):\n",
        "        xc_array_Corr[loc] = xc_array[loc] - xDrift[frames[loc] - 1]\n",
        "        yc_array_Corr[loc] = yc_array[loc] - yDrift[frames[loc] - 1]\n",
        "\n",
        "    return xc_array_Corr, yc_array_Corr\n",
        "\n",
        "def FromLoc2Image_SimpleHistogram(xc_array, yc_array, image_size=(64, 64), pixel_size=100):\n",
        "    \"\"\"Convert localizations to histogram image\"\"\"\n",
        "    w, h = image_size\n",
        "    locImage = np.zeros(image_size)\n",
        "    n_locs = len(xc_array)\n",
        "\n",
        "    for e in range(n_locs):\n",
        "        y_idx = int(max(min(round(yc_array[e] / pixel_size), w - 1), 0))\n",
        "        x_idx = int(max(min(round(xc_array[e] / pixel_size), h - 1), 0))\n",
        "        locImage[y_idx][x_idx] += 1\n",
        "\n",
        "    return locImage\n",
        "\n",
        "def estimate_drift_com_nm(img1, img2, pixel_size_nm, sigma=1.0, patch_radius=3):\n",
        "    \"\"\"Estimate drift using center of mass of cross-correlation\"\"\"\n",
        "    from scipy.ndimage import gaussian_filter\n",
        "    from scipy.signal import fftconvolve\n",
        "\n",
        "    # Smooth images\n",
        "    img1_smooth = gaussian_filter(img1.astype(np.float32), sigma=sigma)\n",
        "    img2_smooth = gaussian_filter(img2.astype(np.float32), sigma=sigma)\n",
        "\n",
        "    # Cross-correlation\n",
        "    corr = fftconvolve(img1_smooth, img2_smooth, mode='same')\n",
        "\n",
        "    # Center of image\n",
        "    center_y, center_x = np.array(corr.shape) // 2\n",
        "\n",
        "    # Crop around center\n",
        "    y_min = max(0, center_y - patch_radius)\n",
        "    y_max = min(corr.shape[0], center_y + patch_radius + 1)\n",
        "    x_min = max(0, center_x - patch_radius)\n",
        "    x_max = min(corr.shape[1], center_x + patch_radius + 1)\n",
        "\n",
        "    patch = corr[y_min:y_max, x_min:x_max]\n",
        "\n",
        "    # Center of mass\n",
        "    y_grid, x_grid = np.meshgrid(\n",
        "        np.arange(y_min, y_max), np.arange(x_min, x_max), indexing='ij'\n",
        "    )\n",
        "\n",
        "    total = np.sum(patch)\n",
        "    if total == 0:\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    y_com = np.sum(patch * y_grid) / total\n",
        "    x_com = np.sum(patch * x_grid) / total\n",
        "\n",
        "    # Drift in pixels\n",
        "    dy_px = y_com - center_y\n",
        "    dx_px = x_com - center_x\n",
        "\n",
        "    if abs(dy_px) > patch_radius or abs(dx_px) > patch_radius:\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    # Convert to nm\n",
        "    dy_nm = dy_px * pixel_size_nm\n",
        "    dx_nm = dx_px * pixel_size_nm\n",
        "\n",
        "    return dy_nm, dx_nm\n",
        "\n",
        "# ============================================================================\n",
        "# 4. Model Download Utilities\n",
        "# ============================================================================\n",
        "\n",
        "def ensure_models(model_names, target_root=\"/content/AutoDS_models\", model_manifest=None):\n",
        "    if model_manifest is None:\n",
        "        raise ValueError(\"model_manifest must be provided.\")\n",
        "\n",
        "    os.makedirs(target_root, exist_ok=True)\n",
        "\n",
        "    for m in model_names:\n",
        "        cfg = model_manifest[m]\n",
        "        mdir = os.path.join(target_root, m)\n",
        "        need_fetch = False\n",
        "\n",
        "        req = cfg.get(\"contains\", [])\n",
        "        if not os.path.isdir(mdir):\n",
        "            need_fetch = True\n",
        "        else:\n",
        "            for f in req:\n",
        "                if not os.path.exists(os.path.join(mdir, f)):\n",
        "                    need_fetch = True\n",
        "                    break\n",
        "\n",
        "        if not need_fetch:\n",
        "            print(f\"[models] found: {m}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"[models] preparing: {m}\")\n",
        "        os.makedirs(mdir, exist_ok=True)\n",
        "\n",
        "        if \"file_urls\" in cfg:\n",
        "            file_urls = cfg[\"file_urls\"]\n",
        "            for fname, url in file_urls.items():\n",
        "                dst = os.path.join(mdir, fname)\n",
        "                print(f\"[models] downloading: {url}\")\n",
        "                urllib.request.urlretrieve(url, dst)\n",
        "        else:\n",
        "            raise ValueError(f\"Model {m} manifest must have 'file_urls'.\")\n",
        "\n",
        "        for f in req:\n",
        "            if not os.path.exists(os.path.join(mdir, f)):\n",
        "                raise FileNotFoundError(f\"Model {m} missing required file: {f}\")\n",
        "\n",
        "        print(f\"[models] ready: {m}\")\n",
        "\n",
        "    return target_root\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import scipy.optimize as opt\n",
        "from numpy.lib.stride_tricks import sliding_window_view\n",
        "from scipy.ndimage import gaussian_filter, zoom\n",
        "from scipy.ndimage import gaussian_laplace, maximum_filter, binary_dilation\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ============================================================================\n",
        "# 1. Image Preprocessing Functions\n",
        "# ============================================================================\n",
        "\n",
        "def normalize_im_01(im):\n",
        "    \"\"\"Normalize image to [0, 1]\"\"\"\n",
        "    im = np.squeeze(im)\n",
        "    min_val = im.min()\n",
        "    max_val = im.max()\n",
        "    return (im - min_val) / (max_val - min_val)\n",
        "\n",
        "def normalize_im_01_ret_vals(im):\n",
        "    \"\"\"Normalize and return normalization parameters\"\"\"\n",
        "    im = np.squeeze(im)\n",
        "    min_val = im.min()\n",
        "    max_val = im.max()\n",
        "    return (im - min_val) / (max_val - min_val), min_val, max_val\n",
        "\n",
        "def normalize_im(im, dmean, dstd):\n",
        "    \"\"\"Normalize image with given mean and std\"\"\"\n",
        "    im = np.squeeze(im)\n",
        "    return (im - dmean) / dstd\n",
        "\n",
        "def subtract_smooth_background(im, sigma=3):\n",
        "    \"\"\"Subtract smoothed background\"\"\"\n",
        "    return im - gaussian_filter(im, sigma)\n",
        "\n",
        "def remove_zero_padding(image):\n",
        "    \"\"\"Remove zero padding from image\"\"\"\n",
        "    image_array = np.array(image)\n",
        "    non_zero_rows = np.where(image_array.sum(axis=1) != 0)\n",
        "    non_zero_cols = np.where(image_array.sum(axis=0) != 0)\n",
        "    cropped_image = image_array[non_zero_rows[0][0]:non_zero_rows[0][-1]+1,\n",
        "                                non_zero_cols[0][0]:non_zero_cols[0][-1]+1]\n",
        "    return cropped_image\n",
        "\n",
        "# ============================================================================\n",
        "# 2. Patch Splitting\n",
        "# ============================================================================\n",
        "\n",
        "def split_image_to_patches(img, num_patches, overlap):\n",
        "    \"\"\"\n",
        "    Split image into overlapping patches\n",
        "\n",
        "    Args:\n",
        "        img: Input image (H, W)\n",
        "        num_patches: Number of patches per dimension\n",
        "        overlap: Overlap size in pixels\n",
        "\n",
        "    Returns:\n",
        "        List of patches\n",
        "    \"\"\"\n",
        "    H, W = img.shape\n",
        "    patch_h = H // num_patches\n",
        "    patch_w = W // num_patches\n",
        "\n",
        "    # Pad image for border patches\n",
        "    padded_img = np.pad(img, ((overlap, overlap), (overlap, overlap)), mode='reflect')\n",
        "\n",
        "    # Window shape including overlap\n",
        "    window_shape = (patch_h + 2 * overlap, patch_w + 2 * overlap)\n",
        "\n",
        "    # Create sliding window view\n",
        "    patches_view = sliding_window_view(padded_img, window_shape)\n",
        "\n",
        "    # Sample at regular intervals\n",
        "    patches_array = patches_view[0::patch_h, 0::patch_w, :, :]\n",
        "\n",
        "    # Flatten to list\n",
        "    num_rows, num_cols, ph, pw = patches_array.shape\n",
        "    patches_list = [patches_array[i, j].copy()\n",
        "                   for i in range(num_rows)\n",
        "                   for j in range(num_cols)]\n",
        "\n",
        "    return patches_list\n",
        "\n",
        "# ============================================================================\n",
        "# 3. Interpolation and Scaling\n",
        "# ============================================================================\n",
        "\n",
        "def gaussian_interpolation_batch(data_batch, scale, sigma=1):\n",
        "    \"\"\"Apply Gaussian interpolation to batch of images\"\"\"\n",
        "    upsampled_data_batch = []\n",
        "\n",
        "    for data in data_batch:\n",
        "        smoothed_data = gaussian_filter(data, sigma=sigma)\n",
        "        upsampled_data = zoom(smoothed_data, scale, order=3)\n",
        "        upsampled_data_batch.append(upsampled_data)\n",
        "\n",
        "    return np.array(upsampled_data_batch)\n",
        "\n",
        "def interpolate_frames(tiff_stack, model_pixel_size, current_pixel_size,\n",
        "                      model_wavelength, current_wavelength,\n",
        "                      model_NA, current_NA):\n",
        "    \"\"\"Interpolate frames to match model parameters\"\"\"\n",
        "    # Set defaults\n",
        "    if model_pixel_size is None:\n",
        "        model_pixel_size = current_pixel_size\n",
        "    if model_wavelength is None:\n",
        "        model_wavelength = current_wavelength\n",
        "    if model_NA is None:\n",
        "        model_NA = current_NA\n",
        "    if current_wavelength is None:\n",
        "        current_wavelength = model_wavelength = 1\n",
        "    if current_NA is None:\n",
        "        current_NA = model_NA = 1\n",
        "\n",
        "    if len(tiff_stack.shape) == 2:\n",
        "        tiff_stack = tiff_stack[None, :, :]\n",
        "\n",
        "    # Compute scaling ratio based on optical parameters\n",
        "    scale_ratio_sq = ((0.21 * model_wavelength / model_NA) ** 2 -\n",
        "                     (0.21 * current_wavelength / current_NA) ** 2)\n",
        "\n",
        "    if scale_ratio_sq > 0:\n",
        "        scale_ratio = np.sqrt(scale_ratio_sq) / model_pixel_size\n",
        "        interpolated_stack = np.stack([\n",
        "            gaussian_filter(tiff_stack[i], scale_ratio)\n",
        "            for i in range(tiff_stack.shape[0])\n",
        "        ])\n",
        "    else:\n",
        "        zoom_factors = (1,\n",
        "                       model_pixel_size / current_pixel_size,\n",
        "                       model_pixel_size / current_pixel_size)\n",
        "        interpolated_stack = zoom(tiff_stack.astype(np.float32),\n",
        "                                 zoom_factors, order=3)\n",
        "\n",
        "    return interpolated_stack.astype(np.float32, copy=False)\n",
        "\n",
        "# ============================================================================\n",
        "# 4. Feature Extraction\n",
        "# ============================================================================\n",
        "\n",
        "def gauss2d(xy, offset, amp, x0, y0, sigma):\n",
        "    \"\"\"2D Gaussian function for fitting\"\"\"\n",
        "    x, y = xy\n",
        "    return offset + (amp * np.exp(-((x - x0) ** 2) / (2 * sigma ** 2) -\n",
        "                                  ((y - y0) ** 2) / (2 * sigma ** 2)))\n",
        "\n",
        "def extract_features_frame(OrigImage, pixel_size, psf_sigma, offset=None, verbose=False):\n",
        "    \"\"\"\n",
        "    Extract features from a single frame\n",
        "\n",
        "    Returns:\n",
        "        ADC_offset: Mean background\n",
        "        ReadOutNoise_ADC: Std of background\n",
        "        Signal_amp: Mean signal amplitude\n",
        "        emitter_density: Density of emitters (per μm²)\n",
        "    \"\"\"\n",
        "    M, N = OrigImage.shape\n",
        "\n",
        "    # Subtract smooth background\n",
        "    Image = OrigImage - gaussian_filter(OrigImage, sigma=5)\n",
        "\n",
        "    # Check if SNR is sufficient\n",
        "    if offset is not None:\n",
        "        if (np.percentile(gaussian_filter(Image, 2), 99) < 2 * Image.mean() or\n",
        "            np.percentile(OrigImage, 99) < 2 * offset):\n",
        "            if verbose:\n",
        "                print(\"SNR too low - ignoring patch\")\n",
        "            return np.mean(OrigImage), np.std(OrigImage), 0, 0\n",
        "\n",
        "    # Laplacian of Gaussian for blob detection\n",
        "    log_image = -gaussian_laplace(Image, sigma=psf_sigma)\n",
        "\n",
        "    # Local maxima filtering\n",
        "    neighborhood_size = 3\n",
        "    local_max = (log_image == maximum_filter(log_image, size=neighborhood_size))\n",
        "\n",
        "    # Intensity threshold\n",
        "    amp_threshold = np.mean(Image) + 0.5 * (np.percentile(Image, 99) - np.mean(Image))\n",
        "    pcntl_threshold = np.percentile(Image, 85)\n",
        "\n",
        "    # Binary mask for emitters\n",
        "    binary_mask = np.logical_and(local_max,\n",
        "                                 Image > np.max([amp_threshold, pcntl_threshold]))\n",
        "\n",
        "    # Dilate and create noise mask\n",
        "    dilated_mask = binary_dilation(binary_mask, structure=np.ones((5, 5)))\n",
        "    noise_mask = np.ones_like(binary_mask)\n",
        "    noise_mask[dilated_mask] = 0\n",
        "\n",
        "    if np.sum(binary_mask) > 0:\n",
        "        ADC_offset = np.mean(OrigImage[noise_mask])\n",
        "        ReadOutNoise_ADC = np.std(OrigImage[noise_mask])\n",
        "        Signal_amp = np.mean(OrigImage[binary_mask == 1])\n",
        "        emitter_density = (10 ** 6) * float(np.sum(binary_mask)) / (M * N * pixel_size ** 2)\n",
        "    else:\n",
        "        if verbose:\n",
        "            print(\"Didn't find any emitters\")\n",
        "        return np.mean(OrigImage), np.std(OrigImage), 0, 0\n",
        "\n",
        "    # Additional SNR check\n",
        "    if Signal_amp / ADC_offset < 2.5:\n",
        "        if emitter_density > 2:\n",
        "            if verbose:\n",
        "                print(\"SNR too low for emitter density estimation\")\n",
        "            return ADC_offset, ReadOutNoise_ADC, Signal_amp, 0\n",
        "\n",
        "    return ADC_offset, ReadOutNoise_ADC, Signal_amp, emitter_density\n",
        "\n",
        "# ============================================================================\n",
        "# 5. Model Selection\n",
        "# ============================================================================\n",
        "\n",
        "def ChooseNetByDifficulty_2025(density, SNR):\n",
        "    \"\"\" Choose network based on density and SNR \"\"\"\n",
        "    num_models = 4\n",
        "    norm_density = np.max([np.min([int(np.round(2 * density)), num_models - 1]), 0])\n",
        "    norm_SNR = num_models - 1 - np.max([np.min([SNR // 2, num_models - 1]), 0])\n",
        "    return int(np.round((norm_SNR + norm_density) / 2))\n",
        "\n",
        "# ============================================================================\n",
        "# Module-level kernel cache (shared across all calls)\n",
        "_kernel_cache = {}\n",
        "\n",
        "def _get_gaussian_kernel(sigma, device):\n",
        "    \"\"\"Generate Gaussian kernel for smoothing\"\"\"\n",
        "    key = f'gauss_{sigma}_{device}'\n",
        "    if key not in _kernel_cache:\n",
        "        kernel_size = int(2 * np.ceil(3 * sigma) + 1)\n",
        "        ax = torch.arange(-kernel_size // 2 + 1., kernel_size // 2 + 1., device=device)\n",
        "        xx, yy = torch.meshgrid(ax, ax, indexing='ij')\n",
        "        kernel = torch.exp(-(xx ** 2 + yy ** 2) / (2 * sigma ** 2))\n",
        "        kernel = kernel / kernel.sum()\n",
        "        _kernel_cache[key] = kernel.view(1, 1, kernel_size, kernel_size)\n",
        "    return _kernel_cache[key]\n",
        "\n",
        "\n",
        "def _get_log_kernel(sigma, device):\n",
        "    \"\"\"Generate Laplacian of Gaussian kernel for blob detection\"\"\"\n",
        "    key = f'log_{sigma}_{device}'\n",
        "    if key not in _kernel_cache:\n",
        "        kernel_size = int(2 * np.ceil(3 * sigma) + 1)\n",
        "        ax = torch.arange(-kernel_size // 2 + 1., kernel_size // 2 + 1., device=device)\n",
        "        xx, yy = torch.meshgrid(ax, ax, indexing='ij')\n",
        "        r2 = xx ** 2 + yy ** 2\n",
        "        kernel = -(1 / (np.pi * sigma ** 4)) * (1 - r2 / (2 * sigma ** 2)) * torch.exp(-r2 / (2 * sigma ** 2))\n",
        "        _kernel_cache[key] = kernel.view(1, 1, kernel_size, kernel_size)\n",
        "    return _kernel_cache[key]\n",
        "\n",
        "\n",
        "def percentile_batch(tensor, percentile):\n",
        "    \"\"\"Calculate percentile for batched tensors\"\"\"\n",
        "    flat = tensor.flatten(1)\n",
        "    result = torch.quantile(flat, percentile / 100.0, dim=1)\n",
        "    return result\n",
        "\n",
        "def extract_features_batch(patches_tensor, pixel_size, psf_sigma, offset_array=None,\n",
        "                           verbose=False, device='cuda'):\n",
        "    \"\"\"Fully GPU-accelerated batch feature extraction\"\"\"\n",
        "    B, H, W = patches_tensor.shape\n",
        "    device = patches_tensor.device\n",
        "\n",
        "    # Add channel dimension for conv operations: [B, 1, H, W]\n",
        "    patches_4d = patches_tensor.unsqueeze(1)\n",
        "\n",
        "    # 1. Gaussian filtering\n",
        "    gauss_kernel = _get_gaussian_kernel(5, device)\n",
        "    padding = gauss_kernel.shape[-1] // 2\n",
        "    smooth_bg = F.conv2d(patches_4d, gauss_kernel, padding=padding)\n",
        "    Image = patches_4d - smooth_bg # [B, 1, H, W]\n",
        "\n",
        "    # 2. LoG filtering\n",
        "    log_kernel = _get_log_kernel(psf_sigma, device)\n",
        "    padding = log_kernel.shape[-1] // 2\n",
        "    log_image = -F.conv2d(Image, log_kernel, padding=padding) # [B, 1, H, W]\n",
        "\n",
        "    # 3. Local maxima\n",
        "    local_max = F.max_pool2d(log_image, kernel_size=3, stride=1, padding=1) # [B, 1, H, W]\n",
        "\n",
        "    # 4. Thresholding (all with size [B, 1, 1, 1])\n",
        "    img_mean = Image.mean(dim=(2, 3), keepdim=True)\n",
        "    img_99 = percentile_batch(Image.squeeze(1), 99).view(B, 1, 1, 1)\n",
        "    img_85 = percentile_batch(Image.squeeze(1), 85).view(B, 1, 1, 1)\n",
        "    threshold = torch.max(img_mean + 0.5 * (img_99 - img_mean), img_85)\n",
        "\n",
        "    # 5. Binary masks (batch-wise)\n",
        "    binary_mask = torch.logical_and(log_image == local_max, Image >= threshold)\n",
        "    mask_float = binary_mask.float()\n",
        "    dilated = F.max_pool2d(mask_float, kernel_size=5, stride=1, padding=2)\n",
        "    noise_mask = (dilated < 0.5)\n",
        "\n",
        "    # 6. PRE-COMPUTE SNR check data on GPU as a batch\n",
        "    gauss_kernel_2 = _get_gaussian_kernel(2, device)\n",
        "    padding_2 = (gauss_kernel_2.shape[-1] // 2)\n",
        "    gauss_smooth = F.conv2d(Image, gauss_kernel_2, padding=padding_2)\n",
        "\n",
        "    # Pre-compute percentiles on GPU (batch-wise)\n",
        "    gauss_99 = percentile_batch(gauss_smooth.squeeze(1), 99) #[B]\n",
        "    patch_99 = percentile_batch(patches_tensor, 99)  # [B]\n",
        "    img_mean_flat = img_mean.squeeze()  # [B]\n",
        "\n",
        "    # 7. Statistics on CPU\n",
        "    patches_cpu = patches_tensor.cpu().numpy()\n",
        "    binary_mask_cpu = binary_mask.squeeze(1).cpu().numpy()\n",
        "    noise_mask_cpu = noise_mask.squeeze(1).cpu().numpy()\n",
        "\n",
        "    # Move pre-computed values to CPU\n",
        "    gauss_99_cpu = gauss_99.cpu().numpy()\n",
        "    patch_99_cpu = patch_99.cpu().numpy()\n",
        "    img_mean_cpu = img_mean_flat.cpu().numpy()\n",
        "\n",
        "    results = []\n",
        "    pixel_area = pixel_size * pixel_size\n",
        "\n",
        "    for i in range(B):\n",
        "        patch = patches_cpu[i]\n",
        "        emitter_mask = binary_mask_cpu[i]\n",
        "        noise_m = noise_mask_cpu[i]\n",
        "        patch_offset = offset_array[i]\n",
        "\n",
        "        if patch_offset is not None:\n",
        "            if (gauss_99_cpu[i] < 2 * img_mean_cpu[i] or\n",
        "                patch_99_cpu[i] < 2 * patch_offset):\n",
        "                if verbose:\n",
        "                    print(f\"Patch {i}: SNR too low - ignoring patch\")\n",
        "                results.append((patch.mean(), patch.std(), 0.0, 0.0))\n",
        "                continue\n",
        "\n",
        "        num_emitters = emitter_mask.sum()\n",
        "        if num_emitters == 0:\n",
        "            if verbose:\n",
        "                print(f\"Patch {i}: Didn't find any emitters\")\n",
        "            results.append((patch.mean(), patch.std(), 0.0, 0.0))\n",
        "            continue\n",
        "\n",
        "        ADC_offset = patch[noise_m].mean()\n",
        "        ReadOutNoise_ADC = patch[noise_m].std()\n",
        "        Signal_amp = patch[emitter_mask].mean()\n",
        "        emitter_density = 1e6 * float(num_emitters) / (H * W * pixel_area)\n",
        "\n",
        "        # Additional SNR check\n",
        "        if Signal_amp / (ADC_offset + 1e-8) < 2.5:\n",
        "            if emitter_density > 2:\n",
        "                if verbose:\n",
        "                    print(f\"Patch {i}: SNR too low for emitter density estimation\")\n",
        "                results.append((float(ADC_offset), float(ReadOutNoise_ADC),\n",
        "                                float(Signal_amp), 0.0))\n",
        "                continue\n",
        "\n",
        "        results.append((float(ADC_offset), float(ReadOutNoise_ADC),\n",
        "                        float(Signal_amp), float(emitter_density)))\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def preprocess_frames_batch(frames_batch, device='cuda'):\n",
        "    \"\"\"GPU-accelerated batch preprocessing of frames\"\"\"\n",
        "    B, H, W = frames_batch.shape\n",
        "\n",
        "    # Calculate 35th percentile for each frame (on GPU)\n",
        "    frames_flat = frames_batch.reshape(B, -1)\n",
        "    p35 = torch.quantile(frames_flat, 0.35, dim=1, keepdim=True)\n",
        "    p35 = p35.view(B, 1, 1)\n",
        "\n",
        "    # Subtract 35th percentile\n",
        "    frames_processed = frames_batch - p35\n",
        "\n",
        "    # Subtract minimum\n",
        "    frames_min = frames_processed.reshape(B, -1).min(dim=1, keepdim=True)[0]\n",
        "    frames_min = frames_min.view(B, 1, 1)\n",
        "    frames_processed = frames_processed - frames_min\n",
        "\n",
        "    # Calculate mean and std for normalization\n",
        "    frames_mean = frames_processed.reshape(B, -1).double().mean(dim=1).float()\n",
        "    frames_std = frames_processed.reshape(B, -1).double().std(dim=1).float() + 1e-6\n",
        "    frames_mean_batch = frames_mean.view(B, 1, 1)\n",
        "    frames_std_batch = frames_std.view(B, 1, 1)\n",
        "\n",
        "    # Normalize\n",
        "    frames_processed = (frames_processed - frames_mean_batch) / frames_std_batch\n",
        "\n",
        "    # Calculate offsets\n",
        "    offsets = frames_processed.reshape(B, -1).mean(dim=1)\n",
        "\n",
        "    return frames_processed, offsets\n",
        "\n",
        "\n",
        "def interpolate_frames_batch(frames_batch, model_pixel_size, current_pixel_size,\n",
        "                                  model_wavelength, current_wavelength,\n",
        "                                  model_NA, current_NA, device='cuda'):\n",
        "    \"\"\"GPU-accelerated batch interpolation for multiple frames\"\"\"\n",
        "    # Handle None values\n",
        "    if model_pixel_size is None: model_pixel_size = current_pixel_size\n",
        "    if model_wavelength is None: model_wavelength = current_wavelength\n",
        "    if model_NA is None: model_NA = current_NA\n",
        "    if current_wavelength is None: current_wavelength = model_wavelength = 1\n",
        "    if current_NA is None: current_NA = model_NA = 1\n",
        "\n",
        "    # Calculate scale ratio\n",
        "    scale_ratio_sq = (0.21 * model_wavelength / model_NA) ** 2 - \\\n",
        "                     (0.21 * current_wavelength / current_NA) ** 2\n",
        "\n",
        "    if scale_ratio_sq > 0:\n",
        "        # Gaussian smoothing path\n",
        "        scale_ratio = np.sqrt(scale_ratio_sq) / model_pixel_size\n",
        "        kernel = _get_gaussian_kernel(scale_ratio, device)\n",
        "\n",
        "        # Apply Gaussian filter to all frames at once\n",
        "        frames_4d = frames_batch.unsqueeze(1)  # (B, 1, H, W)\n",
        "        padding = kernel.shape[-1] // 2\n",
        "        interpolated = F.conv2d(frames_4d, kernel, padding=padding).squeeze(1)\n",
        "    else:\n",
        "        # Zoom/resize path\n",
        "        zoom_factor = model_pixel_size / current_pixel_size\n",
        "\n",
        "        if zoom_factor != 1.0:\n",
        "            # Use bilinear interpolation on GPU\n",
        "            new_h = int(frames_batch.shape[1] * zoom_factor)\n",
        "            new_w = int(frames_batch.shape[2] * zoom_factor)\n",
        "\n",
        "            frames_4d = frames_batch.unsqueeze(1)\n",
        "            interpolated = F.interpolate(frames_4d, size=(new_h, new_w),\n",
        "                                        mode='bicubic', align_corners=False).squeeze(1)\n",
        "        else:\n",
        "            interpolated = frames_batch\n",
        "\n",
        "    return interpolated\n",
        "\n",
        "def split_image_to_patches_batch(img_batch, num_patches, overlap, device='cuda'):\n",
        "    \"\"\" Split tensor of images into overlapping patches \"\"\"\n",
        "    # Handle both 2D and 3D input\n",
        "    if img_batch.dim() == 2:\n",
        "        img_batch = img_batch.unsqueeze(0)  # (H, W) -> (1, H, W)\n",
        "\n",
        "    # Determine the non-overlapping patch size\n",
        "    B, H, W = img_batch.shape\n",
        "    patch_h = H // num_patches\n",
        "    patch_w = W // num_patches\n",
        "\n",
        "    # Pad image for border patches (reflection padding as in the original)\n",
        "    padded = F.pad(img_batch.unsqueeze(1), # (B, 1, H, W)\n",
        "                    (overlap, overlap, overlap, overlap),\n",
        "                    mode='reflect').squeeze(1) # (B, H+2*overlap, W+2*overlap)\n",
        "\n",
        "    # Calculate window shape including overlap\n",
        "    window_h = patch_h + 2 * overlap\n",
        "    window_w = patch_w + 2 * overlap\n",
        "\n",
        "    # create sliding windows along height and then along width with the patch_h and patch_w as the step\n",
        "    patches = padded.unfold(1, window_h, patch_h).unfold(2, window_w, patch_w)\n",
        "    # Shape: (B, num_patches, num_patches, window_h, window_w)\n",
        "\n",
        "    # Reshape to (B, num_patches * num_patches, window_h, window_w)\n",
        "    # Flatten the 2D grid of patches for every frame (row-major order).\n",
        "    B, num_rows, num_cols, ph, pw = patches.shape\n",
        "    patches = patches.reshape(B, num_rows * num_cols, ph, pw)\n",
        "\n",
        "    return patches\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ============================================================================\n",
        "# 1. Basic CNN Model (without upsampling)\n",
        "# ============================================================================\n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, in_channels=1):\n",
        "        super(CNNModel, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.features1 = ConvBNReLU(in_channels, 32, 3)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.features2 = ConvBNReLU(32, 64, 3)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.features3 = ConvBNReLU(64, 128, 3)\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.features4 = ConvBNReLU(128, 512, 3)\n",
        "\n",
        "        # Decoder\n",
        "        self.upsample1 = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.features5 = ConvBNReLU(512, 128, 3)\n",
        "\n",
        "        self.upsample2 = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.features6 = ConvBNReLU(128, 64, 3)\n",
        "\n",
        "        self.upsample3 = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.features7 = ConvBNReLU(64, 32, 3)\n",
        "\n",
        "        # Prediction head\n",
        "        self.prediction = nn.Conv2d(32, 1, 1, stride=1, padding=0, bias=False)\n",
        "        nn.init.orthogonal_(self.prediction.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x = self.features1(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.features2(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = self.features3(x)\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        # Bottleneck\n",
        "        x = self.features4(x)\n",
        "\n",
        "        # Decoder\n",
        "        x = self.upsample1(x)\n",
        "        x = self.features5(x)\n",
        "\n",
        "        x = self.upsample2(x)\n",
        "        x = self.features6(x)\n",
        "\n",
        "        x = self.upsample3(x)\n",
        "        x = self.features7(x)\n",
        "\n",
        "        # Prediction\n",
        "        x = self.prediction(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 2. CNN Building Blocks - optimized with fused Conv+BN+ReL operations\n",
        "# ============================================================================\n",
        "\n",
        "class ConvBNReLU(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=None):\n",
        "        super(ConvBNReLU, self).__init__()\n",
        "\n",
        "        if padding is None:\n",
        "            padding = kernel_size // 2\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size,\n",
        "                              stride=stride, padding=padding, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Initialize with Orthogonal (similar to Keras)\n",
        "        nn.init.orthogonal_(self.conv.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 3. CNN Model with Upsampling - optimized with fused Conv+BN+ReL\n",
        "# ============================================================================\n",
        "\n",
        "class CNNUpsample(nn.Module):\n",
        "    def __init__(self, in_channels=1, upsampling_factor=8):\n",
        "        super(CNNUpsample, self).__init__()\n",
        "        self.upsampling_factor = upsampling_factor\n",
        "\n",
        "        # Encoder with fused blocks\n",
        "        self.conv_bn_relu1 = ConvBNReLU(in_channels, 32, 3, 1)\n",
        "        self.conv_bn_relu2 = ConvBNReLU(32, 64, 3, 1)\n",
        "        self.conv_bn_relu3 = ConvBNReLU(64, 128, 3, 1)\n",
        "        self.conv_bn_relu4 = ConvBNReLU(128, 256, 3, 1)\n",
        "\n",
        "        # Decoder with fused blocks\n",
        "        self.conv_bn_relu5 = ConvBNReLU(256, 128, 3, 1)\n",
        "        self.conv_bn_relu6 = ConvBNReLU(128, 64, 3, 1)\n",
        "\n",
        "        # OPTIMIZED: Upsampling blocks with 3x3 kernels + fused Conv+BN+ReLU\n",
        "        num_upsample_blocks = int(np.log2(upsampling_factor))\n",
        "        self.upsample_blocks = nn.ModuleList()\n",
        "\n",
        "        for i in range(num_upsample_blocks):\n",
        "            in_ch = 64 if i == 0 else 32\n",
        "            block = nn.ModuleDict({\n",
        "                'upsample': nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "                'conv_bn_relu': ConvBNReLU(in_ch, 32, 5, 1)\n",
        "            })\n",
        "            self.upsample_blocks.append(block)\n",
        "\n",
        "        # Prediction head\n",
        "        self.prediction = nn.Conv2d(32, 1, 1, stride=1, padding=0, bias=False)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.orthogonal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x = self.conv_bn_relu1(x)\n",
        "        x = self.conv_bn_relu2(x)\n",
        "        x = self.conv_bn_relu3(x)\n",
        "        x = self.conv_bn_relu4(x)\n",
        "\n",
        "        # Decoder\n",
        "        x = self.conv_bn_relu5(x)\n",
        "        x = self.conv_bn_relu6(x)\n",
        "\n",
        "        # Upsampling\n",
        "        for block in self.upsample_blocks:\n",
        "            x = block['upsample'](x)\n",
        "            x = block['conv_bn_relu'](x)\n",
        "\n",
        "        # Prediction\n",
        "        x = self.prediction(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 1. Gaussian Filter for Loss Computation\n",
        "# ============================================================================\n",
        "\n",
        "def matlab_style_gauss2D(shape=(7, 7), sigma=1):\n",
        "    \"\"\"Create 2D Gaussian kernel matching MATLAB style\"\"\"\n",
        "    m, n = [(ss - 1.) / 2. for ss in shape]\n",
        "    y, x = np.ogrid[-m:m+1, -n:n+1]\n",
        "    h = np.exp(-(x*x + y*y) / (2. * sigma * sigma))\n",
        "    h[h < np.finfo(h.dtype).eps * h.max()] = 0\n",
        "    sumh = h.sum()\n",
        "    if sumh != 0:\n",
        "        h /= sumh\n",
        "    h = h * 2.0\n",
        "    return h.astype(np.float32)\n",
        "\n",
        "# Create Gaussian filter as a tensor\n",
        "psf_heatmap = matlab_style_gauss2D(shape=(7, 7), sigma=1)\n",
        "# Shape: [out_channels, in_channels, height, width] -> [1, 1, 7, 7]\n",
        "gfilter = torch.from_numpy(psf_heatmap).view(1, 1, 7, 7)\n",
        "\n",
        "# ============================================================================\n",
        "# 2. Custom Loss Functions\n",
        "# ============================================================================\n",
        "\n",
        "class L1L2Loss(nn.Module):\n",
        "    \"\"\"Combined L1 + L2 loss with Gaussian filtering\"\"\"\n",
        "    def __init__(self, input_shape):\n",
        "        super(L1L2Loss, self).__init__()\n",
        "        self.input_shape = input_shape\n",
        "        # Register Gaussian filter as buffer (moves with model to GPU)\n",
        "        self.register_buffer('gfilter', gfilter)\n",
        "\n",
        "    def forward(self, spikes_pred, heatmap_true):\n",
        "        # Apply Gaussian convolution to predictions\n",
        "        heatmap_pred = F.conv2d(spikes_pred, self.gfilter, padding=3)\n",
        "\n",
        "        # MSE loss on heatmaps\n",
        "        loss_heatmaps = F.mse_loss(heatmap_pred, heatmap_true)\n",
        "\n",
        "        # L1 loss on spikes (sparsity)\n",
        "        loss_spikes = torch.mean(torch.abs(spikes_pred))\n",
        "\n",
        "        return loss_heatmaps + loss_spikes\n",
        "\n",
        "class CustomLoss(nn.Module):\n",
        "    \"\"\"Custom loss for upsampling model\"\"\"\n",
        "    def __init__(self, input_shape):\n",
        "        super(CustomLoss, self).__init__()\n",
        "        self.input_shape = input_shape\n",
        "        self.register_buffer('gfilter', gfilter)\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        # Apply Gaussian convolution\n",
        "        heatmap_pred = F.conv2d(y_pred, self.gfilter, padding=3)\n",
        "\n",
        "        # MSE on heatmaps\n",
        "        loss_heatmaps = torch.mean((y_true - heatmap_pred) ** 2)\n",
        "\n",
        "        # L1 on predictions (sparsity)\n",
        "        loss_spikes = torch.mean(torch.abs(y_pred))\n",
        "\n",
        "        return loss_heatmaps + loss_spikes\n",
        "\n",
        "# ============================================================================\n",
        "# 3. Maxima Finder Layer (Peak Detection)\n",
        "# ============================================================================\n",
        "\n",
        "class MaximaFinder(nn.Module):\n",
        "    \"\"\"Find local maxima in predicted density maps\"\"\"\n",
        "    def __init__(self, thresh=0.1, neighborhood_size=3, use_local_avg=False):\n",
        "        super(MaximaFinder, self).__init__()\n",
        "        self.thresh = thresh\n",
        "        self.nhood = neighborhood_size\n",
        "        self.use_local_avg = use_local_avg\n",
        "\n",
        "        if use_local_avg:\n",
        "            # Sobel-like kernels for local averaging\n",
        "            kernel_x = torch.tensor([[[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]],\n",
        "                                    dtype=torch.float32).view(1, 1, 3, 3)\n",
        "            kernel_y = torch.tensor([[[-1, -1, -1], [0, 0, 0], [1, 1, 1]]],\n",
        "                                    dtype=torch.float32).view(1, 1, 3, 3)\n",
        "            kernel_sum = torch.ones(1, 1, 3, 3, dtype=torch.float32)\n",
        "\n",
        "            self.register_buffer('kernel_x', kernel_x)\n",
        "            self.register_buffer('kernel_y', kernel_y)\n",
        "            self.register_buffer('kernel_sum', kernel_sum)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Max pooling to find local maxima\n",
        "        max_pool = F.max_pool2d(inputs, kernel_size=self.nhood,\n",
        "                               stride=1, padding=self.nhood//2)\n",
        "\n",
        "        # Condition: value is local max AND above threshold\n",
        "        cond = (max_pool > self.thresh) & (max_pool == inputs)\n",
        "\n",
        "        # Get indices where condition is True\n",
        "        indices = torch.nonzero(cond, as_tuple=False)  # (N, 4): [batch, channel, y, x]\n",
        "\n",
        "        bind = indices[:, 0]  # batch indices\n",
        "        yind = indices[:, 2]  # y coordinates\n",
        "        xind = indices[:, 3]  # x coordinates\n",
        "\n",
        "        # Gather confidence values\n",
        "        confidence = inputs[bind, indices[:, 1], yind, xind]\n",
        "\n",
        "        # Convert to float for potential subpixel refinement\n",
        "        xind = xind.float()\n",
        "        yind = yind.float()\n",
        "\n",
        "        # Subpixel refinement using local averaging\n",
        "        if self.use_local_avg:\n",
        "            # Ensure kernels match input dtype\n",
        "            kernel_x = self.kernel_x.to(inputs.dtype)\n",
        "            kernel_y = self.kernel_y.to(inputs.dtype)\n",
        "            kernel_sum = self.kernel_sum.to(dtype=inputs.dtype)\n",
        "\n",
        "            # Compute gradients\n",
        "            # Sobel-like kernels for local averaging\n",
        "            x_image = F.conv2d(inputs, kernel_x, padding=1)\n",
        "            y_image = F.conv2d(inputs, kernel_y, padding=1)\n",
        "            sum_image = F.conv2d(inputs, kernel_sum, padding=1)\n",
        "\n",
        "            # Gather at detected locations\n",
        "            gathered_sum = sum_image[bind, indices[:, 1], yind.long(), xind.long()]\n",
        "            gathered_x = x_image[bind, indices[:, 1], yind.long(), xind.long()]\n",
        "            gathered_y = y_image[bind, indices[:, 1], yind.long(), xind.long()]\n",
        "\n",
        "            # Compute local offsets\n",
        "            x_local = gathered_x / (gathered_sum + 1e-6)\n",
        "            y_local = gathered_y / (gathered_sum + 1e-6)\n",
        "\n",
        "            # Update positions and confidence\n",
        "            xind = xind + x_local\n",
        "            yind = yind + y_local\n",
        "            confidence = gathered_sum\n",
        "\n",
        "        return bind, xind, yind, confidence\n",
        "\n",
        "# ============================================================================\n",
        "# 6. Maxima Finder Layer (Peak Detection)\n",
        "# ============================================================================\n",
        "\n",
        "class MaximaFinder(nn.Module):\n",
        "    \"\"\"Find local maxima in predicted density maps\"\"\"\n",
        "    def __init__(self, thresh=0.1, neighborhood_size=3, use_local_avg=False):\n",
        "        super(MaximaFinder, self).__init__()\n",
        "        self.thresh = thresh\n",
        "        self.nhood = neighborhood_size\n",
        "        self.use_local_avg = use_local_avg\n",
        "\n",
        "        if use_local_avg:\n",
        "            # Sobel-like kernels for local averaging\n",
        "            kernel_x = torch.tensor([[[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]],\n",
        "                                    dtype=torch.float32).view(1, 1, 3, 3)\n",
        "            kernel_y = torch.tensor([[[-1, -1, -1], [0, 0, 0], [1, 1, 1]]],\n",
        "                                    dtype=torch.float32).view(1, 1, 3, 3)\n",
        "            kernel_sum = torch.ones(1, 1, 3, 3, dtype=torch.float32)\n",
        "\n",
        "            self.register_buffer('kernel_x', kernel_x)\n",
        "            self.register_buffer('kernel_y', kernel_y)\n",
        "            self.register_buffer('kernel_sum', kernel_sum)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Max pooling to find local maxima\n",
        "        max_pool = F.max_pool2d(inputs, kernel_size=self.nhood,\n",
        "                               stride=1, padding=self.nhood//2)\n",
        "\n",
        "        # Condition: value is local max AND above threshold\n",
        "        cond = (max_pool > self.thresh) & (max_pool == inputs)\n",
        "\n",
        "        # Get indices where condition is True\n",
        "        indices = torch.nonzero(cond, as_tuple=False)  # (N, 4): [batch, channel, y, x]\n",
        "\n",
        "        bind = indices[:, 0]  # batch indices\n",
        "        yind = indices[:, 2]  # y coordinates\n",
        "        xind = indices[:, 3]  # x coordinates\n",
        "\n",
        "        # Gather confidence values\n",
        "        confidence = inputs[bind, indices[:, 1], yind, xind]\n",
        "\n",
        "        # Convert to float for potential subpixel refinement\n",
        "        xind = xind.float()\n",
        "        yind = yind.float()\n",
        "\n",
        "        # Subpixel refinement using local averaging\n",
        "        if self.use_local_avg:\n",
        "            # Ensure kernels match input dtype\n",
        "            kernel_x = self.kernel_x.to(inputs.dtype)\n",
        "            kernel_y = self.kernel_y.to(inputs.dtype)\n",
        "            kernel_sum = self.kernel_sum.to(dtype=inputs.dtype)\n",
        "\n",
        "            # Compute gradients\n",
        "            x_image = F.conv2d(inputs, kernel_x, padding=1)\n",
        "            y_image = F.conv2d(inputs, kernel_y, padding=1)\n",
        "            sum_image = F.conv2d(inputs, kernel_sum, padding=1)\n",
        "\n",
        "            # Gather at detected locations\n",
        "            gathered_sum = sum_image[bind, indices[:, 1], yind.long(), xind.long()]\n",
        "            gathered_x = x_image[bind, indices[:, 1], yind.long(), xind.long()]\n",
        "            gathered_y = y_image[bind, indices[:, 1], yind.long(), xind.long()]\n",
        "\n",
        "            # Compute local offsets\n",
        "            x_local = gathered_x / (gathered_sum + 1e-6)\n",
        "            y_local = gathered_y / (gathered_sum + 1e-6)\n",
        "\n",
        "            # Update positions and confidence\n",
        "            xind = xind + x_local\n",
        "            yind = yind + y_local\n",
        "            confidence = gathered_sum\n",
        "\n",
        "        return bind, xind, yind, confidence\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import h5py\n",
        "import re\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 1. Model Builder Function\n",
        "# ============================================================================\n",
        "\n",
        "def build_model_upsample(input_shape, lr=0.001, upsampling_factor=8):\n",
        "    \"\"\"\n",
        "    Build upsampling model for PyTorch\n",
        "\n",
        "    Args:\n",
        "        input_shape: Tuple (H, W, C) - note: will be converted to (C, H, W)\n",
        "        lr: Learning rate\n",
        "        upsampling_factor: Upsampling factor\n",
        "\n",
        "    Returns:\n",
        "        model: PyTorch model\n",
        "        optimizer: Adam optimizer\n",
        "        criterion: Loss function\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert from (H, W, C) to (C, H, W)\n",
        "    in_channels = input_shape[2] if len(input_shape) == 3 else 1\n",
        "\n",
        "    model = CNNUpsample(in_channels=in_channels,\n",
        "                        upsampling_factor=upsampling_factor)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = CustomLoss(input_shape)\n",
        "\n",
        "    return model, optimizer, criterion\n",
        "\n",
        "# ============================================================================\n",
        "# 2. Weight Loading - Support both PyTorch and Keras formats\n",
        "# ============================================================================\n",
        "\n",
        "def load_model_weights(model, weights_path, verbose=True):\n",
        "    \"\"\"\n",
        "    Load model weights from either PyTorch (.pth) or Keras (.h5) format\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "        weights_path: Path to weights file (.pth or .h5)\n",
        "        verbose: Print loading progress\n",
        "    \"\"\"\n",
        "    if weights_path.endswith('.pth'):\n",
        "        load_pytorch_weights(model, weights_path, verbose=verbose)\n",
        "    elif weights_path.endswith('.h5'):\n",
        "        load_keras_weights_to_pytorch(model, weights_path, verbose=verbose)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported weights format: {weights_path}. \"\n",
        "                        f\"Expected .pth or .h5 file\")\n",
        "\n",
        "\n",
        "def load_pytorch_weights(model, pth_path, verbose=True):\n",
        "    \"\"\"\n",
        "    Load PyTorch native weights from .pth file\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "        pth_path: Path to .pth weights file\n",
        "        verbose: Print loading progress\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(f\"Loading PyTorch weights from {pth_path}\")\n",
        "\n",
        "    # Get device from model\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Load state dict\n",
        "    state_dict = torch.load(pth_path, map_location=device)\n",
        "\n",
        "    # Load weights into model\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"✓ PyTorch weights loaded successfully!\")\n",
        "\n",
        "\n",
        "def load_keras_weights_to_pytorch(model, h5_path, verbose=True):\n",
        "    \"\"\"\n",
        "    Load Keras weights from H5 file to PyTorch model\n",
        "\n",
        "    Supports fused Conv+BN+ReLU blocks while maintaining compatibility.\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model (CNNUpsample with fused blocks)\n",
        "        h5_path: Path to Keras H5 weights file\n",
        "        verbose: Print loading progress\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(f\"Loading Keras weights from {h5_path}\")\n",
        "\n",
        "    # Get device from model\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    with h5py.File(h5_path, 'r') as f:\n",
        "        # Get all layer names from the H5 file\n",
        "        if 'model_weights' in f:\n",
        "            weight_group = f['model_weights']\n",
        "        else:\n",
        "            weight_group = f\n",
        "\n",
        "        # Extract layer names\n",
        "        if hasattr(weight_group, 'attrs') and 'layer_names' in weight_group.attrs:\n",
        "            layer_names = [n.decode('utf8') if isinstance(n, bytes) else n\n",
        "                           for n in weight_group.attrs['layer_names']]\n",
        "        else:\n",
        "            layer_names = list(weight_group.keys())\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Found {len(layer_names)} layers in H5 file\")\n",
        "\n",
        "        # Create a dictionary to store weights\n",
        "        keras_weights = {}\n",
        "\n",
        "        for layer_name in layer_names:\n",
        "            if layer_name not in weight_group:\n",
        "                continue\n",
        "\n",
        "            layer_group = weight_group[layer_name]\n",
        "\n",
        "            if not hasattr(layer_group, 'keys'):\n",
        "                continue\n",
        "\n",
        "            # Get weight names for this layer\n",
        "            if hasattr(layer_group, 'attrs') and 'weight_names' in layer_group.attrs:\n",
        "                weight_names = [n.decode('utf8') if isinstance(n, bytes) else n\n",
        "                                for n in layer_group.attrs['weight_names']]\n",
        "            else:\n",
        "                weight_names = list(layer_group.keys())\n",
        "\n",
        "            # Extract weights\n",
        "            layer_weights = {}\n",
        "            for weight_name in weight_names:\n",
        "                if '/' in weight_name:\n",
        "                    weight_key = weight_name.split('/')[-1]\n",
        "                else:\n",
        "                    weight_key = weight_name\n",
        "\n",
        "                try:\n",
        "                    weight_value = layer_group[weight_name][()]\n",
        "                    layer_weights[weight_key] = weight_value\n",
        "                except:\n",
        "                    try:\n",
        "                        weight_value = layer_group[weight_key][()]\n",
        "                        layer_weights[weight_key] = weight_value\n",
        "                    except:\n",
        "                        if verbose:\n",
        "                            print(f\"  Warning: Could not load {weight_name} from {layer_name}\")\n",
        "\n",
        "            if layer_weights:\n",
        "                keras_weights[layer_name] = layer_weights\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Extracted weights from {len(keras_weights)} layers\")\n",
        "\n",
        "        # Assign to PyTorch model with fused blocks\n",
        "        _assign_weights_to_model(model, keras_weights, device, verbose=verbose)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"✓ Keras weights loaded successfully!\")\n",
        "\n",
        "\n",
        "def _assign_weights_to_model(model, keras_weights, device, verbose=True):\n",
        "    \"\"\"Helper function to assign Keras weights to PyTorch model with fused blocks\"\"\"\n",
        "\n",
        "    # Mapping from Keras layer names to PyTorch fused block names\n",
        "    name_mapping = {\n",
        "        'F1': 'conv_bn_relu1',\n",
        "        'BN_1': 'conv_bn_relu1',\n",
        "        'F2': 'conv_bn_relu2',\n",
        "        'BN_2': 'conv_bn_relu2',\n",
        "        'F3': 'conv_bn_relu3',\n",
        "        'BN_3': 'conv_bn_relu3',\n",
        "        'F4': 'conv_bn_relu4',\n",
        "        'BN_4': 'conv_bn_relu4',\n",
        "        'F5': 'conv_bn_relu5',\n",
        "        'BN_5': 'conv_bn_relu5',\n",
        "        'F6': 'conv_bn_relu6',\n",
        "        'BN_6': 'conv_bn_relu6',\n",
        "        'Prediction': 'prediction',\n",
        "    }\n",
        "\n",
        "    model_dict = dict(model.named_modules())\n",
        "    loaded_count = 0\n",
        "\n",
        "    # Load encoder and decoder layers (now fused blocks)\n",
        "    for keras_name, pytorch_name in name_mapping.items():\n",
        "        if keras_name not in keras_weights:\n",
        "            continue\n",
        "\n",
        "        if pytorch_name not in model_dict:\n",
        "            continue\n",
        "\n",
        "        module = model_dict[pytorch_name]\n",
        "        weights = keras_weights[keras_name]\n",
        "\n",
        "        # Check if this is a fused ConvBNReLU block\n",
        "        if hasattr(module, 'conv') and hasattr(module, 'bn'):\n",
        "            # This is a fused block - load into conv and bn sub-modules\n",
        "\n",
        "            # Load Conv2d weights\n",
        "            if 'kernel:0' in weights:\n",
        "                kernel = weights['kernel:0']\n",
        "                kernel_torch = np.transpose(kernel, (3, 2, 0, 1))\n",
        "                module.conv.weight.data = torch.from_numpy(kernel_torch).float().to(device)\n",
        "                loaded_count += 1\n",
        "                if verbose:\n",
        "                    print(f\"  ✓ Loaded {keras_name} -> {pytorch_name}.conv (Conv2d)\")\n",
        "\n",
        "            # Load BatchNorm weights\n",
        "            if 'gamma:0' in weights:\n",
        "                module.bn.weight.data = torch.from_numpy(weights['gamma:0']).float().to(device)\n",
        "            if 'beta:0' in weights:\n",
        "                module.bn.bias.data = torch.from_numpy(weights['beta:0']).float().to(device)\n",
        "            if 'moving_mean:0' in weights:\n",
        "                module.bn.running_mean.data = torch.from_numpy(weights['moving_mean:0']).float().to(device)\n",
        "            if 'moving_variance:0' in weights:\n",
        "                module.bn.running_var.data = torch.from_numpy(weights['moving_variance:0']).float().to(device)\n",
        "\n",
        "            if any(k in weights for k in ['gamma:0', 'beta:0']):\n",
        "                if verbose:\n",
        "                    print(f\"  ✓ Loaded {keras_name} -> {pytorch_name}.bn (BatchNorm)\")\n",
        "\n",
        "        # Load prediction layer (not fused)\n",
        "        elif isinstance(module, nn.Conv2d):\n",
        "            if 'kernel:0' in weights:\n",
        "                kernel = weights['kernel:0']\n",
        "                kernel_torch = np.transpose(kernel, (3, 2, 0, 1))\n",
        "                module.weight.data = torch.from_numpy(kernel_torch).float().to(device)\n",
        "                loaded_count += 1\n",
        "                if verbose:\n",
        "                    print(f\"  ✓ Loaded {keras_name} -> {pytorch_name} (Conv2d)\")\n",
        "\n",
        "            if 'bias:0' in weights and module.bias is not None:\n",
        "                bias = weights['bias:0']\n",
        "                module.bias.data = torch.from_numpy(bias).float().to(device)\n",
        "\n",
        "    # Load upsampling blocks (now with fused conv_bn_relu)\n",
        "    for keras_name in keras_weights.keys():\n",
        "        if 'conv_upsample' in keras_name or 'BN_upsample' in keras_name:\n",
        "            match = re.search(r'(\\d+)', keras_name)\n",
        "            if match:\n",
        "                idx = int(match.group(1)) - 1\n",
        "\n",
        "                if idx >= len(model.upsample_blocks):\n",
        "                    continue\n",
        "\n",
        "                weights = keras_weights[keras_name]\n",
        "\n",
        "                if 'conv_upsample' in keras_name:\n",
        "                    # Access the fused block's conv layer\n",
        "                    fused_block = model.upsample_blocks[idx]['conv_bn_relu']\n",
        "\n",
        "                    if 'kernel:0' in weights and hasattr(fused_block, 'conv'):\n",
        "                        kernel = weights['kernel:0']\n",
        "                        kernel_torch = np.transpose(kernel, (3, 2, 0, 1))\n",
        "                        fused_block.conv.weight.data = torch.from_numpy(kernel_torch).float().to(device)\n",
        "                        loaded_count += 1\n",
        "                        if verbose:\n",
        "                            print(f\"  ✓ Loaded {keras_name} -> upsample_blocks[{idx}]['conv_bn_relu'].conv\")\n",
        "\n",
        "                elif 'BN_upsample' in keras_name:\n",
        "                    # Access the fused block's bn layer\n",
        "                    fused_block = model.upsample_blocks[idx]['conv_bn_relu']\n",
        "\n",
        "                    if hasattr(fused_block, 'bn'):\n",
        "                        if 'gamma:0' in weights:\n",
        "                            fused_block.bn.weight.data = torch.from_numpy(weights['gamma:0']).float().to(device)\n",
        "                        if 'beta:0' in weights:\n",
        "                            fused_block.bn.bias.data = torch.from_numpy(weights['beta:0']).float().to(device)\n",
        "                        if 'moving_mean:0' in weights:\n",
        "                            fused_block.bn.running_mean.data = torch.from_numpy(weights['moving_mean:0']).float().to(\n",
        "                                device)\n",
        "                        if 'moving_variance:0' in weights:\n",
        "                            fused_block.bn.running_var.data = torch.from_numpy(weights['moving_variance:0']).float().to(\n",
        "                                device)\n",
        "                        loaded_count += 1\n",
        "                        if verbose:\n",
        "                            print(f\"  ✓ Loaded {keras_name} -> upsample_blocks[{idx}]['conv_bn_relu'].bn\")\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n✓ Successfully loaded {loaded_count} layer weights\")\n",
        "\n",
        "# ============================================================================\n",
        "# 3. Main Reconstruction Function with Global Profiling\n",
        "# ============================================================================\n",
        "\n",
        "def reconstruct_patches_2025_pytorch(\n",
        "        patches, patch_indices, frame_numbers,\n",
        "        model_num,\n",
        "        num_patches, overlap,\n",
        "        number_of_frames, threshold, neighborhood_size=3,\n",
        "        use_local_avg=True, upsampling_factor=8,\n",
        "        pixel_size=233, batch_size=32, L2_weighting_factor=100,\n",
        "        profiler=None):\n",
        "\n",
        "    profiler.start_timer(\"reconstruction.total\")\n",
        "\n",
        "    pixel_size_hr = pixel_size / upsampling_factor\n",
        "\n",
        "    # Convert patches to float32\n",
        "    device = get_device()\n",
        "    #patches = torch.stack(patches).float().to(device)\n",
        "    patches = patches.float().to(device)\n",
        "\n",
        "    if patches.ndim == 2:\n",
        "        patches = patches.unsqueeze(0)  # Ensure 3D shape\n",
        "    K_frames, M, N = patches.shape\n",
        "\n",
        "    # Determine dimensions of each predicted (cropped) patch\n",
        "    upsampled_patch_h = M * upsampling_factor - 2 * overlap\n",
        "    upsampled_patch_w = N * upsampling_factor - 2 * overlap\n",
        "\n",
        "    # Create full image tensor on GPU\n",
        "    reconstructed_image = torch.zeros((upsampled_patch_h * num_patches, upsampled_patch_w * num_patches), dtype=torch.float32, device=device)\n",
        "\n",
        "    # Prepare lists for detections\n",
        "    recon_xind, recon_yind, frame_index, confidence_list = [], [], [], []\n",
        "\n",
        "    with torch.cuda.device(0):\n",
        "        # Get model from cache\n",
        "        model = get_model(model_num)\n",
        "        model.eval()\n",
        "\n",
        "        # Create the post-processing layer\n",
        "        profiler.start_timer(\"reconstruction.maxima_finder_init\")\n",
        "        max_layer = MaximaFinder(threshold, neighborhood_size, use_local_avg).to(device)\n",
        "        profiler.stop_timer(\"reconstruction.maxima_finder_init\")\n",
        "\n",
        "        # Process in batches\n",
        "        n_batches = int(np.ceil(K_frames / batch_size))\n",
        "\n",
        "        for batch_idx in range(n_batches):\n",
        "            start_idx = batch_idx * batch_size\n",
        "            end_idx = min(K_frames, start_idx + batch_size)\n",
        "            nF = end_idx - start_idx\n",
        "\n",
        "            # --- Move input batch to GPU ---\n",
        "            batch_imgs = patches[start_idx:end_idx].to(device)  # Shape: (nF, M, N)\n",
        "\n",
        "            # add channel dim to match conv2D\n",
        "            batch_imgs = batch_imgs.unsqueeze(1) # Shape: (nF, 1, M, N)\n",
        "\n",
        "            # --- Run prediction on GPU ---\n",
        "            profiler.start_timer(\"reconstruction.model_forward_pass\")\n",
        "            # Enables Automatic Mixed Precision (AMP) for CUDA (use float16 instead of float32)\n",
        "            with torch.no_grad():\n",
        "                with torch.amp.autocast('cuda'):\n",
        "                    predicted_density = model(batch_imgs)\n",
        "            profiler.stop_timer(\"reconstruction.model_forward_pass\")\n",
        "\n",
        "            # Post-processing\n",
        "            predicted_density = torch.relu(predicted_density - 0.5)  # Faster than `predicted_density[predicted_density < 0] = 0`\n",
        "\n",
        "            # Crop off extra overlap\n",
        "            cropped_pred = predicted_density[:, 0, overlap:-overlap, overlap:-overlap]\n",
        "\n",
        "            # --- Post-processing on GPU ---\n",
        "            # Maxima detection\n",
        "            profiler.start_timer(\"reconstruction.maxima_detection\")\n",
        "            bind, xind, yind, conf = max_layer(predicted_density[:, :, overlap:-overlap, overlap:-overlap])\n",
        "\n",
        "            # Convert tensors to NumPy (only when needed)\n",
        "            bind_np = bind.cpu().numpy()\n",
        "            xind_np = xind.cpu().numpy()\n",
        "            yind_np = yind.cpu().numpy()\n",
        "            conf_np = conf.cpu().numpy() / L2_weighting_factor\n",
        "            profiler.stop_timer(\"reconstruction.maxima_detection\")\n",
        "\n",
        "            profiler.start_timer(\"reconstruction.reconstruct_image\")\n",
        "            # --- Place each patch in reconstructed image ---\n",
        "            for i in range(nF):\n",
        "                p_ind = patch_indices[start_idx + i]\n",
        "                y1 = upsampled_patch_h * (p_ind // num_patches)\n",
        "                x1 = upsampled_patch_w * (p_ind % num_patches)\n",
        "\n",
        "                # Use PyTorch addition instead of NumPy\n",
        "                reconstructed_image[y1:y1 + upsampled_patch_h,\n",
        "                    x1:x1 + upsampled_patch_w].add_(cropped_pred[i] / number_of_frames)\n",
        "\n",
        "                # Collect detections (CPU operations)\n",
        "                det_idx = np.where(bind_np == i)[0]\n",
        "                if det_idx.size:\n",
        "                    recon_xind.extend((x1 + xind_np[det_idx]).tolist())\n",
        "                    recon_yind.extend((y1 + yind_np[det_idx]).tolist())\n",
        "                    frame_index.extend([frame_numbers[start_idx + i] + 1] * det_idx.size)\n",
        "                    confidence_list.extend(conf_np[det_idx].tolist())\n",
        "\n",
        "            profiler.stop_timer(\"reconstruction.reconstruct_image\")\n",
        "\n",
        "    # Convert coordinates to physical units\n",
        "    xind_final = (np.array(recon_xind) * pixel_size_hr).tolist()\n",
        "    yind_final = (np.array(recon_yind) * pixel_size_hr).tolist()\n",
        "\n",
        "\n",
        "    profiler.stop_timer(\"reconstruction.total\")\n",
        "\n",
        "    return reconstructed_image, [frame_index, xind_final, yind_final, confidence_list]\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 4. Weight Validation Function\n",
        "# ============================================================================\n",
        "\n",
        "def validate_model_weights(model, verbose=True):\n",
        "    \"\"\"\n",
        "    Validate that model weights are loaded correctly\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model with loaded weights\n",
        "        verbose: Print validation details\n",
        "\n",
        "    Returns:\n",
        "        bool: True if weights appear valid\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"VALIDATING MODEL WEIGHTS\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "    issues = []\n",
        "\n",
        "    # Check encoder/decoder fused blocks\n",
        "    for i in range(1, 7):\n",
        "        block_name = f'conv_bn_relu{i}'\n",
        "        if hasattr(model, block_name):\n",
        "            block = getattr(model, block_name)\n",
        "\n",
        "            # Check conv weights\n",
        "            conv_weights = block.conv.weight.data\n",
        "            if torch.all(conv_weights == 0):\n",
        "                issues.append(f\"{block_name}.conv weights are all zeros\")\n",
        "            elif torch.isnan(conv_weights).any():\n",
        "                issues.append(f\"{block_name}.conv weights contain NaN\")\n",
        "\n",
        "            # Check BN parameters\n",
        "            if torch.all(block.bn.weight.data == 1) and torch.all(block.bn.bias.data == 0):\n",
        "                issues.append(f\"{block_name}.bn parameters are uninitialized (gamma=1, beta=0)\")\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"  {block_name}.conv: shape={tuple(conv_weights.shape)}, \"\n",
        "                      f\"mean={conv_weights.mean().item():.6f}, std={conv_weights.std().item():.6f}\")\n",
        "                print(f\"  {block_name}.bn: gamma_mean={block.bn.weight.mean().item():.6f}, \"\n",
        "                      f\"beta_mean={block.bn.bias.mean().item():.6f}\")\n",
        "\n",
        "    # Check upsampling blocks\n",
        "    if verbose:\n",
        "        print(f\"\\n  Upsampling blocks: {len(model.upsample_blocks)} blocks\")\n",
        "\n",
        "    for idx, block_dict in enumerate(model.upsample_blocks):\n",
        "        fused_block = block_dict['conv_bn_relu']\n",
        "\n",
        "        conv_weights = fused_block.conv.weight.data\n",
        "        expected_kernel_size = 5\n",
        "        actual_kernel_size = conv_weights.shape[2]\n",
        "\n",
        "        if actual_kernel_size != expected_kernel_size:\n",
        "            issues.append(f\"upsample_blocks[{idx}] has {actual_kernel_size}x{actual_kernel_size} kernel, \"\n",
        "                         f\"expected {expected_kernel_size}x{expected_kernel_size}\")\n",
        "\n",
        "        if torch.all(conv_weights == 0):\n",
        "            issues.append(f\"upsample_blocks[{idx}].conv weights are all zeros\")\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"  upsample_blocks[{idx}].conv: shape={tuple(conv_weights.shape)}, \"\n",
        "                  f\"kernel_size={actual_kernel_size}x{actual_kernel_size}, \"\n",
        "                  f\"mean={conv_weights.mean().item():.6f}\")\n",
        "\n",
        "    # Check prediction layer\n",
        "    pred_weights = model.prediction.weight.data\n",
        "    if torch.all(pred_weights == 0):\n",
        "        issues.append(\"prediction layer weights are all zeros\")\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n  prediction: shape={tuple(pred_weights.shape)}, \"\n",
        "              f\"mean={pred_weights.mean().item():.6f}\")\n",
        "\n",
        "    # Report results\n",
        "    if verbose:\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        if issues:\n",
        "            print(\"⚠️ VALIDATION WARNINGS:\")\n",
        "            for issue in issues:\n",
        "                print(f\"  - {issue}\")\n",
        "        else:\n",
        "            print(\"✓ ALL WEIGHTS VALIDATED SUCCESSFULLY\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "    return len(issues) == 0\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "# ============================================================================\n",
        "# Configuration\n",
        "# ============================================================================\n",
        "class Config:\n",
        "    \"\"\"Configuration for AutoDS inference\"\"\"\n",
        "    # Data paths\n",
        "    Data_folder = Data_folder\n",
        "    Result_folder = \"/content/gdrive/MyDrive/AutoDS/Results/TOM20_10nM/V2\"  #@param {type:\"string\"}\n",
        "\n",
        "    # --- Quiet/Preview flags ------------------------------------------------------\n",
        "    QUIET = False  # no training/inference chatter unless set to False\n",
        "    HEADLESS_PREVIEW = True  # set True if you want to see the preview figures\n",
        "\n",
        "    # Detection parameters\n",
        "    threshold = 10 #@param {type:\"number\"}\n",
        "    neighborhood_size = 3 #@param {type:\"integer\"}\n",
        "    use_local_average = True #@param {type:\"boolean\"}\n",
        "\n",
        "    # Patch parameters\n",
        "    num_patches = 8 #@param {type:\"number\"}\n",
        "    overlap = 4 #@param {type:\"number\"}\n",
        "    patch_batch_size = 32 #@param {type:\"number\"}\n",
        "    frame_batch_size = 10  #@param {type:\"number\"}\n",
        "\n",
        "    interpolate_based_on_imaging_parameters = True #@param {type:\"boolean\"}\n",
        "    get_pixel_size_from_file = False #@param {type:\"boolean\"}\n",
        "    pixel_size = 233 #@param {type:\"number\"}\n",
        "    wavelength = 233 #@param {type:\"number\"}\n",
        "    numerical_aperture = 1.49 #@param {type:\"number\"}\n",
        "\n",
        "    chunk_size = 10000 #@param {type:\"number\"}\n",
        "\n",
        "    # Timing parameters\n",
        "    enable_timing = True  # Set to True to enable detailed timing profiling\n",
        "\n",
        "    # Model parameters\n",
        "    use_pytorch_weights = False  # Set to True to use .pth weights, False to use .h5 weights\n",
        "\n",
        "    # Model paths\n",
        "    prediction_model_path = \"/content/AutoDS_models\"\n",
        "    model_names = ['diff_1', 'diff_2', 'diff_3', 'diff_4']\n",
        "\n",
        "    # Model manifest for downloading\n",
        "    MODEL_MANIFEST = {\n",
        "        \"diff_1\": {\n",
        "            \"file_urls\": {\n",
        "                # Add your PyTorch weights URL here if using use_pytorch_weights=True:\n",
        "                # \"best_weights.pth\": \"https://your-url/diff_1/best_weights.pth\",\n",
        "                # Or keep Keras weights if using use_pytorch_weights=False:\n",
        "                \"best_weights.h5\": \"https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/raw/main/AutoDS/models/diff_1/best_weights.h5\",\n",
        "                \"model_metadata.mat\": \"https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/raw/main/AutoDS/models/diff_1/model_metadata.mat\",\n",
        "            },\n",
        "            \"contains\": [\"model_metadata.mat\"]  # Weights file checked based on use_pytorch_weights\n",
        "        },\n",
        "        \"diff_2\": {\n",
        "            \"file_urls\": {\n",
        "                # \"best_weights.pth\": \"https://your-url/diff_2/best_weights.pth\",\n",
        "                \"best_weights.h5\": \"https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/raw/main/AutoDS/models/diff_2/best_weights.h5\",\n",
        "                \"model_metadata.mat\": \"https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/raw/main/AutoDS/models/diff_2/model_metadata.mat\",\n",
        "            },\n",
        "            \"contains\": [\"model_metadata.mat\"]\n",
        "        },\n",
        "        \"diff_3\": {\n",
        "            \"file_urls\": {\n",
        "                # \"best_weights.pth\": \"https://your-url/diff_3/best_weights.pth\",\n",
        "                \"best_weights.h5\": \"https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/raw/main/AutoDS/models/diff_3/best_weights.h5\",\n",
        "                \"model_metadata.mat\": \"https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/raw/main/AutoDS/models/diff_3/model_metadata.mat\",\n",
        "            },\n",
        "            \"contains\": [\"model_metadata.mat\"]\n",
        "        },\n",
        "        \"diff_4\": {\n",
        "            \"file_urls\": {\n",
        "                # \"best_weights.pth\": \"https://your-url/diff_4/best_weights.pth\",\n",
        "                \"best_weights.h5\": \"https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/raw/main/AutoDS/models/diff_4/best_weights.h5\",\n",
        "                \"model_metadata.mat\": \"https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/raw/main/AutoDS/models/diff_4/model_metadata.mat\",\n",
        "            },\n",
        "            \"contains\": [\"model_metadata.mat\"]\n",
        "        },\n",
        "    }\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Global state - simple module-level variables\n",
        "# ============================================================================\n",
        "\n",
        "_models = {}  # Dictionary to store loaded models\n",
        "_device = None  # Device (CPU or CUDA)\n",
        "_is_initialized = False  # Track if we've loaded models\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Simple functions to manage the cache\n",
        "# ============================================================================\n",
        "\n",
        "def initialize_model_cache(config, upsampling_factor, device=None, use_pytorch_weights=False):\n",
        "    \"\"\"\n",
        "    Load all models once and store them in memory\n",
        "\n",
        "    Args:\n",
        "        config: Configuration object with model paths and names\n",
        "        upsampling_factor: Upsampling factor for the models\n",
        "        device: Device to load models on (CPU or CUDA)\n",
        "        use_pytorch_weights: If True, load .pth weights. If False, load .h5 weights\n",
        "    \"\"\"\n",
        "    global _models, _device, _is_initialized\n",
        "\n",
        "    # Skip if already initialized\n",
        "    if _is_initialized:\n",
        "        print(\"⚠️ Models already loaded, skipping initialization\")\n",
        "        return\n",
        "\n",
        "    # Setup device\n",
        "    if device is None:\n",
        "        _device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    else:\n",
        "        _device = device\n",
        "\n",
        "    print(f\"Weight Format: {'PyTorch (.pth)' if use_pytorch_weights else 'Keras (.h5)'}\")\n",
        "\n",
        "    # Determine weight file extension\n",
        "    weight_extension = 'best_weights.pth' if use_pytorch_weights else 'best_weights.h5'\n",
        "    weight_type = \"PyTorch\" if use_pytorch_weights else \"Keras\"\n",
        "\n",
        "    # Load each model\n",
        "    for model_num, model_name in enumerate(config.model_names):\n",
        "        model_path = os.path.join(\n",
        "            config.prediction_model_path,\n",
        "            model_name,\n",
        "            weight_extension\n",
        "        )\n",
        "\n",
        "        if not os.path.exists(model_path):\n",
        "            raise FileNotFoundError(\n",
        "                f\"Model weights not found: {model_path}\\n\"\n",
        "                f\"Expected {weight_type} weights for model: {model_name}\"\n",
        "            )\n",
        "\n",
        "        print(f\"\\nLoading model {model_num + 1}/{len(config.model_names)}: {model_name} ({weight_type} weights)\")\n",
        "\n",
        "        # Create model\n",
        "        model = CNNUpsample(in_channels=1, upsampling_factor=upsampling_factor)\n",
        "        model = model.to(_device)\n",
        "        model.eval()\n",
        "\n",
        "        # Load weights\n",
        "        load_model_weights(model, model_path, verbose=False)\n",
        "\n",
        "        # Optimize for inference\n",
        "        if torch.cuda.is_available():\n",
        "            try:\n",
        "                model = model.to(memory_format=torch.channels_last)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Store in cache\n",
        "        _models[model_num] = model\n",
        "\n",
        "        # Print memory usage\n",
        "        if torch.cuda.is_available():\n",
        "            allocated = torch.cuda.memory_allocated() / 1024**2\n",
        "            print(f\"  ✓ Loaded {model_name} (GPU Memory: {allocated:.1f} MB)\")\n",
        "        else:\n",
        "            print(f\"  ✓ Loaded {model_name}\")\n",
        "\n",
        "    _is_initialized = True\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        total_allocated = torch.cuda.memory_allocated() / 1024**2\n",
        "        print(f\"Total GPU Memory Used: {total_allocated:.1f} MB\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "\n",
        "def get_model(model_num):\n",
        "    \"\"\"Get a cached model by its number\"\"\"\n",
        "    if not _is_initialized:\n",
        "        raise RuntimeError(\"Models not loaded. Call initialize_model_cache() first.\")\n",
        "\n",
        "    if model_num not in _models:\n",
        "        raise KeyError(f\"Model {model_num} not found. Available: {list(_models.keys())}\")\n",
        "\n",
        "    return _models[model_num]\n",
        "\n",
        "\n",
        "def get_device():\n",
        "    \"\"\"Get the device being used (CPU or CUDA)\"\"\"\n",
        "    if _device is None:\n",
        "        raise RuntimeError(\"Model cache not initialized. Call initialize_model_cache() first.\")\n",
        "\n",
        "    return _device\n",
        "\n",
        "\n",
        "def clear_cache():\n",
        "    \"\"\"Clear all cached models from memory\"\"\"\n",
        "    global _models, _device, _is_initialized\n",
        "\n",
        "    print(\"\\n⚠️ Clearing model cache...\")\n",
        "\n",
        "    # Move models to CPU and delete\n",
        "    for model in _models.values():\n",
        "        model.cpu()\n",
        "\n",
        "    _models.clear()\n",
        "    _device = None\n",
        "    _is_initialized = False\n",
        "\n",
        "    # Clear GPU memory\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"✓ Model cache cleared\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Compatibility wrapper (optional - for backwards compatibility)\n",
        "# ============================================================================\n",
        "\n",
        "class ModelCacheManager:\n",
        "    \"\"\"Simple wrapper class for compatibility with existing code\"\"\"\n",
        "\n",
        "    def initialize(self, config, upsampling_factor, device=None, use_pytorch_weights=False):\n",
        "        initialize_model_cache(config, upsampling_factor, device, use_pytorch_weights)\n",
        "\n",
        "    def get_model(self, model_num):\n",
        "        return get_model(model_num)\n",
        "\n",
        "    def get_device(self):\n",
        "        return get_device()\n",
        "\n",
        "    def clear_cache(self):\n",
        "        clear_cache()\n",
        "\n",
        "\n",
        "def get_model_cache():\n",
        "    \"\"\"Return a simple manager instance for compatibility\"\"\"\n",
        "    return ModelCacheManager()\n",
        "\n",
        "\n",
        "# Setup\n",
        "config = Config()\n",
        "\n",
        "# ============================================================================\n",
        "# Entry Point\n",
        "# ============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    if device.type != 'cuda':\n",
        "        log('You do not have GPU access.')\n",
        "        log('Did you change your runtime?')\n",
        "        log('If the runtime settings are correct then GPU might not be allocated to your session.')\n",
        "        log('Expect slow performance. To access GPU try reconnecting later.')\n",
        "    else:\n",
        "        log('You have GPU access')\n",
        "        log('PyTorch version is ' + str(torch.__version__)+'\\n')\n",
        "\n",
        "    # Initialize timing profiler\n",
        "    profiler = timing_profiler(enabled=config.enable_timing)\n",
        "\n",
        "    config.prediction_model_path = ensure_models(config.model_names, target_root=config.prediction_model_path,\n",
        "                                                 model_manifest=config.MODEL_MANIFEST)\n",
        "\n",
        "    MAX_FILE_GB = 5.0  # warn & skip when file is larger than this\n",
        "\n",
        "    # PSF parameters\n",
        "    psf_sigma_nm = 0.21 * config.wavelength / config.numerical_aperture\n",
        "    psf_sigma_pixels = psf_sigma_nm / config.pixel_size\n",
        "\n",
        "    if config.get_pixel_size_from_file:\n",
        "        pixel_size = None\n",
        "\n",
        "    # Load model metadata\n",
        "    matfile = sio.loadmat(os.path.join(config.prediction_model_path, config.model_names[0], 'model_metadata.mat'))\n",
        "    try:\n",
        "        model_wavelength = np.array(matfile['wavelength'].item())\n",
        "    except:\n",
        "        model_wavelength = None\n",
        "    try:\n",
        "        model_NA = np.array(matfile['numerical_aperture'].item())\n",
        "    except:\n",
        "        model_NA = None\n",
        "    try:\n",
        "        model_pixel_size = np.array(matfile['pixel_size'].item())\n",
        "    except:\n",
        "        model_pixel_size = None\n",
        "\n",
        "    if os.path.isdir(config.Data_folder):\n",
        "        # iterate both TIFF and ND2\n",
        "        for filename in list_files_multi(config.Data_folder, extensions=['tif', 'tiff', 'nd2']):\n",
        "\n",
        "            # Install nd2 reader only when needed (optional)\n",
        "            if filename.lower().endswith('.nd2'):\n",
        "                try:\n",
        "                    import nd2  # already installed?\n",
        "                except Exception:\n",
        "                    # Jupyter-only magic; remove if not on Colab\n",
        "                    # get_ipython().system('pip install -q nd2')\n",
        "                    import subprocess\n",
        "                    import sys\n",
        "\n",
        "                    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'nd2'])\n",
        "                    import nd2  # now import it\n",
        "\n",
        "            in_path = os.path.join(config.Data_folder, filename)\n",
        "\n",
        "            # --------- file size guard ----------\n",
        "            try:\n",
        "                file_size_gb = os.path.getsize(in_path) / 1e9\n",
        "                if file_size_gb > MAX_FILE_GB:\n",
        "                    print(f\"\\n⚠️  {filename}: {file_size_gb:.2f} GB > {MAX_FILE_GB:.2f} GB.\")\n",
        "                    print(\"   Video size is too big, please use Google Colab Pro or run locally.\")\n",
        "                    continue\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            # --- Resolve pixel size if requested ---\n",
        "            if config.get_pixel_size_from_file:\n",
        "                if is_tiff(in_path):\n",
        "                    with catch_oom(\"reading TIFF pixel size\", filename):\n",
        "                        pixel_size, _, _ = getPixelSizeTIFFmetadata(in_path, True)\n",
        "                elif is_nd2(in_path):\n",
        "                    with catch_oom(\"reading ND2 pixel size\", filename):\n",
        "                        px_nm, _, _ = getPixelSizeND2metadata(in_path, True)\n",
        "                        pixel_size = px_nm if px_nm is not None else pixel_size  # leave unchanged if unknown\n",
        "\n",
        "            # --- Common model params ---\n",
        "            upsampling_factor = np.array(matfile['upsampling_factor']).item()\n",
        "            try:\n",
        "                L2_weighting_factor = np.array(matfile['Normalization factor']).item()\n",
        "            except:\n",
        "                L2_weighting_factor = 100\n",
        "\n",
        "            # save all models to cache\n",
        "            initialize_model_cache(config, upsampling_factor, device,\n",
        "                                   use_pytorch_weights=config.use_pytorch_weights)\n",
        "\n",
        "            # --- Choose reader & frame count ---\n",
        "            number_of_frames, frame_iter = None, None\n",
        "            with catch_oom(\"opening stack\", filename):\n",
        "                if is_tiff(in_path):\n",
        "                    number_of_frames = count_tiff_frames(in_path)\n",
        "                    frame_iter = iter_tiff_frames(in_path)\n",
        "                    log(f'\\nLoaded tiff stack with {number_of_frames} frames')\n",
        "                elif is_nd2(in_path):\n",
        "                    number_of_frames = count_nd2_frames(in_path)\n",
        "                    frame_iter = iter_nd2_frames(in_path)\n",
        "                    log(f'\\nLoaded ND2 stack with ~{number_of_frames} planes (T*Z*C)')\n",
        "                else:\n",
        "                    log(f\"Skipping unsupported file: {filename}\")\n",
        "\n",
        "            if frame_iter is None:\n",
        "                print(f\"⚠️  Skipping {filename} due to earlier error.\")\n",
        "                continue\n",
        "\n",
        "            # Initialize patch lists for each model\n",
        "            patches_list = [[] for _ in config.model_names]\n",
        "            patch_indices_list = [[] for _ in config.model_names]\n",
        "            frame_numbers = [[] for _ in config.model_names]\n",
        "\n",
        "            # Initialize accumulator variables\n",
        "            M, N = None, None\n",
        "            sum_image = None\n",
        "            patchwise_recon = None\n",
        "            frame_number_list, x_nm_list, y_nm_list, confidence_au_list = [], [], [], []\n",
        "            total_selected_model_hist = np.zeros(len(config.model_names), dtype=float)\n",
        "\n",
        "            # Progress bar for overall process\n",
        "            pbar = tqdm(total=number_of_frames, desc=\"Processing frames\")\n",
        "\n",
        "            for frame_start in range(0, number_of_frames, config.frame_batch_size):\n",
        "                frame_end = min(frame_start + config.frame_batch_size, number_of_frames)\n",
        "\n",
        "                # Collect patches from multiple frames\n",
        "                all_valid_patches = []\n",
        "                all_full_patches = []\n",
        "                all_patches_local_indices = []\n",
        "                all_frames_numbers = []\n",
        "                all_patches_offset = []\n",
        "\n",
        "                # for DEBUG\n",
        "                fproc_list = []\n",
        "                offset_list = []\n",
        "\n",
        "                frames_list = []\n",
        "                # Process frames in this batch\n",
        "\n",
        "                for frame_idx in range(frame_start, frame_end):\n",
        "                    # Start preprocessing timing for EACH frame\n",
        "                    profiler.start_timer(\"preprocessing.frame_reading_and_splitting\")\n",
        "\n",
        "                    # Get frame\n",
        "                    frame_i = next(frame_iter)\n",
        "\n",
        "                    # Initialize sum_image and dimensions on first frame\n",
        "                    if sum_image is None:\n",
        "                        sum_image = np.zeros_like(frame_i, dtype=np.float32)\n",
        "\n",
        "                        # Interpolate first to get actual dimensions\n",
        "                        if config.interpolate_based_on_imaging_parameters:\n",
        "                            temp_frame = interpolate_frames(\n",
        "                                frame_i,\n",
        "                                model_pixel_size, config.pixel_size,\n",
        "                                model_wavelength, config.wavelength,\n",
        "                                model_NA, config.numerical_aperture\n",
        "                            )[0]\n",
        "                            M, N = temp_frame.shape\n",
        "                        else:\n",
        "                            M, N = frame_i.shape\n",
        "\n",
        "                    # Accumulate for preview\n",
        "                    sum_image += frame_i.astype(np.float32) / number_of_frames\n",
        "\n",
        "                    # Interpolate frame\n",
        "                    if config.interpolate_based_on_imaging_parameters:\n",
        "                        frame_i = interpolate_frames(\n",
        "                            frame_i,\n",
        "                            model_pixel_size, config.pixel_size,\n",
        "                            model_wavelength, config.wavelength,\n",
        "                            model_NA, config.numerical_aperture\n",
        "                        )[0]\n",
        "                    frames_list.append(frame_i)\n",
        "\n",
        "                # Preprocess on GPU\n",
        "                frames_torch = torch.from_numpy(np.array(frames_list)).float().to(device)\n",
        "                fproc_tensor, frames_offsets = preprocess_frames_batch(frames_torch, device)\n",
        "                #fproc_tensor, frames_offsets = preprocess_frames_batch(frames_tensor, device)\n",
        "\n",
        "\n",
        "                # Split all frames to patches (GPU)\n",
        "                all_patches_tensor = split_image_to_patches_batch(\n",
        "                    fproc_tensor,\n",
        "                    config.num_patches,\n",
        "                    config.overlap,\n",
        "                    device=device\n",
        "                )\n",
        "\n",
        "                for frame_idx in range(frame_start, frame_end):\n",
        "                    #fproc = fproc_list[frame_idx - frame_start]\n",
        "                    offset = frames_offsets[frame_idx - frame_start].cpu().item()\n",
        "                    #fproc = fproc_tensor[frame_idx - frame_start].cpu().numpy()\n",
        "                    # Split into patches\n",
        "                    patches = all_patches_tensor[frame_idx - frame_start]\n",
        "\n",
        "                    # Process each patch\n",
        "                    for m in range(config.num_patches):\n",
        "                        for n in range(config.num_patches):\n",
        "                            down = config.overlap if m == 0 else 0\n",
        "                            up = (M // config.num_patches) - config.overlap if m == config.num_patches - 1 else (\n",
        "                                    M // config.num_patches)\n",
        "                            left = config.overlap if n == 0 else 0\n",
        "                            right = (N // config.num_patches) - config.overlap if n == config.num_patches - 1 else (\n",
        "                                    N // config.num_patches)\n",
        "\n",
        "                            local_patch_idx = m * config.num_patches + n\n",
        "                            full_patch = patches[local_patch_idx]\n",
        "                            valid_patch = full_patch[down:up, left:right]\n",
        "\n",
        "                            all_full_patches.append(full_patch)\n",
        "                            all_valid_patches.append(valid_patch)\n",
        "                            all_patches_local_indices.append(local_patch_idx)\n",
        "                            all_patches_offset.append(offset)\n",
        "                            all_frames_numbers.append(frame_idx)\n",
        "\n",
        "\n",
        "                    pbar.update(1)\n",
        "\n",
        "                profiler.stop_timer(\"preprocessing.frame_reading_and_splitting\")\n",
        "                profiler.start_timer(\"preprocessing.feature_extraction\")\n",
        "\n",
        "                # Group patches by size and extract features\n",
        "                shape_groups = defaultdict(lambda: {'patches': [], 'indices': [], 'offsets': []})\n",
        "\n",
        "                for idx, patch in enumerate(all_valid_patches):\n",
        "                    shape = patch.shape\n",
        "                    shape_groups[shape]['patches'].append(patch)\n",
        "                    shape_groups[shape]['indices'].append(idx)\n",
        "                    shape_groups[shape]['offsets'].append(all_patches_offset[idx])\n",
        "\n",
        "                # Process each size group\n",
        "                all_features = []\n",
        "\n",
        "                for shape, group_data in shape_groups.items():\n",
        "                    patches_tensor  = torch.stack(group_data['patches'])\n",
        "                    offsets_array = np.array(group_data['offsets'])\n",
        "\n",
        "                    features_batch = extract_features_batch(\n",
        "                        patches_tensor,\n",
        "                        config.pixel_size,\n",
        "                        psf_sigma_pixels,\n",
        "                        offsets_array,\n",
        "                        verbose=False,\n",
        "                        device=device\n",
        "                    )\n",
        "\n",
        "                    for feat, idx in zip(features_batch, group_data['indices']):\n",
        "                        all_features.append((feat, idx))\n",
        "\n",
        "                profiler.stop_timer(\"preprocessing.feature_extraction\")\n",
        "                profiler.start_timer(\"preprocessing.patch_classification\")\n",
        "\n",
        "                # Classify and accumulate patches for reconstruction\n",
        "                for features, idx in all_features:\n",
        "                    curr_mean_noise, curr_std_noise, signal_amp, curr_emitter_density = features\n",
        "\n",
        "                    # Skip invalid patches\n",
        "                    if signal_amp == 0 or curr_mean_noise == 0:\n",
        "                        continue\n",
        "                    if any(np.isnan(v) for v in (signal_amp, curr_mean_noise, curr_std_noise, curr_emitter_density)):\n",
        "                        continue\n",
        "\n",
        "                    # Choose difficulty level\n",
        "                    difficulty_choice = ChooseNetByDifficulty_2025(\n",
        "                        curr_emitter_density,\n",
        "                        signal_amp / curr_mean_noise\n",
        "                    )\n",
        "\n",
        "                    # Store patch data\n",
        "                    patches_list[difficulty_choice].append(all_full_patches[idx])\n",
        "                    patch_indices_list[difficulty_choice].append(all_patches_local_indices[idx])\n",
        "                    frame_numbers[difficulty_choice].append(all_frames_numbers[idx])\n",
        "\n",
        "                profiler.stop_timer(\"preprocessing.patch_classification\")\n",
        "\n",
        "                # Initialize reconstruction array on first batch and move it to the GPU\n",
        "                if patchwise_recon is None:\n",
        "                    M, N = fproc_tensor.shape[1], fproc_tensor.shape[2]\n",
        "                    patchwise_recon = torch.zeros(M * upsampling_factor, N * upsampling_factor,\n",
        "                                                  dtype=torch.float32, device=device)\n",
        "\n",
        "                # Check if there are any patches to process\n",
        "                total_patches = sum(len(patches) for patches in patches_list)\n",
        "                if total_patches > 0:\n",
        "                    # Process with each model\n",
        "                    for model_num, model_name in enumerate(config.model_names):\n",
        "                        if not patches_list[model_num]:\n",
        "                            continue\n",
        "\n",
        "                        # Process in chunks\n",
        "                        t_chunks = (len(patches_list[model_num]) // config.chunk_size) + 1\n",
        "\n",
        "                        for chunk_num in range(t_chunks):\n",
        "                            chunk_start = chunk_num * config.chunk_size\n",
        "                            chunk_end = min((chunk_num + 1) * config.chunk_size, len(patches_list[model_num]))\n",
        "\n",
        "                            if chunk_start >= chunk_end:\n",
        "                                continue\n",
        "\n",
        "                            # Reconstruct using CACHED model\n",
        "                            pw_recon, loc_list = reconstruct_patches_2025_pytorch(\n",
        "                                torch.stack(patches_list[model_num][chunk_start:chunk_end]),\n",
        "                                patch_indices_list[model_num][chunk_start:chunk_end],\n",
        "                                frame_numbers[model_num][chunk_start:chunk_end],\n",
        "                                model_num,\n",
        "                                config.num_patches,\n",
        "                                config.overlap * upsampling_factor,\n",
        "                                number_of_frames,\n",
        "                                config.threshold,\n",
        "                                neighborhood_size=config.neighborhood_size,\n",
        "                                use_local_avg=config.use_local_average,\n",
        "                                upsampling_factor=upsampling_factor,\n",
        "                                pixel_size=config.pixel_size,\n",
        "                                batch_size=config.patch_batch_size,\n",
        "                                L2_weighting_factor=L2_weighting_factor,\n",
        "                                profiler=profiler\n",
        "                            )\n",
        "\n",
        "                            # Accumulate results\n",
        "                            frame_number_list.extend(loc_list[0])\n",
        "                            x_nm_list.extend(loc_list[1])\n",
        "                            y_nm_list.extend(loc_list[2])\n",
        "                            confidence_au_list.extend(loc_list[3])\n",
        "\n",
        "                            patchwise_recon[:M // config.num_patches * upsampling_factor * config.num_patches,\n",
        "                                            :N // config.num_patches * upsampling_factor * config.num_patches] += pw_recon\n",
        "\n",
        "                    # Clear patches lists after processing to free memory\n",
        "                    for i in range(len(patches_list)):\n",
        "                        patches_list[i].clear()\n",
        "                        patch_indices_list[i].clear()\n",
        "                        frame_numbers[i].clear()\n",
        "\n",
        "                    ## Force garbage collection\n",
        "                    #if torch.cuda.is_available():\n",
        "                    #    torch.cuda.empty_cache()\n",
        "\n",
        "            # close progress bar\n",
        "            pbar.close()\n",
        "\n",
        "            if not os.path.exists(config.Result_folder):\n",
        "                print('Result folder was created.')\n",
        "                os.makedirs(config.Result_folder, exist_ok=True)\n",
        "\n",
        "            print(f\"\\n{'=' * 70}\")\n",
        "            print(f\"Streaming processing complete for {filename}\")\n",
        "            print(f\"Total localizations found: {len(frame_number_list)}\")\n",
        "            print(f\"{'=' * 70}\")\n",
        "\n",
        "            # Save results\n",
        "            os.makedirs(config.Result_folder, exist_ok=True)\n",
        "            ext = '_avg' if config.use_local_average else '_max'\n",
        "            base = os.path.splitext(filename)[0]\n",
        "\n",
        "            # Save localizations\n",
        "            with open(os.path.join(config.Result_folder, f'Localizations_{base}{ext}.csv'), \"w\", newline='') as file:\n",
        "                writer = csv.writer(file)\n",
        "                writer.writerow(['frame', 'x [nm]', 'y [nm]', 'confidence [a.u]'])\n",
        "                sort_ind = np.argsort(frame_number_list)\n",
        "                locs = list(zip(\n",
        "                    list(np.array(frame_number_list)[sort_ind]),\n",
        "                    list(np.array(x_nm_list)[sort_ind]),\n",
        "                    list(np.array(y_nm_list)[sort_ind]),\n",
        "                    list(np.array(confidence_au_list)[sort_ind])\n",
        "                ))\n",
        "                writer.writerows(locs)\n",
        "\n",
        "            print(f\"Saved {len(frame_number_list)} localizations\")\n",
        "\n",
        "            # move image to CPU for saving\n",
        "            patchwise_recon = patchwise_recon.cpu().numpy()\n",
        "\n",
        "            # Save reconstruction\n",
        "            pw_recon_tif = np.copy(patchwise_recon)\n",
        "            cap = np.percentile(pw_recon_tif, 99.5)\n",
        "            pw_recon_tif[pw_recon_tif > cap] = cap\n",
        "            saveAsTIF(config.Result_folder, f\"Predicted_patchwise_{base}\", pw_recon_tif, config.pixel_size / upsampling_factor)\n",
        "\n",
        "            # Create preview\n",
        "            fig, axes = plt.subplots(1, 3, figsize=(20, 16))\n",
        "            axes[0].axis('off')\n",
        "            axes[0].imshow(sum_image)\n",
        "            axes[0].set_title('Original', fontsize=15)\n",
        "            axes[1].axis('off')\n",
        "            axes[1].imshow(patchwise_recon)\n",
        "            axes[1].set_title('Prediction', fontsize=15)\n",
        "            axes[2].axis('off')\n",
        "            axes[2].imshow(np.clip(patchwise_recon,\n",
        "                                   np.percentile(patchwise_recon, 1),\n",
        "                                   np.percentile(patchwise_recon, 99)))\n",
        "            axes[2].set_title('Normalized Prediction', fontsize=15)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(config.Result_folder, f'preview_{base}.png'), dpi=150)\n",
        "            plt.close()\n",
        "\n",
        "            print(f\"\\nCompleted processing: {filename}\")\n",
        "\n",
        "            # Print timing summary at the end\n",
        "            profiler.print_timing_summary()\n",
        "\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "# Clear GPU memory\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.synchronize()\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "di0hVAvzawp9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}