{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Itamar-Horowitz/real-time-AutoDS/blob/main/AutoDS/Realtime_AutoDS_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpCtYevLHfl4"
      },
      "source": [
        "# **AutoDS**\n",
        "\n",
        "---\n",
        "\n",
        "<font size = 4> Deep-STORM is a neural network capable of image reconstruction from high-density single-molecule localization microscopy (SMLM), first published in 2018 by [Nehme *et al.* in Optica](https://www.osapublishing.org/optica/abstract.cfm?uri=optica-5-4-458). The architecture used here is a U-Net based network without skip connections. This network allows image reconstruction of 2D super-resolution images, in a supervised training manner. The network is trained using simulated high-density SMLM data for which the ground-truth is available. These simulations are obtained from random distribution of single molecules in a field-of-view and therefore do not imprint structural priors during training. The network output a super-resolution image with increased pixel density (typically upsampling factor of 8 in each dimension).\n",
        "\n",
        "<font size = 4> AutoDS is an extension of Deep-STORM automating the reconstruction process and aleviating the need in human intervension. This is done by automatic detection of the experimental condition in the analyzed videos and automatic selection of a Deep-STORM model out of a set of pre-trained model for the data processing.\n",
        "\n",
        "<font size = 4> Additionally, AutoDS pipeline splits each input frame into patches and enables processing of different regions in the field-of-view with different models. This mechanism led to an improvment in the reconstruction quality beyond the capabilities of Deep-STORM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEy4EBXHHyAX"
      },
      "source": [
        "# **Before getting started**\n",
        "---\n",
        "<font size = 4> This notebook contains the code required only for inference of SMLM data using a set of pre-trained Deep-STORM models. For model training please follow this [link](https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/blob/main/AutoDS/AutoDS_training.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OlaDqH75fdK"
      },
      "source": [
        "# **Run configuration**\n",
        "---\n",
        "<font size = 4>**`Data_folder`:** This folder should contain the images that you want to use your trained network on for processing.\n",
        "\n",
        "<font size = 4>**`Result_folder`:** This folder will contain the found localizations csv.\n",
        "\n",
        "<font size = 4>**`threshold`:** This paramter determines threshold for local maxima finding. A higher `threshold` will result in less localizations. **DEFAULT: 10**\n",
        "\n",
        "<font size = 4>**`neighborhood_size`:** This paramter determines size of the neighborhood within which the prediction needs to be a local maxima in recovery pixels (CCD pixel/upsampling_factor). A high `neighborhood_size` will make the prediction slower and potentially discard nearby localizations. **DEFAULT: 3**\n",
        "\n",
        "<font size = 4>**`use_local_average`:** This paramter determines whether to locally average the prediction in a 3x3 neighborhood to get the final localizations. If set to **True** it will make inference slightly slower depending on the size of the FOV. **DEFAULT: True**\n",
        "\n",
        "<font size = 4>**`num_patches`:** Determines the number of patches in each row and each column after splitting the frames to patches. The total number of patches will be num_patches<sup>2</sup>. **DEFAULT: 4**\n",
        "\n",
        "<font size = 4>**`batch_size`:** This paramter determines how many frames are processed by any single pass on the GPU. A higher `batch_size` will make the prediction faster but will use more GPU memory. If an OutOfMemory (OOM) error occurs, decrease the `batch_size`. **DEFAULT: 1**\n",
        "\n",
        "<font size = 4>**The following parameters are relevant only if `interpolate_based_on_imaging_parameters` is checked:**\n",
        "\n",
        "<font size = 4> - **`pixel_size` [nm]:** the pixels size of the analyzed video. **DEFAULT: 107**\n",
        "\n",
        "<font size = 4> - **`wavelength` [nm]:** the emission wavelength of the analyzed video. **DEFAULT: 715**\n",
        "\n",
        "<font size = 4> - **`numerical_aperture`:** the optical setup numerical aperture of the analyzed video. **DEFAULT: 1.49**\n",
        "\n",
        "<font size = 4> - **`chunk_size`:** determine the number of patches that will be analyzed in each prediction iteration. This parameter is used for managing compute resources in Google Colab. If you are facing crashes due to RAM memory limitation, decrease the number of patches per chunk. **DEFAULT: 10000**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download testing flie"
      ],
      "metadata": {
        "id": "IZ8Q0-tbf1cB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Data_folder = \"https://github.com/Itamar-Horowitz/real-time-AutoDS/tree/41c8f1af1c8e513cffc314e496424f642f0ddf92/dataset/TOM20_10nM/1\" #@param {type:\"string\"}\n",
        "\n",
        "def download_github_directory(repo_url, local_path, branch='main'):\n",
        "    \"\"\"\n",
        "    Download all files from a GitHub directory\n",
        "    Uses git clone with LFS support as primary method\n",
        "\n",
        "    Args:\n",
        "        repo_url: GitHub directory URL (e.g., https://github.com/user/repo/tree/branch/path/to/dir)\n",
        "        local_path: Local directory to save files\n",
        "        branch: Git branch name (default: 'main')\n",
        "    \"\"\"\n",
        "    # Parse the GitHub URL\n",
        "    parts = repo_url.split('github.com/')[-1].split('/')\n",
        "    if len(parts) < 5:\n",
        "        raise ValueError(\"Invalid GitHub directory URL\")\n",
        "\n",
        "    user = parts[0]\n",
        "    repo = parts[1]\n",
        "\n",
        "    # Find where the path starts (after 'tree' and branch/commit)\n",
        "    if 'tree' in parts:\n",
        "        tree_idx = parts.index('tree')\n",
        "        path_parts = parts[tree_idx + 2:]  # Skip 'tree' and branch/commit\n",
        "        dir_path = '/'.join(path_parts)\n",
        "    else:\n",
        "        dir_path = '/'.join(parts[3:])\n",
        "\n",
        "    # Get the commit/branch from URL\n",
        "    if 'tree' in parts:\n",
        "        commit = parts[parts.index('tree') + 1]\n",
        "    else:\n",
        "        commit = branch\n",
        "\n",
        "    # Method 2: Fall back to API-based download\n",
        "    print(\"\\nStart downloading data file\")\n",
        "\n",
        "    api_url = f\"https://api.github.com/repos/{user}/{repo}/contents/{dir_path}?ref={commit}\"\n",
        "\n",
        "    try:\n",
        "        req = urllib.request.Request(api_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "        with urllib.request.urlopen(req) as response:\n",
        "            files_data = json.loads(response.read().decode())\n",
        "    except Exception as e:\n",
        "        return download_tiff_files_fallback(user, repo, commit, dir_path, local_path)\n",
        "\n",
        "    os.makedirs(local_path, exist_ok=True)\n",
        "\n",
        "    downloaded_files = []\n",
        "    for item in files_data:\n",
        "        if item['type'] == 'file':\n",
        "            file_name = item['name']\n",
        "\n",
        "            # Download TIFF, TIF, and ND2 files\n",
        "            if file_name.lower().endswith(('.tif', '.tiff', '.nd2')):\n",
        "                # Use raw.githubusercontent.com for binary files\n",
        "                file_url = f\"https://raw.githubusercontent.com/{user}/{repo}/{commit}/{dir_path}/{file_name}\"\n",
        "                dest_path = os.path.join(local_path, file_name)\n",
        "\n",
        "                try:\n",
        "                    download_github_file(file_url, dest_path, commit_or_branch=commit)\n",
        "                    downloaded_files.append(dest_path)\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to download {file_name}: {e}\")\n",
        "                    # Try using download_url from API as fallback\n",
        "                    if 'download_url' in item and item['download_url']:\n",
        "                        try:\n",
        "                            download_github_file(item['download_url'], dest_path, commit_or_branch=commit)\n",
        "                            downloaded_files.append(dest_path)\n",
        "                        except Exception as e2:\n",
        "                            print(f\"Also failed with API URL: {e2}\")\n",
        "\n",
        "    return downloaded_files\n",
        "\n",
        "# ============================================================================\n",
        "# DOWNLOAD DATA FROM GITHUB\n",
        "# ============================================================================\n",
        "if Data_folder_URL.startswith('http'):\n",
        "    downloaded_files = download_github_directory(Data_folder_URL, Data_folder)\n",
        "else:\n",
        "    log(\"Using local data folder (not downloading from GitHub)\")\n",
        "    Data_folder = Data_folder_URL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiOGc41hefqQ",
        "outputId": "c6565cce-81d2-4ebd-b4a6-e99f57fea0d8"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start downloading data file\n",
            "Saved to: https://github.com/Itamar-Horowitz/real-time-AutoDS/tree/41c8f1af1c8e513cffc314e496424f642f0ddf92/dataset/TOM20_10nM/1/High-density-frames_Image1_TOM20_10nM.tif (200.06 MB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRnQZWSZhArJ"
      },
      "source": [
        "# **V1: Original TensorFlow Version**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "kSrZMo3X_NhO",
        "outputId": "b252f4ca-6a11-4be7-e515-ed159fb9cf4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[models] found: diff_1\n",
            "[models] found: diff_2\n",
            "[models] found: diff_3\n",
            "[models] found: diff_4\n",
            "--------------------------------\n",
            "AutoDS installation complete.\n",
            "You have GPU access\n",
            "Tensorflow version is 2.19.0\n",
            "\n",
            "Processing: High-density-frames_Image1_TOM20_10nM.tif (0.21 GB)\n",
            "Loaded tiff stack with 400 frames\n",
            "Splitting stack to patches and selecting Deep-STORM model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
            "  0%|          | 1/400 [00:00<01:12,  5.51it/s]\u001b[A\n",
            "  0%|          | 2/400 [00:00<01:12,  5.47it/s]\u001b[A\n",
            "  1%|          | 3/400 [00:00<01:12,  5.48it/s]\u001b[A\n",
            "  1%|          | 4/400 [00:00<01:13,  5.35it/s]\u001b[A\n",
            "  1%|▏         | 5/400 [00:00<01:17,  5.07it/s]\u001b[A\n",
            "  2%|▏         | 6/400 [00:01<01:16,  5.15it/s]\u001b[A\n",
            "  2%|▏         | 7/400 [00:01<01:14,  5.26it/s]\u001b[A\n",
            "  2%|▏         | 8/400 [00:01<01:13,  5.32it/s]\u001b[A\n",
            "  2%|▏         | 9/400 [00:01<01:13,  5.31it/s]\u001b[A\n",
            "  2%|▎         | 10/400 [00:01<01:12,  5.37it/s]\u001b[A\n",
            "  3%|▎         | 11/400 [00:02<01:17,  5.02it/s]\u001b[A\n",
            "  3%|▎         | 12/400 [00:02<01:16,  5.07it/s]\u001b[A\n",
            "  3%|▎         | 13/400 [00:02<01:18,  4.96it/s]\u001b[A\n",
            "  4%|▎         | 14/400 [00:02<01:18,  4.89it/s]\u001b[A\n",
            "  4%|▍         | 15/400 [00:02<01:19,  4.87it/s]\u001b[A\n",
            "  4%|▍         | 16/400 [00:03<01:16,  4.99it/s]\u001b[A\n",
            "  4%|▍         | 17/400 [00:03<01:16,  4.99it/s]\u001b[A\n",
            "  4%|▍         | 18/400 [00:03<01:14,  5.11it/s]\u001b[A\n",
            "  5%|▍         | 19/400 [00:03<01:10,  5.39it/s]\u001b[A\n",
            "  5%|▌         | 20/400 [00:03<01:03,  5.98it/s]\u001b[A\n",
            "  5%|▌         | 21/400 [00:03<00:59,  6.42it/s]\u001b[A\n",
            "  6%|▌         | 22/400 [00:04<00:55,  6.85it/s]\u001b[A\n",
            "  6%|▌         | 23/400 [00:04<00:52,  7.16it/s]\u001b[A\n",
            "  6%|▌         | 24/400 [00:04<00:52,  7.20it/s]\u001b[A\n",
            "  6%|▋         | 25/400 [00:04<00:50,  7.38it/s]\u001b[A\n",
            "  6%|▋         | 26/400 [00:04<00:49,  7.53it/s]\u001b[A\n",
            "  7%|▋         | 27/400 [00:04<00:49,  7.61it/s]\u001b[A\n",
            "  7%|▋         | 28/400 [00:04<00:48,  7.70it/s]\u001b[A\n",
            "  7%|▋         | 29/400 [00:04<00:48,  7.66it/s]\u001b[A\n",
            "  8%|▊         | 30/400 [00:05<00:47,  7.73it/s]\u001b[A\n",
            "  8%|▊         | 31/400 [00:05<00:47,  7.76it/s]\u001b[A\n",
            "  8%|▊         | 32/400 [00:05<00:48,  7.52it/s]\u001b[A\n",
            "  8%|▊         | 33/400 [00:05<00:48,  7.55it/s]\u001b[A\n",
            "  8%|▊         | 34/400 [00:05<00:48,  7.62it/s]\u001b[A\n",
            "  9%|▉         | 35/400 [00:05<00:47,  7.61it/s]\u001b[A\n",
            "  9%|▉         | 36/400 [00:05<00:49,  7.33it/s]\u001b[A\n",
            "  9%|▉         | 37/400 [00:06<00:49,  7.37it/s]\u001b[A\n",
            " 10%|▉         | 38/400 [00:06<00:48,  7.50it/s]\u001b[A\n",
            " 10%|▉         | 39/400 [00:06<00:49,  7.35it/s]\u001b[A\n",
            " 10%|█         | 40/400 [00:06<00:48,  7.38it/s]\u001b[A\n",
            " 10%|█         | 41/400 [00:06<00:47,  7.54it/s]\u001b[A\n",
            " 10%|█         | 42/400 [00:06<00:46,  7.62it/s]\u001b[A\n",
            " 11%|█         | 43/400 [00:06<00:46,  7.64it/s]\u001b[A\n",
            " 11%|█         | 44/400 [00:06<00:46,  7.68it/s]\u001b[A\n",
            " 11%|█▏        | 45/400 [00:07<00:45,  7.72it/s]\u001b[A\n",
            " 12%|█▏        | 46/400 [00:07<00:45,  7.78it/s]\u001b[A\n",
            " 12%|█▏        | 47/400 [00:07<00:48,  7.32it/s]\u001b[A\n",
            " 12%|█▏        | 48/400 [00:07<00:51,  6.85it/s]\u001b[A\n",
            " 12%|█▏        | 49/400 [00:07<00:52,  6.70it/s]\u001b[A\n",
            " 12%|█▎        | 50/400 [00:07<00:50,  6.99it/s]\u001b[A\n",
            " 13%|█▎        | 51/400 [00:07<00:48,  7.14it/s]\u001b[A\n",
            " 13%|█▎        | 52/400 [00:08<00:47,  7.35it/s]\u001b[A\n",
            " 13%|█▎        | 53/400 [00:08<00:46,  7.46it/s]\u001b[A\n",
            " 14%|█▎        | 54/400 [00:08<00:45,  7.58it/s]\u001b[A\n",
            " 14%|█▍        | 55/400 [00:08<00:46,  7.43it/s]\u001b[A\n",
            " 14%|█▍        | 56/400 [00:08<00:45,  7.48it/s]\u001b[A\n",
            " 14%|█▍        | 57/400 [00:08<00:46,  7.40it/s]\u001b[A\n",
            " 14%|█▍        | 58/400 [00:08<00:45,  7.50it/s]\u001b[A\n",
            " 15%|█▍        | 59/400 [00:08<00:45,  7.51it/s]\u001b[A\n",
            " 15%|█▌        | 60/400 [00:09<00:44,  7.57it/s]\u001b[A\n",
            " 15%|█▌        | 61/400 [00:09<00:44,  7.63it/s]\u001b[A\n",
            " 16%|█▌        | 62/400 [00:09<00:45,  7.44it/s]\u001b[A\n",
            " 16%|█▌        | 63/400 [00:09<00:44,  7.53it/s]\u001b[A\n",
            " 16%|█▌        | 64/400 [00:09<00:44,  7.60it/s]\u001b[A\n",
            " 16%|█▋        | 65/400 [00:09<00:44,  7.61it/s]\u001b[A\n",
            " 16%|█▋        | 66/400 [00:09<00:44,  7.58it/s]\u001b[A\n",
            " 17%|█▋        | 67/400 [00:10<00:43,  7.58it/s]\u001b[A\n",
            " 17%|█▋        | 68/400 [00:10<00:44,  7.51it/s]\u001b[A\n",
            " 17%|█▋        | 69/400 [00:10<00:43,  7.57it/s]\u001b[A\n",
            " 18%|█▊        | 70/400 [00:10<00:44,  7.39it/s]\u001b[A\n",
            " 18%|█▊        | 71/400 [00:10<00:43,  7.49it/s]\u001b[A\n",
            " 18%|█▊        | 72/400 [00:10<00:43,  7.58it/s]\u001b[A\n",
            " 18%|█▊        | 73/400 [00:10<00:43,  7.60it/s]\u001b[A\n",
            " 18%|█▊        | 74/400 [00:10<00:42,  7.62it/s]\u001b[A\n",
            " 19%|█▉        | 75/400 [00:11<00:42,  7.64it/s]\u001b[A\n",
            " 19%|█▉        | 76/400 [00:11<00:42,  7.66it/s]\u001b[A\n",
            " 19%|█▉        | 77/400 [00:11<00:42,  7.62it/s]\u001b[A\n",
            " 20%|█▉        | 78/400 [00:11<00:43,  7.42it/s]\u001b[A\n",
            " 20%|█▉        | 79/400 [00:11<00:43,  7.43it/s]\u001b[A\n",
            " 20%|██        | 80/400 [00:11<00:42,  7.51it/s]\u001b[A\n",
            " 20%|██        | 81/400 [00:11<00:42,  7.47it/s]\u001b[A\n",
            " 20%|██        | 82/400 [00:12<00:41,  7.60it/s]\u001b[A\n",
            " 21%|██        | 83/400 [00:12<00:41,  7.63it/s]\u001b[A\n",
            " 21%|██        | 84/400 [00:12<00:41,  7.68it/s]\u001b[A\n",
            " 21%|██▏       | 85/400 [00:12<00:41,  7.67it/s]\u001b[A\n",
            " 22%|██▏       | 86/400 [00:12<00:42,  7.39it/s]\u001b[A\n",
            " 22%|██▏       | 87/400 [00:12<00:42,  7.45it/s]\u001b[A\n",
            " 22%|██▏       | 88/400 [00:12<00:41,  7.57it/s]\u001b[A\n",
            " 22%|██▏       | 89/400 [00:12<00:41,  7.51it/s]\u001b[A\n",
            " 22%|██▎       | 90/400 [00:13<00:40,  7.64it/s]\u001b[A\n",
            " 23%|██▎       | 91/400 [00:13<00:40,  7.68it/s]\u001b[A\n",
            " 23%|██▎       | 92/400 [00:13<00:39,  7.70it/s]\u001b[A\n",
            " 23%|██▎       | 93/400 [00:13<00:40,  7.64it/s]\u001b[A\n",
            " 24%|██▎       | 94/400 [00:13<00:41,  7.40it/s]\u001b[A\n",
            " 24%|██▍       | 95/400 [00:13<00:47,  6.38it/s]\u001b[A\n",
            " 24%|██▍       | 96/400 [00:14<00:51,  5.93it/s]\u001b[A\n",
            " 24%|██▍       | 97/400 [00:14<00:52,  5.79it/s]\u001b[A\n",
            " 24%|██▍       | 98/400 [00:14<00:52,  5.70it/s]\u001b[A\n",
            " 25%|██▍       | 99/400 [00:14<00:53,  5.61it/s]\u001b[A\n",
            " 25%|██▌       | 100/400 [00:14<00:54,  5.52it/s]\u001b[A\n",
            " 25%|██▌       | 101/400 [00:14<00:55,  5.37it/s]\u001b[A\n",
            " 26%|██▌       | 102/400 [00:15<00:55,  5.39it/s]\u001b[A\n",
            " 26%|██▌       | 103/400 [00:15<00:55,  5.38it/s]\u001b[A\n",
            " 26%|██▌       | 104/400 [00:15<00:55,  5.37it/s]\u001b[A\n",
            " 26%|██▋       | 105/400 [00:15<00:56,  5.23it/s]\u001b[A\n",
            " 26%|██▋       | 106/400 [00:15<00:56,  5.19it/s]\u001b[A\n",
            " 27%|██▋       | 107/400 [00:16<00:56,  5.14it/s]\u001b[A\n",
            " 27%|██▋       | 108/400 [00:16<00:57,  5.11it/s]\u001b[A\n",
            " 27%|██▋       | 109/400 [00:16<00:57,  5.02it/s]\u001b[A\n",
            " 28%|██▊       | 110/400 [00:16<00:59,  4.86it/s]\u001b[A\n",
            " 28%|██▊       | 111/400 [00:16<00:57,  5.00it/s]\u001b[A\n",
            " 28%|██▊       | 112/400 [00:17<00:56,  5.08it/s]\u001b[A\n",
            " 28%|██▊       | 113/400 [00:17<00:55,  5.13it/s]\u001b[A\n",
            " 28%|██▊       | 114/400 [00:17<00:55,  5.17it/s]\u001b[A\n",
            " 29%|██▉       | 115/400 [00:17<00:51,  5.58it/s]\u001b[A\n",
            " 29%|██▉       | 116/400 [00:17<00:47,  6.02it/s]\u001b[A\n",
            " 29%|██▉       | 117/400 [00:17<00:44,  6.35it/s]\u001b[A\n",
            " 30%|██▉       | 118/400 [00:18<00:42,  6.71it/s]\u001b[A\n",
            " 30%|██▉       | 119/400 [00:18<00:40,  7.01it/s]\u001b[A\n",
            " 30%|███       | 120/400 [00:18<00:38,  7.22it/s]\u001b[A\n",
            " 30%|███       | 121/400 [00:18<00:38,  7.32it/s]\u001b[A\n",
            " 30%|███       | 122/400 [00:18<00:37,  7.43it/s]\u001b[A\n",
            " 31%|███       | 123/400 [00:18<00:36,  7.51it/s]\u001b[A\n",
            " 31%|███       | 124/400 [00:18<00:37,  7.39it/s]\u001b[A\n",
            " 31%|███▏      | 125/400 [00:18<00:37,  7.40it/s]\u001b[A\n",
            " 32%|███▏      | 126/400 [00:19<00:36,  7.56it/s]\u001b[A\n",
            " 32%|███▏      | 127/400 [00:19<00:36,  7.58it/s]\u001b[A\n",
            " 32%|███▏      | 128/400 [00:19<00:35,  7.66it/s]\u001b[A\n",
            " 32%|███▏      | 129/400 [00:19<00:35,  7.67it/s]\u001b[A\n",
            " 32%|███▎      | 130/400 [00:19<00:35,  7.64it/s]\u001b[A\n",
            " 33%|███▎      | 131/400 [00:19<00:35,  7.64it/s]\u001b[A\n",
            " 33%|███▎      | 132/400 [00:19<00:35,  7.49it/s]\u001b[A\n",
            " 33%|███▎      | 133/400 [00:20<00:35,  7.48it/s]\u001b[A\n",
            " 34%|███▎      | 134/400 [00:20<00:35,  7.57it/s]\u001b[A\n",
            " 34%|███▍      | 135/400 [00:20<00:34,  7.67it/s]\u001b[A\n",
            " 34%|███▍      | 136/400 [00:20<00:34,  7.69it/s]\u001b[A\n",
            " 34%|███▍      | 137/400 [00:20<00:34,  7.66it/s]\u001b[A\n",
            " 34%|███▍      | 138/400 [00:20<00:34,  7.59it/s]\u001b[A\n",
            " 35%|███▍      | 139/400 [00:20<00:34,  7.56it/s]\u001b[A\n",
            " 35%|███▌      | 140/400 [00:20<00:35,  7.35it/s]\u001b[A\n",
            " 35%|███▌      | 141/400 [00:21<00:34,  7.43it/s]\u001b[A\n",
            " 36%|███▌      | 142/400 [00:21<00:34,  7.55it/s]\u001b[A\n",
            " 36%|███▌      | 143/400 [00:21<00:33,  7.57it/s]\u001b[A\n",
            " 36%|███▌      | 144/400 [00:21<00:33,  7.65it/s]\u001b[A\n",
            " 36%|███▋      | 145/400 [00:21<00:33,  7.66it/s]\u001b[A\n",
            " 36%|███▋      | 146/400 [00:21<00:33,  7.67it/s]\u001b[A\n",
            " 37%|███▋      | 147/400 [00:21<00:33,  7.67it/s]\u001b[A\n",
            " 37%|███▋      | 148/400 [00:22<00:33,  7.42it/s]\u001b[A\n",
            " 37%|███▋      | 149/400 [00:22<00:33,  7.50it/s]\u001b[A\n",
            " 38%|███▊      | 150/400 [00:22<00:33,  7.54it/s]\u001b[A\n",
            " 38%|███▊      | 151/400 [00:22<00:32,  7.55it/s]\u001b[A\n",
            " 38%|███▊      | 152/400 [00:22<00:33,  7.50it/s]\u001b[A\n",
            " 38%|███▊      | 153/400 [00:22<00:32,  7.51it/s]\u001b[A\n",
            " 38%|███▊      | 154/400 [00:22<00:32,  7.59it/s]\u001b[A\n",
            " 39%|███▉      | 155/400 [00:22<00:32,  7.44it/s]\u001b[A\n",
            " 39%|███▉      | 156/400 [00:23<00:33,  7.32it/s]\u001b[A\n",
            " 39%|███▉      | 157/400 [00:23<00:32,  7.44it/s]\u001b[A\n",
            " 40%|███▉      | 158/400 [00:23<00:32,  7.52it/s]\u001b[A\n",
            " 40%|███▉      | 159/400 [00:23<00:31,  7.55it/s]\u001b[A\n",
            " 40%|████      | 160/400 [00:23<00:31,  7.62it/s]\u001b[A\n",
            " 40%|████      | 161/400 [00:23<00:31,  7.63it/s]\u001b[A\n",
            " 40%|████      | 162/400 [00:23<00:31,  7.63it/s]\u001b[A\n",
            " 41%|████      | 163/400 [00:24<00:32,  7.30it/s]\u001b[A\n",
            " 41%|████      | 164/400 [00:24<00:32,  7.35it/s]\u001b[A\n",
            " 41%|████▏     | 165/400 [00:24<00:31,  7.47it/s]\u001b[A\n",
            " 42%|████▏     | 166/400 [00:24<00:31,  7.44it/s]\u001b[A\n",
            " 42%|████▏     | 167/400 [00:24<00:31,  7.46it/s]\u001b[A\n",
            " 42%|████▏     | 168/400 [00:24<00:31,  7.42it/s]\u001b[A\n",
            " 42%|████▏     | 169/400 [00:24<00:31,  7.44it/s]\u001b[A\n",
            " 42%|████▎     | 170/400 [00:24<00:30,  7.46it/s]\u001b[A\n",
            " 43%|████▎     | 171/400 [00:25<00:31,  7.25it/s]\u001b[A\n",
            " 43%|████▎     | 172/400 [00:25<00:30,  7.40it/s]\u001b[A\n",
            " 43%|████▎     | 173/400 [00:25<00:30,  7.47it/s]\u001b[A\n",
            " 44%|████▎     | 174/400 [00:25<00:30,  7.51it/s]\u001b[A\n",
            " 44%|████▍     | 175/400 [00:25<00:29,  7.50it/s]\u001b[A\n",
            " 44%|████▍     | 176/400 [00:25<00:29,  7.50it/s]\u001b[A\n",
            " 44%|████▍     | 177/400 [00:25<00:29,  7.52it/s]\u001b[A\n",
            " 44%|████▍     | 178/400 [00:26<00:29,  7.49it/s]\u001b[A\n",
            " 45%|████▍     | 179/400 [00:26<00:30,  7.29it/s]\u001b[A\n",
            " 45%|████▌     | 180/400 [00:26<00:29,  7.45it/s]\u001b[A\n",
            " 45%|████▌     | 181/400 [00:26<00:29,  7.47it/s]\u001b[A\n",
            " 46%|████▌     | 182/400 [00:26<00:29,  7.50it/s]\u001b[A\n",
            " 46%|████▌     | 183/400 [00:26<00:28,  7.57it/s]\u001b[A\n",
            " 46%|████▌     | 184/400 [00:26<00:28,  7.61it/s]\u001b[A\n",
            " 46%|████▋     | 185/400 [00:26<00:28,  7.52it/s]\u001b[A\n",
            " 46%|████▋     | 186/400 [00:27<00:28,  7.61it/s]\u001b[A\n",
            " 47%|████▋     | 187/400 [00:27<00:28,  7.44it/s]\u001b[A\n",
            " 47%|████▋     | 188/400 [00:27<00:28,  7.41it/s]\u001b[A\n",
            " 47%|████▋     | 189/400 [00:27<00:28,  7.40it/s]\u001b[A\n",
            " 48%|████▊     | 190/400 [00:27<00:31,  6.60it/s]\u001b[A\n",
            " 48%|████▊     | 191/400 [00:27<00:34,  6.05it/s]\u001b[A\n",
            " 48%|████▊     | 192/400 [00:28<00:35,  5.83it/s]\u001b[A\n",
            " 48%|████▊     | 193/400 [00:28<00:36,  5.60it/s]\u001b[A\n",
            " 48%|████▊     | 194/400 [00:28<00:37,  5.50it/s]\u001b[A\n",
            " 49%|████▉     | 195/400 [00:28<00:37,  5.40it/s]\u001b[A\n",
            " 49%|████▉     | 196/400 [00:28<00:39,  5.22it/s]\u001b[A\n",
            " 49%|████▉     | 197/400 [00:29<00:39,  5.14it/s]\u001b[A\n",
            " 50%|████▉     | 198/400 [00:29<00:39,  5.17it/s]\u001b[A\n",
            " 50%|████▉     | 199/400 [00:29<00:38,  5.16it/s]\u001b[A\n",
            " 50%|█████     | 200/400 [00:29<00:38,  5.20it/s]\u001b[A\n",
            " 50%|█████     | 201/400 [00:29<00:37,  5.25it/s]\u001b[A\n",
            " 50%|█████     | 202/400 [00:30<00:37,  5.22it/s]\u001b[A\n",
            " 51%|█████     | 203/400 [00:30<00:38,  5.18it/s]\u001b[A\n",
            " 51%|█████     | 204/400 [00:30<00:39,  4.97it/s]\u001b[A\n",
            " 51%|█████▏    | 205/400 [00:30<00:40,  4.86it/s]\u001b[A\n",
            " 52%|█████▏    | 206/400 [00:30<00:39,  4.95it/s]\u001b[A\n",
            " 52%|█████▏    | 207/400 [00:31<00:38,  4.98it/s]\u001b[A\n",
            " 52%|█████▏    | 208/400 [00:31<00:37,  5.09it/s]\u001b[A\n",
            " 52%|█████▏    | 209/400 [00:31<00:37,  5.05it/s]\u001b[A\n",
            " 52%|█████▎    | 210/400 [00:31<00:36,  5.18it/s]\u001b[A\n",
            " 53%|█████▎    | 211/400 [00:31<00:33,  5.73it/s]\u001b[A\n",
            " 53%|█████▎    | 212/400 [00:31<00:30,  6.21it/s]\u001b[A\n",
            " 53%|█████▎    | 213/400 [00:31<00:28,  6.54it/s]\u001b[A\n",
            " 54%|█████▎    | 214/400 [00:32<00:27,  6.84it/s]\u001b[A\n",
            " 54%|█████▍    | 215/400 [00:32<00:26,  7.03it/s]\u001b[A\n",
            " 54%|█████▍    | 216/400 [00:32<00:26,  6.83it/s]\u001b[A\n",
            " 54%|█████▍    | 217/400 [00:32<00:26,  6.90it/s]\u001b[A\n",
            " 55%|█████▍    | 218/400 [00:32<00:25,  7.17it/s]\u001b[A\n",
            " 55%|█████▍    | 219/400 [00:32<00:24,  7.28it/s]\u001b[A\n",
            " 55%|█████▌    | 220/400 [00:32<00:24,  7.44it/s]\u001b[A\n",
            " 55%|█████▌    | 221/400 [00:33<00:23,  7.55it/s]\u001b[A\n",
            " 56%|█████▌    | 222/400 [00:33<00:23,  7.63it/s]\u001b[A\n",
            " 56%|█████▌    | 223/400 [00:33<00:22,  7.70it/s]\u001b[A\n",
            " 56%|█████▌    | 224/400 [00:33<00:22,  7.74it/s]\u001b[A\n",
            " 56%|█████▋    | 225/400 [00:33<00:23,  7.56it/s]\u001b[A\n",
            " 56%|█████▋    | 226/400 [00:33<00:22,  7.60it/s]\u001b[A\n",
            " 57%|█████▋    | 227/400 [00:33<00:22,  7.57it/s]\u001b[A\n",
            " 57%|█████▋    | 228/400 [00:33<00:22,  7.62it/s]\u001b[A\n",
            " 57%|█████▋    | 229/400 [00:34<00:22,  7.62it/s]\u001b[A\n",
            " 57%|█████▊    | 230/400 [00:34<00:22,  7.65it/s]\u001b[A\n",
            " 58%|█████▊    | 231/400 [00:34<00:22,  7.63it/s]\u001b[A\n",
            " 58%|█████▊    | 232/400 [00:34<00:22,  7.41it/s]\u001b[A\n",
            " 58%|█████▊    | 233/400 [00:34<00:22,  7.47it/s]\u001b[A\n",
            " 58%|█████▊    | 234/400 [00:34<00:22,  7.49it/s]\u001b[A\n",
            " 59%|█████▉    | 235/400 [00:34<00:21,  7.50it/s]\u001b[A\n",
            " 59%|█████▉    | 236/400 [00:35<00:21,  7.54it/s]\u001b[A\n",
            " 59%|█████▉    | 237/400 [00:35<00:21,  7.59it/s]\u001b[A\n",
            " 60%|█████▉    | 238/400 [00:35<00:21,  7.61it/s]\u001b[A\n",
            " 60%|█████▉    | 239/400 [00:35<00:21,  7.58it/s]\u001b[A\n",
            " 60%|██████    | 240/400 [00:35<00:21,  7.29it/s]\u001b[A\n",
            " 60%|██████    | 241/400 [00:35<00:21,  7.40it/s]\u001b[A\n",
            " 60%|██████    | 242/400 [00:35<00:21,  7.46it/s]\u001b[A\n",
            " 61%|██████    | 243/400 [00:35<00:21,  7.43it/s]\u001b[A\n",
            " 61%|██████    | 244/400 [00:36<00:21,  7.34it/s]\u001b[A\n",
            " 61%|██████▏   | 245/400 [00:36<00:20,  7.46it/s]\u001b[A\n",
            " 62%|██████▏   | 246/400 [00:36<00:20,  7.46it/s]\u001b[A\n",
            " 62%|██████▏   | 247/400 [00:36<00:20,  7.39it/s]\u001b[A\n",
            " 62%|██████▏   | 248/400 [00:36<00:21,  7.23it/s]\u001b[A\n",
            " 62%|██████▏   | 249/400 [00:36<00:20,  7.34it/s]\u001b[A\n",
            " 62%|██████▎   | 250/400 [00:36<00:20,  7.39it/s]\u001b[A\n",
            " 63%|██████▎   | 251/400 [00:37<00:20,  7.40it/s]\u001b[A\n",
            " 63%|██████▎   | 252/400 [00:37<00:19,  7.46it/s]\u001b[A\n",
            " 63%|██████▎   | 253/400 [00:37<00:19,  7.46it/s]\u001b[A\n",
            " 64%|██████▎   | 254/400 [00:37<00:19,  7.50it/s]\u001b[A\n",
            " 64%|██████▍   | 255/400 [00:37<00:20,  7.18it/s]\u001b[A\n",
            " 64%|██████▍   | 256/400 [00:37<00:19,  7.29it/s]\u001b[A\n",
            " 64%|██████▍   | 257/400 [00:37<00:19,  7.33it/s]\u001b[A\n",
            " 64%|██████▍   | 258/400 [00:38<00:19,  7.35it/s]\u001b[A\n",
            " 65%|██████▍   | 259/400 [00:38<00:18,  7.46it/s]\u001b[A\n",
            " 65%|██████▌   | 260/400 [00:38<00:18,  7.52it/s]\u001b[A\n",
            " 65%|██████▌   | 261/400 [00:38<00:18,  7.57it/s]\u001b[A\n",
            " 66%|██████▌   | 262/400 [00:38<00:18,  7.59it/s]\u001b[A\n",
            " 66%|██████▌   | 263/400 [00:38<00:18,  7.42it/s]\u001b[A\n",
            " 66%|██████▌   | 264/400 [00:38<00:18,  7.49it/s]\u001b[A\n",
            " 66%|██████▋   | 265/400 [00:38<00:18,  7.38it/s]\u001b[A\n",
            " 66%|██████▋   | 266/400 [00:39<00:18,  7.42it/s]\u001b[A\n",
            " 67%|██████▋   | 267/400 [00:39<00:17,  7.46it/s]\u001b[A\n",
            " 67%|██████▋   | 268/400 [00:39<00:17,  7.52it/s]\u001b[A\n",
            " 67%|██████▋   | 269/400 [00:39<00:17,  7.55it/s]\u001b[A\n",
            " 68%|██████▊   | 270/400 [00:39<00:17,  7.62it/s]\u001b[A\n",
            " 68%|██████▊   | 271/400 [00:39<00:17,  7.44it/s]\u001b[A\n",
            " 68%|██████▊   | 272/400 [00:39<00:17,  7.52it/s]\u001b[A\n",
            " 68%|██████▊   | 273/400 [00:40<00:16,  7.56it/s]\u001b[A\n",
            " 68%|██████▊   | 274/400 [00:40<00:16,  7.58it/s]\u001b[A\n",
            " 69%|██████▉   | 275/400 [00:40<00:16,  7.56it/s]\u001b[A\n",
            " 69%|██████▉   | 276/400 [00:40<00:16,  7.63it/s]\u001b[A\n",
            " 69%|██████▉   | 277/400 [00:40<00:16,  7.66it/s]\u001b[A\n",
            " 70%|██████▉   | 278/400 [00:40<00:15,  7.69it/s]\u001b[A\n",
            " 70%|██████▉   | 279/400 [00:40<00:16,  7.42it/s]\u001b[A\n",
            " 70%|███████   | 280/400 [00:40<00:15,  7.52it/s]\u001b[A\n",
            " 70%|███████   | 281/400 [00:41<00:15,  7.55it/s]\u001b[A\n",
            " 70%|███████   | 282/400 [00:41<00:15,  7.61it/s]\u001b[A\n",
            " 71%|███████   | 283/400 [00:41<00:15,  7.63it/s]\u001b[A\n",
            " 71%|███████   | 284/400 [00:41<00:15,  7.66it/s]\u001b[A\n",
            " 71%|███████▏  | 285/400 [00:41<00:16,  7.18it/s]\u001b[A\n",
            " 72%|███████▏  | 286/400 [00:41<00:18,  6.14it/s]\u001b[A\n",
            " 72%|███████▏  | 287/400 [00:42<00:19,  5.85it/s]\u001b[A\n",
            " 72%|███████▏  | 288/400 [00:42<00:19,  5.72it/s]\u001b[A\n",
            " 72%|███████▏  | 289/400 [00:42<00:19,  5.67it/s]\u001b[A\n",
            " 72%|███████▎  | 290/400 [00:42<00:19,  5.61it/s]\u001b[A\n",
            " 73%|███████▎  | 291/400 [00:42<00:19,  5.47it/s]\u001b[A\n",
            " 73%|███████▎  | 292/400 [00:42<00:20,  5.18it/s]\u001b[A\n",
            " 73%|███████▎  | 293/400 [00:43<00:20,  5.16it/s]\u001b[A\n",
            " 74%|███████▎  | 294/400 [00:43<00:20,  5.19it/s]\u001b[A\n",
            " 74%|███████▍  | 295/400 [00:43<00:20,  5.22it/s]\u001b[A\n",
            " 74%|███████▍  | 296/400 [00:43<00:19,  5.28it/s]\u001b[A\n",
            " 74%|███████▍  | 297/400 [00:43<00:19,  5.24it/s]\u001b[A\n",
            " 74%|███████▍  | 298/400 [00:44<00:19,  5.22it/s]\u001b[A\n",
            " 75%|███████▍  | 299/400 [00:44<00:19,  5.19it/s]\u001b[A\n",
            " 75%|███████▌  | 300/400 [00:44<00:19,  5.10it/s]\u001b[A\n",
            " 75%|███████▌  | 301/400 [00:44<00:19,  4.95it/s]\u001b[A\n",
            " 76%|███████▌  | 302/400 [00:44<00:19,  5.02it/s]\u001b[A\n",
            " 76%|███████▌  | 303/400 [00:45<00:19,  5.00it/s]\u001b[A\n",
            " 76%|███████▌  | 304/400 [00:45<00:18,  5.06it/s]\u001b[A\n",
            " 76%|███████▋  | 305/400 [00:45<00:18,  5.10it/s]\u001b[A\n",
            " 76%|███████▋  | 306/400 [00:45<00:17,  5.47it/s]\u001b[A\n",
            " 77%|███████▋  | 307/400 [00:45<00:15,  6.00it/s]\u001b[A\n",
            " 77%|███████▋  | 308/400 [00:45<00:14,  6.40it/s]\u001b[A\n",
            " 77%|███████▋  | 309/400 [00:46<00:13,  6.52it/s]\u001b[A\n",
            " 78%|███████▊  | 310/400 [00:46<00:13,  6.88it/s]\u001b[A\n",
            " 78%|███████▊  | 311/400 [00:46<00:12,  7.12it/s]\u001b[A\n",
            " 78%|███████▊  | 312/400 [00:46<00:12,  7.28it/s]\u001b[A\n",
            " 78%|███████▊  | 313/400 [00:46<00:11,  7.39it/s]\u001b[A\n",
            " 78%|███████▊  | 314/400 [00:46<00:11,  7.51it/s]\u001b[A\n",
            " 79%|███████▉  | 315/400 [00:46<00:11,  7.54it/s]\u001b[A\n",
            " 79%|███████▉  | 316/400 [00:46<00:11,  7.60it/s]\u001b[A\n",
            " 79%|███████▉  | 317/400 [00:47<00:11,  7.40it/s]\u001b[A\n",
            " 80%|███████▉  | 318/400 [00:47<00:10,  7.50it/s]\u001b[A\n",
            " 80%|███████▉  | 319/400 [00:47<00:10,  7.54it/s]\u001b[A\n",
            " 80%|████████  | 320/400 [00:47<00:10,  7.54it/s]\u001b[A\n",
            " 80%|████████  | 321/400 [00:47<00:10,  7.57it/s]\u001b[A\n",
            " 80%|████████  | 322/400 [00:47<00:10,  7.58it/s]\u001b[A\n",
            " 81%|████████  | 323/400 [00:47<00:10,  7.55it/s]\u001b[A\n",
            " 81%|████████  | 324/400 [00:48<00:09,  7.64it/s]\u001b[A\n",
            " 81%|████████▏ | 325/400 [00:48<00:10,  7.41it/s]\u001b[A\n",
            " 82%|████████▏ | 326/400 [00:48<00:09,  7.49it/s]\u001b[A\n",
            " 82%|████████▏ | 327/400 [00:48<00:09,  7.53it/s]\u001b[A\n",
            " 82%|████████▏ | 328/400 [00:48<00:09,  7.54it/s]\u001b[A\n",
            " 82%|████████▏ | 329/400 [00:48<00:09,  7.54it/s]\u001b[A\n",
            " 82%|████████▎ | 330/400 [00:48<00:09,  7.55it/s]\u001b[A\n",
            " 83%|████████▎ | 331/400 [00:48<00:09,  7.36it/s]\u001b[A\n",
            " 83%|████████▎ | 332/400 [00:49<00:09,  7.18it/s]\u001b[A\n",
            " 83%|████████▎ | 333/400 [00:49<00:09,  7.22it/s]\u001b[A\n",
            " 84%|████████▎ | 334/400 [00:49<00:09,  7.31it/s]\u001b[A\n",
            " 84%|████████▍ | 335/400 [00:49<00:08,  7.35it/s]\u001b[A\n",
            " 84%|████████▍ | 336/400 [00:49<00:08,  7.37it/s]\u001b[A\n",
            " 84%|████████▍ | 337/400 [00:49<00:08,  7.40it/s]\u001b[A\n",
            " 84%|████████▍ | 338/400 [00:49<00:08,  7.45it/s]\u001b[A\n",
            " 85%|████████▍ | 339/400 [00:50<00:08,  7.43it/s]\u001b[A\n",
            " 85%|████████▌ | 340/400 [00:50<00:08,  7.19it/s]\u001b[A\n",
            " 85%|████████▌ | 341/400 [00:50<00:08,  7.27it/s]\u001b[A\n",
            " 86%|████████▌ | 342/400 [00:50<00:08,  7.24it/s]\u001b[A\n",
            " 86%|████████▌ | 343/400 [00:50<00:07,  7.23it/s]\u001b[A\n",
            " 86%|████████▌ | 344/400 [00:50<00:07,  7.31it/s]\u001b[A\n",
            " 86%|████████▋ | 345/400 [00:50<00:07,  7.31it/s]\u001b[A\n",
            " 86%|████████▋ | 346/400 [00:51<00:07,  7.36it/s]\u001b[A\n",
            " 87%|████████▋ | 347/400 [00:51<00:07,  7.38it/s]\u001b[A\n",
            " 87%|████████▋ | 348/400 [00:51<00:07,  7.22it/s]\u001b[A\n",
            " 87%|████████▋ | 349/400 [00:51<00:06,  7.39it/s]\u001b[A\n",
            " 88%|████████▊ | 350/400 [00:51<00:06,  7.49it/s]\u001b[A\n",
            " 88%|████████▊ | 351/400 [00:51<00:06,  7.45it/s]\u001b[A\n",
            " 88%|████████▊ | 352/400 [00:51<00:06,  7.53it/s]\u001b[A\n",
            " 88%|████████▊ | 353/400 [00:51<00:06,  7.57it/s]\u001b[A\n",
            " 88%|████████▊ | 354/400 [00:52<00:06,  7.65it/s]\u001b[A\n",
            " 89%|████████▉ | 355/400 [00:52<00:06,  7.49it/s]\u001b[A\n",
            " 89%|████████▉ | 356/400 [00:52<00:05,  7.50it/s]\u001b[A\n",
            " 89%|████████▉ | 357/400 [00:52<00:05,  7.57it/s]\u001b[A\n",
            " 90%|████████▉ | 358/400 [00:52<00:05,  7.66it/s]\u001b[A\n",
            " 90%|████████▉ | 359/400 [00:52<00:05,  7.70it/s]\u001b[A\n",
            " 90%|█████████ | 360/400 [00:52<00:05,  7.71it/s]\u001b[A\n",
            " 90%|█████████ | 361/400 [00:53<00:05,  7.69it/s]\u001b[A\n",
            " 90%|█████████ | 362/400 [00:53<00:04,  7.70it/s]\u001b[A\n",
            " 91%|█████████ | 363/400 [00:53<00:04,  7.57it/s]\u001b[A\n",
            " 91%|█████████ | 364/400 [00:53<00:04,  7.54it/s]\u001b[A\n",
            " 91%|█████████▏| 365/400 [00:53<00:04,  7.54it/s]\u001b[A\n",
            " 92%|█████████▏| 366/400 [00:53<00:04,  7.63it/s]\u001b[A\n",
            " 92%|█████████▏| 367/400 [00:53<00:04,  7.61it/s]\u001b[A\n",
            " 92%|█████████▏| 368/400 [00:53<00:04,  7.67it/s]\u001b[A\n",
            " 92%|█████████▏| 369/400 [00:54<00:04,  7.74it/s]\u001b[A\n",
            " 92%|█████████▎| 370/400 [00:54<00:03,  7.78it/s]\u001b[A\n",
            " 93%|█████████▎| 371/400 [00:54<00:03,  7.48it/s]\u001b[A\n",
            " 93%|█████████▎| 372/400 [00:54<00:03,  7.58it/s]\u001b[A\n",
            " 93%|█████████▎| 373/400 [00:54<00:03,  7.59it/s]\u001b[A\n",
            " 94%|█████████▎| 374/400 [00:54<00:03,  7.58it/s]\u001b[A\n",
            " 94%|█████████▍| 375/400 [00:54<00:03,  7.58it/s]\u001b[A\n",
            " 94%|█████████▍| 376/400 [00:54<00:03,  7.62it/s]\u001b[A\n",
            " 94%|█████████▍| 377/400 [00:55<00:03,  7.63it/s]\u001b[A\n",
            " 94%|█████████▍| 378/400 [00:55<00:02,  7.66it/s]\u001b[A\n",
            " 95%|█████████▍| 379/400 [00:55<00:02,  7.45it/s]\u001b[A\n",
            " 95%|█████████▌| 380/400 [00:55<00:02,  7.53it/s]\u001b[A\n",
            " 95%|█████████▌| 381/400 [00:55<00:02,  6.71it/s]\u001b[A\n",
            " 96%|█████████▌| 382/400 [00:55<00:02,  6.06it/s]\u001b[A\n",
            " 96%|█████████▌| 383/400 [00:56<00:02,  5.80it/s]\u001b[A\n",
            " 96%|█████████▌| 384/400 [00:56<00:02,  5.63it/s]\u001b[A\n",
            " 96%|█████████▋| 385/400 [00:56<00:02,  5.47it/s]\u001b[A\n",
            " 96%|█████████▋| 386/400 [00:56<00:02,  5.48it/s]\u001b[A\n",
            " 97%|█████████▋| 387/400 [00:56<00:02,  5.29it/s]\u001b[A\n",
            " 97%|█████████▋| 388/400 [00:57<00:02,  5.22it/s]\u001b[A\n",
            " 97%|█████████▋| 389/400 [00:57<00:02,  5.17it/s]\u001b[A\n",
            " 98%|█████████▊| 390/400 [00:57<00:01,  5.23it/s]\u001b[A\n",
            " 98%|█████████▊| 391/400 [00:57<00:01,  5.23it/s]\u001b[A\n",
            " 98%|█████████▊| 392/400 [00:57<00:01,  5.33it/s]\u001b[A\n",
            " 98%|█████████▊| 393/400 [00:58<00:01,  5.36it/s]\u001b[A\n",
            " 98%|█████████▊| 394/400 [00:58<00:01,  5.32it/s]\u001b[A\n",
            " 99%|█████████▉| 395/400 [00:58<00:00,  5.13it/s]\u001b[A\n",
            " 99%|█████████▉| 396/400 [00:58<00:00,  4.86it/s]\u001b[A\n",
            " 99%|█████████▉| 397/400 [00:58<00:00,  4.91it/s]\u001b[A\n",
            "100%|█████████▉| 398/400 [00:59<00:00,  4.99it/s]\u001b[A\n",
            "100%|█████████▉| 399/400 [00:59<00:00,  5.01it/s]\u001b[A\n",
            "100%|██████████| 400/400 [00:59<00:00,  6.73it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing patches for each model\n",
            "The diff_1 model will be used.\n",
            "Using local averaging\n",
            "Result folder was created.\n",
            "Reconstructing in chunks\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [01:23<00:00, 83.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The diff_2 model will be used.\n",
            "Using local averaging\n",
            "The diff_3 model will be used.\n",
            "Using local averaging\n",
            "Reconstructing in chunks\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/2 [01:52<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3904859029.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1420\u001b[0m                     with catch_oom(\"reconstructing chunk\",\n\u001b[1;32m   1421\u001b[0m                                    detail=f\"{model_name} [{chunk_start}:{chunk_end}] of {len(patches_list[model_num])}\"):\n\u001b[0;32m-> 1422\u001b[0;31m                         pw_recon, loc_list = reconstruct_patches_2025(\n\u001b[0m\u001b[1;32m   1423\u001b[0m                             \u001b[0mpatches_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchunk_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mchunk_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m                             \u001b[0mpatch_indices_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchunk_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mchunk_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3904859029.py\u001b[0m in \u001b[0;36mreconstruct_patches_2025\u001b[0;34m(Images, patch_ind, frame_numbers, weights_file, num_patches, overlap, number_of_frames, thresh, neighborhood_size, use_local_avg, upsampling_factor, pixel_size, batch_size)\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;31m# --- Run prediction on GPU ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mpredicted_density\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         \u001b[0mpredicted_density\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_density\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;31m# Crop off extra overlap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \"\"\"\n\u001b[1;32m    418\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArrayLike\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "Notebook_version = '1.2'\n",
        "Network = 'AutoDS'\n",
        "\n",
        "# Import keras modules and libraries from tensorflow.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Activation, UpSampling2D, Conv2D, MaxPooling2D, BatchNormalization, Layer\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Other libraries\n",
        "import scipy.optimize as opt\n",
        "import scipy.io as sio\n",
        "import scipy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tifffile as tiff\n",
        "from numpy.lib.stride_tricks import sliding_window_view\n",
        "from scipy.ndimage import gaussian_laplace, maximum_filter, binary_dilation\n",
        "from scipy.signal import fftconvolve\n",
        "from skimage.morphology import white_tophat, disk\n",
        "import h5py\n",
        "import cv2\n",
        "from skimage import io\n",
        "import sys, os, traceback\n",
        "import csv\n",
        "from PIL import Image\n",
        "from PIL.TiffTags import TAGS\n",
        "import math\n",
        "from skimage.feature import peak_local_max\n",
        "from scipy.ndimage import gaussian_filter, zoom\n",
        "from tqdm import tqdm\n",
        "from contextlib import contextmanager\n",
        "import time\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List\n",
        "\n",
        "# Create a variable to get and store relative base path\n",
        "base_path = os.getcwd()\n",
        "\n",
        "import io, json, zipfile, hashlib, shutil, urllib.request\n",
        "from pathlib import Path\n",
        "\n",
        "# ============================================================================\n",
        "# TIMING PROFILER CLASS\n",
        "# ============================================================================\n",
        "class timing_profiler:\n",
        "    def __init__(self, enabled=True):\n",
        "        self.enabled = enabled\n",
        "        self.accu_timing: Dict[str, List[float]] = defaultdict(list)\n",
        "        self.active_timers: Dict[str, float] = {}\n",
        "\n",
        "    def start_timer(self, name):\n",
        "        if not self.enabled:\n",
        "            return\n",
        "\n",
        "        if tf.test.is_built_with_cuda() and len(tf.config.list_physical_devices('GPU')) > 0:\n",
        "            # TensorFlow doesn't have synchronize like PyTorch, but we can use a dummy op\n",
        "            _ = tf.constant(0)\n",
        "\n",
        "        self.active_timers[name] = time.perf_counter()\n",
        "\n",
        "    def stop_timer(self, name):\n",
        "        if not self.enabled:\n",
        "            return\n",
        "\n",
        "        if tf.test.is_built_with_cuda() and len(tf.config.list_physical_devices('GPU')) > 0:\n",
        "            _ = tf.constant(0)\n",
        "\n",
        "        if name in self.active_timers:\n",
        "            run_time = time.perf_counter() - self.active_timers[name]\n",
        "            self.accu_timing[name].append(run_time)\n",
        "            del self.active_timers[name]\n",
        "\n",
        "    def get_stats(self, name):\n",
        "        times = self.accu_timing[name]\n",
        "        if not times:\n",
        "            return {'total': 0, 'average': 0, 'count': 0, 'min': 0, 'max': 0}\n",
        "        return {\n",
        "            'total': sum(times),\n",
        "            'average': sum(times) / len(times),\n",
        "            'count': len(times),\n",
        "            'min': min(times),\n",
        "            'max': max(times)\n",
        "        }\n",
        "\n",
        "    def print_timing_summary(self):\n",
        "        if not self.enabled:\n",
        "            return\n",
        "\n",
        "        sub_sections_timers = {}\n",
        "        for name, times in self.accu_timing.items():\n",
        "            sub_sections_timers[name] = times\n",
        "\n",
        "        self._print_section(sub_sections_timers, \"\")\n",
        "\n",
        "    def _print_section(self, timers, prefix):\n",
        "        hierarchy = {}\n",
        "        for name, times in timers.items():\n",
        "            relative_name = name[len(prefix) + 1:] if name.startswith(prefix + '.') else name\n",
        "            hierarchy[relative_name] = times\n",
        "\n",
        "        if not hierarchy:\n",
        "            return\n",
        "\n",
        "        if 'total' in hierarchy:\n",
        "            total_time = sum(hierarchy['total'])\n",
        "        else:\n",
        "            total_time = sum(sum(times) for times in hierarchy.values())\n",
        "\n",
        "        print(\"-\" * 84)\n",
        "        print(f\"{'Step':<40} {'total time':<12} {'avg (per call)':<8} {'calls':<8} {'% of total':<12}\")\n",
        "        print(\"-\" * 84)\n",
        "\n",
        "        sorted_items = sorted(hierarchy.items(), key=lambda x: sum(x[1]), reverse=True)\n",
        "\n",
        "        for section_name, times in sorted_items:\n",
        "            total = sum(times)\n",
        "            avg_per_call = (total / len(times)) if times else 0\n",
        "            count = len(times)\n",
        "            percentage = (total / total_time * 100) if total_time > 0 else 0\n",
        "\n",
        "            print(f\"{section_name:<40} {total:>11.3f} {avg_per_call:>11.2f} {count:>7} {percentage:>10.1f}%\")\n",
        "\n",
        "    def reset(self):\n",
        "        self.accu_timing.clear()\n",
        "        self.active_timers.clear()\n",
        "\n",
        "# Initialize global timing profiler\n",
        "timing_prof = timing_profiler(enabled=True)\n",
        "\n",
        "# ============================================================================\n",
        "# GITHUB DATA DOWNLOAD UTILITIES\n",
        "# ============================================================================\n",
        "def download_github_file(url, destination, commit_or_branch='master'):\n",
        "    \"\"\"Download a single file from GitHub, handling Git LFS if needed\"\"\"\n",
        "    # Convert GitHub web URL to raw content URL if needed\n",
        "    if 'github.com' in url and '/blob/' in url:\n",
        "        url = url.replace('github.com', 'raw.githubusercontent.com').replace('/blob/', '/')\n",
        "\n",
        "    os.makedirs(os.path.dirname(destination), exist_ok=True)\n",
        "\n",
        "    # Add headers to avoid GitHub's HTML wrapper\n",
        "    req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "\n",
        "    with urllib.request.urlopen(req) as response:\n",
        "        content = response.read()\n",
        "\n",
        "        # Check if this is a Git LFS pointer file\n",
        "        if b'version https://git-lfs.github.com/spec/' in content[:200]:\n",
        "            # Parse LFS pointer to get actual file URL\n",
        "            content_str = content.decode('utf-8')\n",
        "            for line in content_str.split('\\n'):\n",
        "                if line.startswith('oid sha256:'):\n",
        "                    oid = line.split(':')[1].strip()\n",
        "                    # Extract user/repo and path from original URL\n",
        "                    parts = url.split('/')\n",
        "                    user = parts[3]\n",
        "                    repo = parts[4]\n",
        "                    commit = parts[5]  # Get commit from URL\n",
        "                    file_path = '/'.join(parts[6:])\n",
        "\n",
        "                    # Try multiple LFS URL formats\n",
        "                    lfs_urls = [\n",
        "                        f\"https://media.githubusercontent.com/media/{user}/{repo}/{commit}/{file_path}\",\n",
        "                        f\"https://github.com/{user}/{repo}/raw/{commit}/{file_path}?raw=true\",\n",
        "                        f\"https://github.com/{user}/{repo}/blob/{commit}/{file_path}?raw=true\",\n",
        "                    ]\n",
        "\n",
        "                    # Try each URL\n",
        "                    for lfs_url in lfs_urls:\n",
        "                        try:\n",
        "                            req_lfs = urllib.request.Request(lfs_url, headers={\n",
        "                                'User-Agent': 'Mozilla/5.0',\n",
        "                                'Accept': 'application/vnd.git-lfs+json'\n",
        "                            })\n",
        "                            with urllib.request.urlopen(req_lfs) as lfs_response:\n",
        "                                content = lfs_response.read()\n",
        "                                # Check if we still got an LFS pointer\n",
        "                                if b'version https://git-lfs.github.com/spec/' not in content[:200]:\n",
        "                                    break\n",
        "                        except Exception as e:\n",
        "                            print(f\"    Failed: {e}\")\n",
        "                            continue\n",
        "                    break\n",
        "\n",
        "        # Write content to file\n",
        "        with open(destination, 'wb') as out_file:\n",
        "            out_file.write(content)\n",
        "\n",
        "    # Verify file was downloaded correctly\n",
        "    file_size = os.path.getsize(destination)\n",
        "    if file_size < 1000:  # Files smaller than 1KB are likely error pages\n",
        "        with open(destination, 'rb') as f:\n",
        "            content_check = f.read(100)\n",
        "            if b'<!DOCTYPE' in content_check or b'<html' in content_check:\n",
        "                raise ValueError(f\"Downloaded HTML instead of binary file. URL may be incorrect.\")\n",
        "            elif b'version https://git-lfs.github.com/spec/' in content_check:\n",
        "                raise ValueError(f\"Still got LFS pointer file, not actual content.\")\n",
        "\n",
        "    print(f\"Saved to: {destination} ({file_size / (1024*1024):.2f} MB)\")\n",
        "\n",
        "\n",
        "def download_tiff_files_fallback(user, repo, commit, dir_path, local_path):\n",
        "    \"\"\"Fallback method to download TIFF files when API fails\"\"\"\n",
        "    os.makedirs(local_path, exist_ok=True)\n",
        "    downloaded = []\n",
        "\n",
        "    # Try common TIFF file numbering patterns\n",
        "    for i in range(1, 100):  # Try up to 100 files\n",
        "        for ext in ['.tif', '.tiff']:\n",
        "            file_name = f\"{i}{ext}\"\n",
        "            raw_url = f\"https://raw.githubusercontent.com/{user}/{repo}/{commit}/{dir_path}/{file_name}\"\n",
        "            dest_path = os.path.join(local_path, file_name)\n",
        "\n",
        "            try:\n",
        "                urllib.request.urlretrieve(raw_url, dest_path)\n",
        "                print(f\"Downloaded: {file_name}\")\n",
        "                downloaded.append(dest_path)\n",
        "                break  # Found file with this number, try next\n",
        "            except:\n",
        "                continue  # File doesn't exist, try next\n",
        "\n",
        "        if i > 10 and len(downloaded) == 0:\n",
        "            break  # Stop if first 10 attempts fail\n",
        "\n",
        "    if len(downloaded) == 0:\n",
        "        print(\"No files found with fallback method.\")\n",
        "    else:\n",
        "        print(f\"Downloaded {len(downloaded)} files using fallback method\")\n",
        "\n",
        "    return downloaded\n",
        "\n",
        "# ============================================================================\n",
        "# REST OF THE ORIGINAL CODE\n",
        "# ============================================================================\n",
        "\n",
        "def is_tiff(path):\n",
        "    \"\"\"Check if file is TIFF\"\"\"\n",
        "    return path.lower().endswith(('.tif', '.tiff'))\n",
        "\n",
        "def _printer():\n",
        "    # use global log() if you defined QUIET/log earlier; else print\n",
        "    return log if 'log' in globals() else print\n",
        "\n",
        "def _sha256(path):\n",
        "    h = hashlib.sha256()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(1024 * 1024), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def _download(url, dst_path):\n",
        "    os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
        "    _printer()(f\"[models] downloading: {url}\")\n",
        "    urllib.request.urlretrieve(url, dst_path)\n",
        "\n",
        "def _flatten_if_needed(target_dir, required_files):\n",
        "    \"\"\"\n",
        "    If the extracted ZIP created a nested top-level folder (e.g., target_dir/diff_1/*),\n",
        "    but we expect files directly under target_dir, move them up one level.\n",
        "    \"\"\"\n",
        "    present = all(os.path.exists(os.path.join(target_dir, f)) for f in required_files)\n",
        "    if present:\n",
        "        return\n",
        "\n",
        "    # look for a single subdir containing the stuff\n",
        "    subdirs = [d for d in os.listdir(target_dir) if os.path.isdir(os.path.join(target_dir, d))]\n",
        "    if len(subdirs) == 1:\n",
        "        candidate = os.path.join(target_dir, subdirs[0])\n",
        "        # if moving would fix it, move contents up\n",
        "        if all(os.path.exists(os.path.join(candidate, f)) for f in required_files):\n",
        "            for name in os.listdir(candidate):\n",
        "                shutil.move(os.path.join(candidate, name), os.path.join(target_dir, name))\n",
        "            # remove now-empty subdir\n",
        "            try:\n",
        "                os.rmdir(candidate)\n",
        "            except OSError:\n",
        "                pass\n",
        "\n",
        "def ensure_models(model_names, target_root=\"/content/AutoDS_models\", model_manifest=None):\n",
        "    \"\"\"\n",
        "    model_manifest schema (choose ONE per model):\n",
        "      # ZIP asset per model (recommended)\n",
        "      {\n",
        "        \"diff_1\": {\n",
        "          \"zip_url\": \"<direct zip url>\",\n",
        "          \"sha256\":  \"<optional sha256 of the zip>\",\n",
        "          \"contains\": [\"best_weights.h5\", \"model_metadata.mat\"]\n",
        "        },\n",
        "        ...\n",
        "      }\n",
        "\n",
        "      # Raw files (no zip)\n",
        "      {\n",
        "        \"diff_1\": {\n",
        "          \"file_urls\": {\n",
        "            \"best_weights.h5\": \"<direct file url>\",\n",
        "            \"model_metadata.mat\": \"<direct file url>\"\n",
        "          },\n",
        "          \"file_sha256\": {             # optional, per-file\n",
        "            \"best_weights.h5\": \"<sha256>\",\n",
        "            \"model_metadata.mat\": \"<sha256>\"\n",
        "          },\n",
        "          \"contains\": [\"best_weights.h5\", \"model_metadata.mat\"]\n",
        "        },\n",
        "        ...\n",
        "      }\n",
        "    \"\"\"\n",
        "    if model_manifest is None:\n",
        "        raise ValueError(\"ensure_models: model_manifest must be provided.\")\n",
        "    os.makedirs(target_root, exist_ok=True)\n",
        "\n",
        "    for m in model_names:\n",
        "        cfg = model_manifest[m]\n",
        "        mdir = os.path.join(target_root, m)\n",
        "        need_fetch = False\n",
        "\n",
        "        # fast-path: check presence\n",
        "        req = cfg.get(\"contains\", [])\n",
        "        if not os.path.isdir(mdir):\n",
        "            need_fetch = True\n",
        "        else:\n",
        "            for f in req:\n",
        "                if not os.path.exists(os.path.join(mdir, f)):\n",
        "                    need_fetch = True\n",
        "                    break\n",
        "\n",
        "        if not need_fetch:\n",
        "            _printer()(f\"[models] found: {m}\")\n",
        "            continue\n",
        "\n",
        "        _printer()(f\"[models] preparing: {m}\")\n",
        "        os.makedirs(mdir, exist_ok=True)\n",
        "\n",
        "        if \"zip_url\" in cfg:\n",
        "            # ZIP flow\n",
        "            zip_url = cfg[\"zip_url\"]\n",
        "            zip_path = os.path.join(target_root, f\"{m}.zip\")\n",
        "            _download(zip_url, zip_path)\n",
        "\n",
        "            if \"sha256\" in cfg:\n",
        "                digest = _sha256(zip_path)\n",
        "                if digest != cfg[\"sha256\"]:\n",
        "                    raise ValueError(f\"SHA256 mismatch for {m} zip. expected {cfg['sha256']} got {digest}\")\n",
        "\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "                zf.extractall(mdir)\n",
        "            os.remove(zip_path)\n",
        "\n",
        "            # handle nested folder cases\n",
        "            _flatten_if_needed(mdir, req)\n",
        "\n",
        "        elif \"file_urls\" in cfg:\n",
        "            # Per-file flow\n",
        "            file_urls = cfg[\"file_urls\"]\n",
        "            file_sha = cfg.get(\"file_sha256\", {})\n",
        "            for fname, url in file_urls.items():\n",
        "                dst = os.path.join(mdir, fname)\n",
        "                _download(url, dst)\n",
        "                if fname in file_sha:\n",
        "                    digest = _sha256(dst)\n",
        "                    if digest != file_sha[fname]:\n",
        "                        raise ValueError(f\"SHA256 mismatch for {m}/{fname}. expected {file_sha[fname]} got {digest}\")\n",
        "        else:\n",
        "            raise ValueError(f\"Model {m} manifest must have either 'zip_url' or 'file_urls'.\")\n",
        "\n",
        "        # Final presence check\n",
        "        for f in req:\n",
        "            if not os.path.exists(os.path.join(mdir, f)):\n",
        "                raise FileNotFoundError(f\"Model {m} missing required file after fetch: {f}\")\n",
        "\n",
        "        _printer()(f\"[models] ready: {m}\")\n",
        "\n",
        "    return target_root\n",
        "\n",
        "# --- Quiet/Preview flags ------------------------------------------------------\n",
        "QUIET = False            # no training/inference chatter unless set to False\n",
        "HEADLESS_PREVIEW = True  # set True if you want to see the preview figures\n",
        "\n",
        "def log(*args, **kwargs):\n",
        "    if not QUIET:\n",
        "        print(*args, **kwargs)\n",
        "\n",
        "# Define where to fetch each model\n",
        "# Replace the example zip URLs with your actual GitHub Release (or other) asset URLs.\n",
        "model_names = ['diff_1', 'diff_2', 'diff_3', 'diff_4']\n",
        "MODEL_MANIFEST = {\n",
        "    \"diff_1\": {\n",
        "        \"file_urls\": {\n",
        "            \"best_weights.h5\": \"https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/raw/main/AutoDS/models/diff_1/best_weights.h5\",\n",
        "            \"model_metadata.mat\": \"https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/raw/main/AutoDS/models/diff_1/model_metadata.mat\",\n",
        "        },\n",
        "        \"contains\": [\"best_weights.h5\", \"model_metadata.mat\"]\n",
        "    },\n",
        "    \"diff_2\": {\n",
        "        \"file_urls\": {\n",
        "            \"best_weights.h5\": \"https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/raw/main/AutoDS/models/diff_2/best_weights.h5\",\n",
        "            \"model_metadata.mat\": \"https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/raw/main/AutoDS/models/diff_2/model_metadata.mat\",\n",
        "        },\n",
        "        \"contains\": [\"best_weights.h5\", \"model_metadata.mat\"]\n",
        "    },\n",
        "    \"diff_3\": {\n",
        "        \"file_urls\": {\n",
        "            \"best_weights.h5\": \"https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/raw/main/AutoDS/models/diff_3/best_weights.h5\",\n",
        "            \"model_metadata.mat\": \"https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/raw/main/AutoDS/models/diff_3/model_metadata.mat\",\n",
        "        },\n",
        "        \"contains\": [\"best_weights.h5\", \"model_metadata.mat\"]\n",
        "    },\n",
        "    \"diff_4\": {\n",
        "        \"file_urls\": {\n",
        "            \"best_weights.h5\": \"https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/raw/main/AutoDS/models/diff_4/best_weights.h5\",\n",
        "            \"model_metadata.mat\": \"https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/raw/main/AutoDS/models/diff_4/model_metadata.mat\",\n",
        "        },\n",
        "        \"contains\": [\"best_weights.h5\", \"model_metadata.mat\"]\n",
        "    },\n",
        "}\n",
        "# Download (only if missing) and set prediction_model_path accordingly\n",
        "prediction_model_path = ensure_models(model_names, target_root=\"/content/AutoDS_models\", model_manifest=MODEL_MANIFEST)\n",
        "\n",
        "def correctDriftLocalization(xc_array, yc_array, frames, xDrift, yDrift):\n",
        "  n_locs = xc_array.shape[0]\n",
        "  xc_array_Corr = np.empty(n_locs)\n",
        "  yc_array_Corr = np.empty(n_locs)\n",
        "\n",
        "  for loc in range(n_locs):\n",
        "    xc_array_Corr[loc] = xc_array[loc] - xDrift[frames[loc] - 1]\n",
        "    yc_array_Corr[loc] = yc_array[loc] - yDrift[frames[loc] - 1]\n",
        "\n",
        "  return (xc_array_Corr, yc_array_Corr)\n",
        "\n",
        "def FromLoc2Image_SimpleHistogram(xc_array, yc_array, image_size = (64,64), pixel_size = 100):\n",
        "  w = image_size[0]\n",
        "  h = image_size[1]\n",
        "  locImage = np.zeros((image_size[0],image_size[1]) )\n",
        "  n_locs = len(xc_array)\n",
        "\n",
        "  for e in range(n_locs):\n",
        "    locImage[int(max(min(round(yc_array[e]/pixel_size),w-1),0))][int(max(min(round(xc_array[e]/pixel_size),h-1),0))] += 1\n",
        "\n",
        "  return locImage\n",
        "\n",
        "def estimate_drift_com_nm(img1, img2, pixel_size_nm, sigma=1.0, patch_radius=3):\n",
        "    # Smooth images\n",
        "    img1_smooth = gaussian_filter(img1.astype(np.float32), sigma=sigma)\n",
        "    img2_smooth = gaussian_filter(img2.astype(np.float32), sigma=sigma)\n",
        "\n",
        "    # Cross-correlation\n",
        "    corr = fftconvolve(img1_smooth, img2_smooth, mode='same')\n",
        "\n",
        "    # Define center of the image\n",
        "    center_y, center_x = np.array(corr.shape) // 2\n",
        "\n",
        "    # Define a crop region around the center\n",
        "    y_min = max(0, center_y - patch_radius)\n",
        "    y_max = min(corr.shape[0], center_y + patch_radius + 1)\n",
        "    x_min = max(0, center_x - patch_radius)\n",
        "    x_max = min(corr.shape[1], center_x + patch_radius + 1)\n",
        "\n",
        "    # Crop around center\n",
        "    patch = corr[y_min:y_max, x_min:x_max]\n",
        "\n",
        "    # Find subpixel center of mass in cropped patch\n",
        "    y_grid, x_grid = np.meshgrid(\n",
        "        np.arange(y_min, y_max), np.arange(x_min, x_max), indexing='ij'\n",
        "    )\n",
        "\n",
        "    total = np.sum(patch)\n",
        "    if total == 0:\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    y_com = np.sum(patch * y_grid) / total\n",
        "    x_com = np.sum(patch * x_grid) / total\n",
        "\n",
        "    # Drift relative to center, in pixels\n",
        "    dy_px = y_com - center_y\n",
        "    dx_px = x_com - center_x\n",
        "\n",
        "    # Limit the drift to patch_radius\n",
        "    if abs(dy_px) > patch_radius or abs(dx_px) > patch_radius:\n",
        "        return 0.0, 0.0  # or raise an exception\n",
        "\n",
        "    # Convert to nanometers\n",
        "    dy_nm = dy_px * pixel_size_nm\n",
        "    dx_nm = dx_px * pixel_size_nm\n",
        "\n",
        "    return dy_nm, dx_nm\n",
        "\n",
        "def gaussian_interpolation_batch(data_batch, scale, sigma=1):\n",
        "    \"\"\"\n",
        "    Applies Gaussian interpolation (smoothing and upsampling) to a batch of images.\n",
        "\n",
        "    Parameters:\n",
        "    - data_batch: A numpy array of shape (batch_size, height, width), where each entry is an image.\n",
        "    - scale: The scaling factor for upsampling.\n",
        "    - sigma: The standard deviation for the Gaussian filter (default is 1).\n",
        "\n",
        "    Returns:\n",
        "    - upsampled_data_batch: A numpy array containing the upsampled images.\n",
        "    \"\"\"\n",
        "    upsampled_data_batch = []\n",
        "\n",
        "    for data in data_batch:\n",
        "        # Apply Gaussian filter to each image in the batch\n",
        "        smoothed_data = gaussian_filter(data, sigma=sigma)\n",
        "\n",
        "        # Upsample the smoothed image\n",
        "        upsampled_data = zoom(smoothed_data, scale, order=3)  # Using cubic interpolation for smooth upsampling\n",
        "        upsampled_data_batch.append(upsampled_data)\n",
        "\n",
        "    # Convert the list of upsampled images back into a numpy array\n",
        "    return np.array(upsampled_data_batch)\n",
        "\n",
        "def interpolate_frames(tiff_stack, model_pixel_size, current_pixel_size,\n",
        "                             model_wavelength, current_wavelength, model_NA, current_NA):\n",
        "    timing_prof.start_timer('interpolate_frames')\n",
        "\n",
        "    # Compute ratio\n",
        "    if model_pixel_size is None: model_pixel_size = current_pixel_size\n",
        "    if model_wavelength is None: model_wavelength = current_wavelength\n",
        "    if model_NA is None: model_NA = current_NA\n",
        "    if current_wavelength is None: current_wavelength = model_wavelength = 1\n",
        "    if current_NA is None: current_NA = model_NA = 1\n",
        "\n",
        "    if len(tiff_stack.shape) == 2:\n",
        "        tiff_stack = tiff_stack[None, :, :]\n",
        "\n",
        "    scale_ratio_sq = (0.21 * model_wavelength / model_NA) ** 2 - (0.21 * current_wavelength / current_NA) ** 2\n",
        "    if (scale_ratio_sq) > 0:\n",
        "        scale_ratio = np.sqrt(scale_ratio_sq) / model_pixel_size\n",
        "        interpolated_stack = np.stack([gaussian_filter(tiff_stack[i], scale_ratio) for i in range(tiff_stack.shape[0])])\n",
        "    else:\n",
        "        zoom_factors = (1, model_pixel_size / current_pixel_size, model_pixel_size / current_pixel_size)\n",
        "        interpolated_stack = zoom(tiff_stack.astype(np.float32), zoom_factors, order=3)\n",
        "\n",
        "    timing_prof.stop_timer('interpolate_frames')\n",
        "    return interpolated_stack.astype(np.float32, copy=False)\n",
        "\n",
        "def ChooseNetByDifficulty_2025(density, SNR):\n",
        "    num_models = 4\n",
        "    norm_density = np.max([np.min([int(np.round(2 * density)), num_models-1]), 0])\n",
        "    norm_SNR = num_models - 1 - np.max([np.min([SNR//2, num_models - 1]), 0])\n",
        "    return int(np.round((norm_SNR + norm_density) / 2))\n",
        "\n",
        "def reconstruct_patches_2025(Images, patch_ind, frame_numbers, weights_file, num_patches, overlap, number_of_frames,\n",
        "                             thresh=0.1, neighborhood_size=3, use_local_avg=False, upsampling_factor=8, pixel_size=None,\n",
        "                             batch_size=1):\n",
        "    timing_prof.start_timer('reconstruct_patches_2025')\n",
        "\n",
        "    pixel_size_hr = pixel_size / upsampling_factor\n",
        "\n",
        "    # Convert Images to float32 Tensor and move to GPU\n",
        "    Images = tf.convert_to_tensor(Images, dtype=tf.float32)\n",
        "    if Images.ndim == 2:\n",
        "        Images = tf.expand_dims(Images, axis=0)  # Ensure 3D shape\n",
        "    K_frames, M, N = Images.shape\n",
        "\n",
        "    # Determine dimensions of each predicted (cropped) patch.\n",
        "    patch_height = M * upsampling_factor - 2 * overlap\n",
        "    patch_width = N * upsampling_factor - 2 * overlap\n",
        "\n",
        "    # Create full image tensor on GPU\n",
        "    reconstructed_image = np.zeros((patch_height * num_patches, patch_width * num_patches), dtype=np.float32)\n",
        "\n",
        "    # Prepare lists for detections\n",
        "    recon_xind, recon_yind, frame_index, confidence_list = [], [], [], []\n",
        "\n",
        "    # Load the model on the GPU\n",
        "    timing_prof.start_timer('model_loading')\n",
        "    with tf.device('/GPU:0'):\n",
        "        model = build_model_upsample((M, N, 1), lr=1e-3, upsampling_factor=upsampling_factor)\n",
        "        model.load_weights(weights_file)\n",
        "\n",
        "        # Create the post-processing layer\n",
        "        max_layer = Maximafinder(thresh, neighborhood_size, use_local_avg)\n",
        "    timing_prof.stop_timer('model_loading')\n",
        "\n",
        "    timing_prof.start_timer('inference_patches')\n",
        "    n_batches = int(np.ceil(K_frames / batch_size))\n",
        "    for b in range(n_batches):\n",
        "        start = b * batch_size\n",
        "        end = min(K_frames, start + batch_size)\n",
        "        nF = end - start\n",
        "\n",
        "        # --- Move input batch to GPU ---\n",
        "        batch_imgs = Images[start:end]  # Shape: (nF, M, N)\n",
        "\n",
        "        # --- Run prediction on GPU ---\n",
        "        predicted_density = model(batch_imgs, training=False)\n",
        "        predicted_density = tf.nn.relu(predicted_density - 0.5).numpy()\n",
        "\n",
        "        # Crop off extra overlap\n",
        "        cropped_pred = predicted_density[:, overlap:-overlap, overlap:-overlap, 0]\n",
        "\n",
        "        # --- Post-processing on GPU ---\n",
        "        bind, xind, yind, conf = max_layer(predicted_density[:, overlap:-overlap, overlap:-overlap])\n",
        "\n",
        "        # Convert tensors to NumPy (only when needed)\n",
        "        bind_np, xind_np, yind_np, conf_np = bind.numpy(), xind.numpy(), yind.numpy(), conf.numpy() / L2_weighting_factor\n",
        "\n",
        "        # --- Place each patch in reconstructed image ---\n",
        "        for i in range(nF):\n",
        "            p_ind = patch_ind[start + i]\n",
        "            y1 = patch_height * (p_ind // num_patches)\n",
        "            x1 = patch_width * (p_ind % num_patches)\n",
        "\n",
        "            # Use TensorFlow addition instead of NumPy\n",
        "            reconstructed_image[y1:y1 + patch_height, x1:x1 + patch_width] += (cropped_pred[i] / number_of_frames)\n",
        "\n",
        "            # Collect detections\n",
        "            det_idx = np.where(bind_np == i)[0]\n",
        "            if det_idx.size:\n",
        "                recon_xind.extend((x1 + xind_np[det_idx]).tolist())\n",
        "                recon_yind.extend((y1 + yind_np[det_idx]).tolist())\n",
        "                frame_index.extend([frame_numbers[start + i] + 1] * det_idx.size)\n",
        "                confidence_list.extend(conf_np[det_idx].tolist())\n",
        "\n",
        "    timing_prof.stop_timer('inference_patches')\n",
        "\n",
        "    # Convert coordinates to physical units\n",
        "    xind_final = (np.array(recon_xind) * pixel_size_hr).tolist()\n",
        "    yind_final = (np.array(recon_yind) * pixel_size_hr).tolist()\n",
        "\n",
        "    timing_prof.stop_timer('reconstruct_patches_2025')\n",
        "    return reconstructed_image, [frame_index, xind_final, yind_final, confidence_list]\n",
        "\n",
        "\n",
        "def split_image_to_patches(img, num_patches, overlap):\n",
        "    timing_prof.start_timer('split_image_to_patches')\n",
        "\n",
        "    # Determine the non-overlapping patch size.\n",
        "    H, W = img.shape\n",
        "    patch_h = H // num_patches\n",
        "    patch_w = W // num_patches\n",
        "\n",
        "    # Pad the image so that border patches have the proper overlap.\n",
        "    padded_img = np.pad(img, ((overlap, overlap), (overlap, overlap)), mode='reflect')\n",
        "\n",
        "    # Define the window (patch) shape including overlap.\n",
        "    window_shape = (patch_h + 2 * overlap, patch_w + 2 * overlap)\n",
        "\n",
        "    # Create a sliding window view of the padded image.\n",
        "    patches_view = sliding_window_view(padded_img, window_shape)\n",
        "\n",
        "    # Sample patches at strides equal to the basic patch size.\n",
        "    patches_array = patches_view[0::patch_h, 0::patch_w, :, :]\n",
        "\n",
        "    # Flatten the 2D grid of patches (row-major order) into a list.\n",
        "    num_rows, num_cols, ph, pw = patches_array.shape\n",
        "    patches_list = [patches_array[i, j].copy() for i in range(num_rows) for j in range(num_cols)]\n",
        "\n",
        "    timing_prof.stop_timer('split_image_to_patches')\n",
        "    return patches_list\n",
        "\n",
        "def gauss2d(xy, offset, amp, x0, y0, sigma):\n",
        "  x, y = xy\n",
        "  return offset + (amp * np.exp(-((x - x0) ** 2) / (2 * sigma ** 2) - ((y - y0) ** 2) / (2 * sigma ** 2)))\n",
        "\n",
        "def extract_all_features(Images, FOV_size, pixel_size):\n",
        "    timing_prof.start_timer('extract_all_features')\n",
        "\n",
        "    M, N = FOV_size\n",
        "    patch_size = 7\n",
        "    xy = np.zeros([2, int(patch_size ** 2)])\n",
        "    for i1 in range(patch_size):\n",
        "        for j1 in range(patch_size):\n",
        "            xy[:, int(i1 + patch_size * j1)] = [i1, j1]\n",
        "\n",
        "    if(len(Images.shape) == 2):\n",
        "        Images = Images[None, :, :]\n",
        "\n",
        "    peaks_first_frame = peak_local_max(Images[0],\n",
        "                                       min_distance=patch_size // 2,\n",
        "                                       threshold_abs=np.mean(Images[0]) + np.std(Images[0]))\n",
        "    peaks_for_analysis = []\n",
        "    cnt = 0\n",
        "    for i in range(len(peaks_first_frame)):\n",
        "        if (np.sum(np.abs(peaks_first_frame[:, 0] - peaks_first_frame[i, 0]) +\n",
        "                   np.abs(peaks_first_frame[:, 1] - peaks_first_frame[i, 1]) < 2) == 1):\n",
        "            peaks_for_analysis.append([peaks_first_frame[cnt, 0], peaks_first_frame[cnt, 1]])\n",
        "            cnt += 1\n",
        "            if (cnt > 100):\n",
        "                break\n",
        "\n",
        "    peaks_for_analysis = np.array(peaks_for_analysis)\n",
        "    number_of_PSFs_to_fit = np.min([100, peaks_for_analysis.shape[0]])\n",
        "\n",
        "    sigmas_list = []\n",
        "    gaussian_amp_list = []\n",
        "    for i in range(number_of_PSFs_to_fit):\n",
        "        down = np.max([0, peaks_for_analysis[i, 0] - patch_size // 2])\n",
        "        up = np.min([M - 1, peaks_for_analysis[i, 0] + patch_size // 2])\n",
        "        left = np.max([0, peaks_for_analysis[i, 1] - patch_size // 2])\n",
        "        right = np.min([N - 1, peaks_for_analysis[i, 1] + patch_size // 2])\n",
        "        zobs = (Images[0][down:up + 1, left:right + 1]).reshape(1, -1).squeeze()\n",
        "        try:\n",
        "            guess = [np.median(zobs), np.median(zobs), patch_size // 2, patch_size // 2, 1]\n",
        "            bounds = ([0, 0, 0, 0, 0.5], [np.inf, np.inf, patch_size, patch_size, patch_size // 2])\n",
        "            pred_params, uncert_cov = opt.curve_fit(gauss2d, xy, zobs, p0=guess, bounds=bounds)\n",
        "        except Exception as e:\n",
        "            continue\n",
        "        fit = gauss2d(xy, *pred_params)\n",
        "        if (1 - np.sqrt(np.mean((zobs / np.max(zobs) - fit / np.max(fit)) ** 2)) < 0.9):\n",
        "           continue\n",
        "        sigmas_list.append(pred_params[4])\n",
        "        gaussian_amp_list.append(pred_params[1])\n",
        "\n",
        "    if(len(sigmas_list) < 1):\n",
        "        log(\"Did not find emitters for sigma estimation! setting sigma to 1 pixel\")\n",
        "        sigma = 1\n",
        "        sigma_std = 0\n",
        "    else:\n",
        "        sigma = np.mean(sigmas_list)\n",
        "        sigma_std = np.std(sigmas_list)\n",
        "\n",
        "    mean_noise_list = []\n",
        "    std_noise_list = []\n",
        "    emitter_density_list = []\n",
        "    for i in range(np.min([Images.shape[0], 100])):\n",
        "        curr_mean_noise, curr_std_noise, signal_amp, curr_emitter_density = extract_features_frame(Images[i],\n",
        "                                                                                                   pixel_size,\n",
        "                                                                                                   verbose=False)\n",
        "        mean_noise_list.append(curr_mean_noise)\n",
        "        std_noise_list.append(curr_std_noise)\n",
        "        emitter_density_list.append(curr_emitter_density)\n",
        "\n",
        "    ADC_offset = np.mean(mean_noise_list)\n",
        "    ReadOutNoise_ADC = np.mean(std_noise_list)\n",
        "    gaussian_amp_mean = np.mean(gaussian_amp_list)\n",
        "    gaussian_amp_std = np.std(gaussian_amp_list)\n",
        "    emitter_density = np.mean(emitter_density_list)\n",
        "\n",
        "    timing_prof.stop_timer('extract_all_features')\n",
        "    return ADC_offset, ReadOutNoise_ADC, gaussian_amp_mean, gaussian_amp_std, \\\n",
        "           emitter_density, sigma, sigma_std\n",
        "\n",
        "def remove_zero_padding(image):\n",
        "    image_array = np.array(image)\n",
        "    non_zero_rows = np.where(image_array.sum(axis=1) != 0)\n",
        "    non_zero_cols = np.where(image_array.sum(axis=0) != 0)\n",
        "    cropped_image = image_array[non_zero_rows[0][0]:non_zero_rows[0][-1]+1, non_zero_cols[0][0]:non_zero_cols[0][-1]+1]\n",
        "    return cropped_image\n",
        "\n",
        "def subtract_smooth_background(im, sigma=3):\n",
        "    return im - gaussian_filter(im, sigma)\n",
        "\n",
        "def subtract_background_tophat(im, radius=15):\n",
        "    return white_tophat(im, footprint=disk(radius))\n",
        "\n",
        "def extract_features_frame(OrigImage, pixel_size, psf_sigma, offset=None, verbose=False):\n",
        "    M, N = OrigImage.shape\n",
        "\n",
        "    Image = OrigImage - gaussian_filter(OrigImage, sigma=5)\n",
        "\n",
        "    if(offset is not None):\n",
        "        if(np.percentile(gaussian_filter(Image, 2), 99) < 2 * Image.mean() or np.percentile(OrigImage, 99) < 2 * offset):\n",
        "            if(verbose):\n",
        "                plt.figure(figsize=(7, 7))\n",
        "                plt.title(\"SNR is too low - ignoring patch\")\n",
        "                plt.imshow(OrigImage)\n",
        "                plt.show()\n",
        "            return np.mean(OrigImage), np.std(OrigImage), 0, 0\n",
        "\n",
        "    log_image = -gaussian_laplace(Image, sigma=psf_sigma)  # negative = blob-like peaks\n",
        "\n",
        "    # Local maxima filtering\n",
        "    neighborhood_size = 3\n",
        "    local_max = (log_image == maximum_filter(log_image, size=neighborhood_size))\n",
        "\n",
        "    # Compute the threshold\n",
        "    amp_threshold = np.mean(Image) + 0.5 * (np.percentile(Image, 99) - np.mean(Image))\n",
        "\n",
        "    # Apply intensity threshold\n",
        "    pcntl_threshold = np.percentile(Image, 85)\n",
        "    binary_mask = np.logical_and(local_max, Image > np.max([amp_threshold, pcntl_threshold]))\n",
        "\n",
        "    dilated_mask = binary_dilation(binary_mask, structure=np.ones((5, 5)))\n",
        "    noise_mask = np.ones_like(binary_mask)\n",
        "    noise_mask[dilated_mask] = 0\n",
        "\n",
        "    if(np.sum(binary_mask) > 0):\n",
        "        ADC_offset = np.mean(OrigImage[noise_mask])\n",
        "        ReadOutNoise_ADC = np.std(OrigImage[noise_mask])\n",
        "        Signal_amp = np.mean(OrigImage[binary_mask == 1])\n",
        "        emitter_density = (10 ** 6) * float(np.sum(binary_mask)) / (M * N * pixel_size ** 2)\n",
        "    else:\n",
        "        if(verbose):\n",
        "            log(\"Didn't find any emitters\")\n",
        "        return np.mean(OrigImage), np.std(OrigImage), 0, 0\n",
        "\n",
        "    if(Signal_amp / ADC_offset < 2.5):\n",
        "        if(emitter_density > 2):\n",
        "            if(verbose):\n",
        "                plt.figure(figsize=(8, 8))\n",
        "                plt.title(\"SNR is too low for emitter density estimation\")\n",
        "                plt.imshow(OrigImage)\n",
        "                plt.show()\n",
        "\n",
        "            return ADC_offset, ReadOutNoise_ADC, Signal_amp, 0\n",
        "\n",
        "    if(verbose):\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.subplot(131)\n",
        "        plt.imshow(OrigImage)\n",
        "        plt.title(\"Offset = {}\".format(offset))\n",
        "        plt.subplot(132)\n",
        "        plt.imshow(binary_mask)\n",
        "        plt.title(\"signal mask - emitter density {:.3f}\".format(emitter_density))\n",
        "        plt.subplot(133)\n",
        "        plt.imshow(noise_mask)\n",
        "        plt.title(\"noise mask - SNR pred {:.3f}\".format(Signal_amp / ADC_offset))\n",
        "        plt.show()\n",
        "\n",
        "    return ADC_offset, ReadOutNoise_ADC, Signal_amp, emitter_density\n",
        "\n",
        "def project_01(im):\n",
        "    im = np.squeeze(im)\n",
        "    min_val = im.min()\n",
        "    max_val = im.max()\n",
        "    return (im - min_val)/(max_val - min_val)\n",
        "\n",
        "def project_01_ret_vals(im):\n",
        "    im = np.squeeze(im)\n",
        "    min_val = im.min()\n",
        "    max_val = im.max()\n",
        "    return (im - min_val)/(max_val - min_val), min_val, max_val\n",
        "\n",
        "def normalize_im(im, dmean, dstd):\n",
        "    im = np.squeeze(im)\n",
        "    return (im - dmean)/dstd\n",
        "\n",
        "def conv_bn_relu(nb_filter, rk, ck, name):\n",
        "    def f(input_tensor):\n",
        "        conv = Conv2D(nb_filter, kernel_size=(rk, ck), strides=(1,1),\n",
        "                      padding=\"same\", use_bias=False,\n",
        "                      kernel_initializer=\"Orthogonal\", name='conv-'+name)(input_tensor)\n",
        "        conv_norm = BatchNormalization(name='BN-'+name)(conv)\n",
        "        conv_norm_relu = Activation(\"relu\", name='Relu-'+name)(conv_norm)\n",
        "        return conv_norm_relu\n",
        "    return f\n",
        "\n",
        "def CNN(input_tensor, names):\n",
        "    Features1 = conv_bn_relu(32,3,3,names+'F1')(input_tensor)\n",
        "    pool1 = MaxPooling2D(pool_size=(2,2), name=names+'Pool1')(Features1)\n",
        "    Features2 = conv_bn_relu(64,3,3,names+'F2')(pool1)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2), name=names+'Pool2')(Features2)\n",
        "    Features3 = conv_bn_relu(128,3,3,names+'F3')(pool2)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2), name=names+'Pool3')(Features3)\n",
        "    Features4 = conv_bn_relu(512,3,3,names+'F4')(pool3)\n",
        "    up5 = UpSampling2D(size=(2, 2), name=names+'Upsample1')(Features4)\n",
        "    Features5 = conv_bn_relu(128,3,3,names+'F5')(up5)\n",
        "    up6 = UpSampling2D(size=(2, 2), name=names+'Upsample2')(Features5)\n",
        "    Features6 = conv_bn_relu(64,3,3,names+'F6')(up6)\n",
        "    up7 = UpSampling2D(size=(2, 2), name=names+'Upsample3')(Features6)\n",
        "    Features7 = conv_bn_relu(32,3,3,names+'F7')(up7)\n",
        "    return Features7\n",
        "\n",
        "def buildModel(input_dim, initial_learning_rate=0.001):\n",
        "    input_ = Input(shape=input_dim)\n",
        "    act_ = CNN(input_, 'CNN')\n",
        "    density_pred = Conv2D(1, kernel_size=(1, 1), strides=(1, 1), padding=\"same\",\n",
        "                           activation=\"linear\", use_bias=False,\n",
        "                           kernel_initializer=\"Orthogonal\", name='Prediction')(act_)\n",
        "    model = Model(inputs=input_, outputs=density_pred)\n",
        "    opt = Adam(learning_rate=initial_learning_rate)\n",
        "    model.compile(optimizer=opt, loss=L1L2loss(input_dim))\n",
        "    return model\n",
        "\n",
        "def CNN_upsample(input, upsampling_factor):\n",
        "    # Encoder\n",
        "    x = Conv2D(32, (3, 3), padding='same', name=\"F1\")(input)\n",
        "    x = BatchNormalization(name=\"BN_1\")(x)\n",
        "    x = Activation('relu',name=\"ReLU_1\")(x)\n",
        "\n",
        "    x = Conv2D(64, (3, 3), padding='same', name=\"F2\")(x)\n",
        "    x = BatchNormalization(name=\"BN_2\")(x)\n",
        "    x = Activation('relu', name=\"ReLU_2\")(x)\n",
        "\n",
        "    x = Conv2D(128, (3, 3), padding='same', name=\"F3\")(x)\n",
        "    x = BatchNormalization(name=\"BN_3\")(x)\n",
        "    x = Activation('relu', name=\"ReLU_3\")(x)\n",
        "\n",
        "    x = Conv2D(256, (3, 3), padding='same', name=\"F4\")(x)\n",
        "    x = BatchNormalization(name=\"BN_4\")(x)\n",
        "    x = Activation('relu', name=\"ReLU_4\")(x)\n",
        "\n",
        "    # Decoder\n",
        "    x = Conv2D(128, (3, 3), padding='same', name=\"F5\")(x)\n",
        "    x = BatchNormalization(name=\"BN_5\")(x)\n",
        "    x = Activation('relu', name=\"ReLU_5\")(x)\n",
        "\n",
        "    x = Conv2D(64, (3, 3), padding='same', name=\"F6\")(x)\n",
        "    x = BatchNormalization(name=\"BN_6\")(x)\n",
        "    x = Activation('relu', name=\"ReLU_6\")(x)\n",
        "\n",
        "    for ind, scale in enumerate(range(int(np.log2(upsampling_factor)))):\n",
        "        x = UpSampling2D(size=(2, 2), interpolation='bilinear', name=\"upsample_{}\".format(ind+1))(x)\n",
        "        x = Conv2D(32, (5, 5), padding='same', name=\"conv_upsample{}\".format(ind+1))(x)\n",
        "        x = BatchNormalization(name=\"BN_upsample{}\".format(ind+1))(x)\n",
        "        x = Activation('relu', name=\"ReLU_upsample{}\".format(ind+1))(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def build_model_upsample(input_shape, lr=0.001, upsampling_factor=2):\n",
        "    input_ = Input(shape=input_shape)\n",
        "    act_ = CNN_upsample(input_, upsampling_factor)\n",
        "    density_pred = Conv2D(1, kernel_size=(1, 1), strides=(1, 1), padding=\"same\",\n",
        "                                  activation=\"linear\", use_bias = False,\n",
        "                                  kernel_initializer=\"Orthogonal\",name='Prediction')(act_)\n",
        "    model = Model(inputs= input_, outputs=density_pred)\n",
        "    opt = Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=opt, loss = custom_loss(input_shape))\n",
        "    return model\n",
        "\n",
        "def custom_loss(input_shape):\n",
        "    def loss_fn(y_true, y_pred):\n",
        "        heatmap_pred = tf.nn.conv2d(y_pred, gfilter, strides=1, padding='SAME')\n",
        "        loss_heatmaps = tf.reduce_mean(tf.square(y_true - heatmap_pred))\n",
        "        loss_spikes = tf.reduce_mean(tf.abs(y_pred))\n",
        "        return loss_heatmaps + loss_spikes\n",
        "\n",
        "    return loss_fn\n",
        "\n",
        "def matlab_style_gauss2D(shape=(7,7), sigma=1):\n",
        "    m, n = [(ss-1.)/2. for ss in shape]\n",
        "    y, x = np.ogrid[-m:m+1, -n:n+1]\n",
        "    h = np.exp( -(x*x + y*y) / (2.*sigma*sigma) )\n",
        "    h = h.astype(K.floatx())\n",
        "    h[h < np.finfo(h.dtype).eps * h.max()] = 0\n",
        "    sumh = h.sum()\n",
        "    if sumh != 0:\n",
        "        h /= sumh\n",
        "    h = h * 2.0\n",
        "    h = h.astype('float32')\n",
        "    return h\n",
        "\n",
        "# Expand the filter dimensions\n",
        "psf_heatmap = matlab_style_gauss2D(shape=(7,7), sigma=1)\n",
        "gfilter = tf.reshape(psf_heatmap, [7, 7, 1, 1])\n",
        "\n",
        "# Combined MSE + L1 loss\n",
        "def L1L2loss(input_shape):\n",
        "    def bump_mse(heatmap_true, spikes_pred):\n",
        "        heatmap_pred = K.conv2d(spikes_pred, gfilter, strides=(1, 1), padding='same')\n",
        "        loss_heatmaps = losses.mean_squared_error(heatmap_true, heatmap_pred)\n",
        "        loss_spikes = losses.mean_absolute_error(spikes_pred, tf.zeros(input_shape))\n",
        "        return loss_heatmaps + loss_spikes\n",
        "    return bump_mse\n",
        "\n",
        "def getPixelSizeTIFFmetadata(TIFFpath, display=False):\n",
        "  with Image.open(TIFFpath) as img:\n",
        "    meta_dict = {TAGS[key] : img.tag[key] for key in img.tag.keys()}\n",
        "\n",
        "  ResolutionUnit = meta_dict['ResolutionUnit'][0]\n",
        "  width = meta_dict['ImageWidth'][0]\n",
        "  height = meta_dict['ImageLength'][0]\n",
        "  xResolution = meta_dict['XResolution'][0]\n",
        "  if len(xResolution) == 1:\n",
        "    xResolution = xResolution[0]\n",
        "  elif len(xResolution) == 2:\n",
        "    xResolution = xResolution[0]/xResolution[1]\n",
        "  else:\n",
        "    log('Image resolution not defined.')\n",
        "    xResolution = 1\n",
        "\n",
        "  if ResolutionUnit == 2:\n",
        "    pixel_size = 0.025*1e9/xResolution\n",
        "  elif ResolutionUnit == 3:\n",
        "    pixel_size = 0.01*1e9/xResolution\n",
        "  else:\n",
        "    log('Resolution unit not defined. Assuming: um')\n",
        "    pixel_size = 1e3/xResolution\n",
        "\n",
        "  if display:\n",
        "    log('Pixel size obtained from metadata: '+str(pixel_size)+' nm')\n",
        "    log('Image size: '+str(width)+'x'+str(height))\n",
        "\n",
        "  return (pixel_size, width, height)\n",
        "\n",
        "def saveAsTIF(path, filename, array, pixel_size):\n",
        "  if (array.dtype == np.uint16):\n",
        "    mode = 'I;16'\n",
        "  elif (array.dtype == np.uint32):\n",
        "    mode = 'I'\n",
        "  else:\n",
        "    mode = 'F'\n",
        "\n",
        "  if len(array.shape) == 2:\n",
        "    im = Image.fromarray(array)\n",
        "    im.save(os.path.join(path, filename+'.tif'),\n",
        "                  mode=mode,\n",
        "                  resolution_unit=3,\n",
        "                  resolution=0.01*1e9/pixel_size)\n",
        "  elif len(array.shape) == 3:\n",
        "    imlist = []\n",
        "    for frame in array:\n",
        "      imlist.append(Image.fromarray(frame))\n",
        "    imlist[0].save(os.path.join(path, filename+'.tif'), save_all=True,\n",
        "                  append_images=imlist[1:],\n",
        "                  mode=mode,\n",
        "                  resolution_unit=3,\n",
        "                  resolution=0.01*1e9/pixel_size)\n",
        "  return\n",
        "\n",
        "class Maximafinder(Layer):\n",
        "    def __init__(self, thresh, neighborhood_size, use_local_avg, **kwargs):\n",
        "        super(Maximafinder, self).__init__(**kwargs)\n",
        "        self.thresh = tf.constant(thresh, dtype=tf.float32)\n",
        "        self.nhood = neighborhood_size\n",
        "        self.use_local_avg = use_local_avg\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if self.use_local_avg:\n",
        "          self.kernel_x = tf.reshape(tf.constant([[-1,0,1],[-1,0,1],[-1,0,1]], dtype=tf.float32), [3, 3, 1, 1])\n",
        "          self.kernel_y = tf.reshape(tf.constant([[-1,-1,-1],[0,0,0],[1,1,1]], dtype=tf.float32), [3, 3, 1, 1])\n",
        "          self.kernel_sum = tf.reshape(tf.constant([[1,1,1],[1,1,1],[1,1,1]], dtype=tf.float32), [3, 3, 1, 1])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        max_pool_image = MaxPooling2D(pool_size=(self.nhood,self.nhood), strides=(1,1), padding='same')(inputs)\n",
        "        cond = tf.math.greater(max_pool_image, self.thresh) & tf.math.equal(max_pool_image, inputs)\n",
        "        indices = tf.where(cond)\n",
        "        bind, xind, yind = indices[:, 0], indices[:, 2], indices[:, 1]\n",
        "        confidence = tf.gather_nd(inputs, indices)\n",
        "\n",
        "        if self.use_local_avg:\n",
        "          x_image = K.conv2d(inputs, self.kernel_x, padding='same')\n",
        "          y_image = K.conv2d(inputs, self.kernel_y, padding='same')\n",
        "          sum_image = K.conv2d(inputs, self.kernel_sum, padding='same')\n",
        "          confidence = tf.cast(tf.gather_nd(sum_image, indices), dtype=tf.float32)\n",
        "          x_local = tf.math.divide(tf.gather_nd(x_image, indices), tf.gather_nd(sum_image, indices))\n",
        "          y_local = tf.math.divide(tf.gather_nd(y_image, indices), tf.gather_nd(sum_image, indices))\n",
        "          xind = tf.cast(xind, dtype=tf.float32) + tf.cast(x_local, dtype=tf.float32)\n",
        "          yind = tf.cast(yind, dtype=tf.float32) + tf.cast(y_local, dtype=tf.float32)\n",
        "        else:\n",
        "          xind = tf.cast(xind, dtype=tf.float32)\n",
        "          yind = tf.cast(yind, dtype=tf.float32)\n",
        "\n",
        "        return bind, xind, yind, confidence\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super(Maximafinder, self).get_config()\n",
        "        config = {}\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "def iter_tiff_frames(path):\n",
        "    \"\"\"Yield frames (float32 HxW) and return total count at the end.\"\"\"\n",
        "    with tiff.TiffFile(path) as tif:\n",
        "        number_of_frames = len(tif.pages)\n",
        "        for page in tif.pages:\n",
        "            yield page.asarray().astype(np.float32)\n",
        "\n",
        "def count_tiff_frames(path):\n",
        "    with tiff.TiffFile(path) as tif:\n",
        "        return len(tif.pages)\n",
        "\n",
        "def is_nd2(path: str) -> bool:\n",
        "    try:\n",
        "        import nd2\n",
        "        return nd2.is_supported_file(path)\n",
        "    except Exception:\n",
        "        return path.lower().endswith(\".nd2\")\n",
        "\n",
        "def count_nd2_frames(path: str) -> int:\n",
        "    import nd2\n",
        "    with nd2.ND2File(path) as f:\n",
        "        try:\n",
        "            return len(f.loop_indices)\n",
        "        except Exception:\n",
        "            sz = getattr(f, \"sizes\", {}) or {}\n",
        "            prod = 1\n",
        "            for ax in (\"T\", \"Z\", \"C\", \"V\"):\n",
        "                prod *= int(sz.get(ax, 1))\n",
        "            return prod\n",
        "\n",
        "def _nd2_to_2d(arr, channel=None):\n",
        "    import numpy as np\n",
        "    a = np.asarray(arr)\n",
        "    if a.ndim == 2:\n",
        "        return a\n",
        "    if a.ndim == 3:\n",
        "        if a.shape[-1] in (1, 3, 4):\n",
        "            idx = channel if (channel is not None and channel < a.shape[-1]) else 0\n",
        "            return a[..., idx]\n",
        "        if a.shape[0] in (1, 3, 4):\n",
        "            idx = channel if (channel is not None and channel < a.shape[0]) else 0\n",
        "            return a[idx, ...]\n",
        "        return a.mean(axis=0)\n",
        "    a = a.squeeze()\n",
        "    return a if a.ndim == 2 else a.reshape(a.shape[-2], a.shape[-1])\n",
        "\n",
        "def iter_nd2_frames(path: str, channel: int | None = None):\n",
        "    import nd2, numpy as np\n",
        "    n = count_nd2_frames(path)\n",
        "    with nd2.ND2File(path) as f:\n",
        "        for i in range(n):\n",
        "            fr = f.read_frame(i)\n",
        "            fr2d = _nd2_to_2d(fr, channel=channel)\n",
        "            yield fr2d.astype(np.float32, copy=False)\n",
        "\n",
        "def getPixelSizeND2metadata(path, display=False):\n",
        "    import nd2\n",
        "    with nd2.ND2File(path) as f:\n",
        "        vox_um = getattr(f, \"voxel_size\", None)\n",
        "        if vox_um is None:\n",
        "            return None, None, None\n",
        "        px_nm = vox_um[2] * 1e3\n",
        "        try:\n",
        "            h, w = f.shape[-2], f.shape[-1]\n",
        "        except Exception:\n",
        "            h = w = None\n",
        "        if display:\n",
        "            print(f\"Pixel size (ND2): {px_nm:.2f} nm | image ~ {w}x{h}\")\n",
        "        return px_nm, w, h\n",
        "\n",
        "def list_files_multi(directory, extensions):\n",
        "    exts = {('.' + e.lower()) for e in extensions}\n",
        "    for f in os.listdir(directory):\n",
        "        if os.path.splitext(f)[1].lower() in exts:\n",
        "            yield f\n",
        "\n",
        "def list_files(directory, extension):\n",
        "  return (f for f in os.listdir(directory) if f.endswith('.' + extension))\n",
        "\n",
        "log('--------------------------------')\n",
        "log('AutoDS installation complete.')\n",
        "\n",
        "if tf.test.gpu_device_name() == '':\n",
        "  log('You do not have GPU access.')\n",
        "  log('Did you change your runtime?')\n",
        "  log('If the runtime settings are correct then GPU might not be allocated to your session.')\n",
        "  log('Expect slow performance. To access GPU try reconnecting later.')\n",
        "else:\n",
        "  log('You have GPU access')\n",
        "\n",
        "log('Tensorflow version is ' + str(tf.__version__))\n",
        "\n",
        "MAX_FILE_GB = 5.0\n",
        "\n",
        "def _is_oom(exc: BaseException) -> bool:\n",
        "    msg = (str(exc) or \"\").upper()\n",
        "    return (\n",
        "        isinstance(exc, tf.errors.ResourceExhaustedError) or\n",
        "        isinstance(exc, MemoryError) or\n",
        "        \"OUT OF MEMORY\" in msg or \"OOM\" in msg or\n",
        "        exc.__class__.__name__ in {\"_ArrayMemoryError\"}\n",
        "    )\n",
        "\n",
        "@contextmanager\n",
        "def catch_oom(phase: str, detail: str = \"\", on_oom=\"continue\"):\n",
        "    \"\"\"\n",
        "    Wrap any memory-heavy block. Prints a friendly message on OOM and continues.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        yield\n",
        "    except Exception as e:\n",
        "        if _is_oom(e):\n",
        "            print(f\"\\n⚠️  OOM while {phase}{(' - ' + detail) if detail else ''}.\")\n",
        "            print(\"   Tip: reduce chunk_size/batch_size/upsampling, or downsample input.\")\n",
        "            if isinstance(e, tf.errors.ResourceExhaustedError) and getattr(e, \"message\", None):\n",
        "                print(\"   TensorFlow says:\", e.message.splitlines()[0][:200])\n",
        "            else:\n",
        "                traceback.print_exc(limit=1, file=sys.stdout)\n",
        "            if on_oom != \"continue\":\n",
        "                raise\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "verbose = False\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN PROCESSING WITH GITHUB DATA DOWNLOAD\n",
        "# ============================================================================\n",
        "\n",
        "# ------------------------------- User input -------------------------------\n",
        "# Set to GitHub URL or local path\n",
        "Result_folder = \"/content/gdrive/MyDrive/AutoDS/Results/TOM20_10nM/V1\"  #@param {type:\"string\"}\n",
        "\n",
        "threshold = 10 #@param {type:\"number\"}\n",
        "neighborhood_size = 3 #@param {type:\"integer\"}\n",
        "use_local_average = True #@param {type:\"boolean\"}\n",
        "\n",
        "num_patches = 8 #@param {type:\"number\"}\n",
        "overlap = 4\n",
        "batch_size = 32 #@param {type:\"integer\"}\n",
        "\n",
        "interpolate_based_on_imaging_parameters = True #@param {type:\"boolean\"}\n",
        "get_pixel_size_from_file = False #@param {type:\"boolean\"}\n",
        "pixel_size = 233 #@param {type:\"number\"}\n",
        "wavelength = 233 #@param {type:\"number\"}\n",
        "numerical_aperture = 1.49 #@param {type:\"number\"}\n",
        "\n",
        "chunk_size = 10000 #@param {type:\"number\"}\n",
        "\n",
        "psf_sigma_nm = 0.21 * wavelength / numerical_aperture\n",
        "psf_sigma_pixels = psf_sigma_nm / pixel_size\n",
        "\n",
        "if get_pixel_size_from_file:\n",
        "  pixel_size = None\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# PROCESSING PIPELINE\n",
        "# ============================================================================\n",
        "timing_prof.start_timer('total_processing')\n",
        "\n",
        "matfile = sio.loadmat(os.path.join(prediction_model_path, model_names[0], 'model_metadata.mat'))\n",
        "try:\n",
        "    model_wavelength = np.array(matfile['wavelength'].item())\n",
        "except:\n",
        "    model_wavelength = None\n",
        "try:\n",
        "    model_NA = np.array(matfile['numerical_aperture'].item())\n",
        "except:\n",
        "    model_NA = None\n",
        "try:\n",
        "    model_pixel_size = np.array(matfile['pixel_size'].item())\n",
        "except:\n",
        "    model_pixel_size = None\n",
        "\n",
        "if os.path.isdir(Data_folder):\n",
        "    for filename in list_files_multi(Data_folder, extensions=['tif','tiff','nd2']):\n",
        "        timing_prof.start_timer(f'file_{filename}')\n",
        "\n",
        "        if filename.lower().endswith('.nd2'):\n",
        "            try:\n",
        "                import nd2\n",
        "            except Exception:\n",
        "                get_ipython().system('pip install -q nd2')\n",
        "\n",
        "        in_path = os.path.join(Data_folder, filename)\n",
        "\n",
        "        # File size guard\n",
        "        try:\n",
        "            file_size_gb = os.path.getsize(in_path) / 1e9\n",
        "            log(f\"\\nProcessing: {filename} ({file_size_gb:.2f} GB)\")\n",
        "            if file_size_gb > MAX_FILE_GB:\n",
        "                print(f\"\\n⚠️  {filename}: {file_size_gb:.2f} GB > {MAX_FILE_GB:.2f} GB.\")\n",
        "                print(\"   Video size is too big, please use Google Colab Pro or run locally.\")\n",
        "                continue\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # Resolve pixel size if requested\n",
        "        if get_pixel_size_from_file:\n",
        "            if is_tiff(in_path):\n",
        "                with catch_oom(\"reading TIFF pixel size\", filename):\n",
        "                    pixel_size, _, _ = getPixelSizeTIFFmetadata(in_path, True)\n",
        "            elif is_nd2(in_path):\n",
        "                with catch_oom(\"reading ND2 pixel size\", filename):\n",
        "                    px_nm, _, _ = getPixelSizeND2metadata(in_path, True)\n",
        "                    pixel_size = px_nm if px_nm is not None else pixel_size\n",
        "\n",
        "        # Common model params\n",
        "        upsampling_factor = np.array(matfile['upsampling_factor']).item()\n",
        "        try:\n",
        "            L2_weighting_factor = np.array(matfile['Normalization factor']).item()\n",
        "        except:\n",
        "            L2_weighting_factor = 100\n",
        "\n",
        "        patches_list = [[] for _ in model_names]\n",
        "        patch_indices_list = [[] for _ in model_names]\n",
        "        frame_numbers = [[] for _ in model_names]\n",
        "\n",
        "        # Choose reader & frame count\n",
        "        timing_prof.start_timer('frame_loading')\n",
        "        number_of_frames, frame_iter = None, None\n",
        "        with catch_oom(\"opening stack\", filename):\n",
        "            if is_tiff(in_path):\n",
        "                number_of_frames = count_tiff_frames(in_path)\n",
        "                frame_iter = iter_tiff_frames(in_path)\n",
        "                log(f'Loaded tiff stack with {number_of_frames} frames')\n",
        "            elif is_nd2(in_path):\n",
        "                number_of_frames = count_nd2_frames(in_path)\n",
        "                frame_iter = iter_nd2_frames(in_path)\n",
        "                log(f'Loaded ND2 stack with ~{number_of_frames} planes (T*Z*C)')\n",
        "            else:\n",
        "                log(f\"Skipping unsupported file: {filename}\")\n",
        "        timing_prof.stop_timer('frame_loading')\n",
        "\n",
        "        if frame_iter is None:\n",
        "            print(f\"⚠️  Skipping {filename} due to earlier error.\")\n",
        "            continue\n",
        "\n",
        "        # Initialize sum image\n",
        "        sum_image = None\n",
        "\n",
        "        log('Splitting stack to patches and selecting Deep-STORM model')\n",
        "        timing_prof.start_timer('patch_splitting')\n",
        "        for i, frame in enumerate(tqdm(frame_iter, total=number_of_frames if number_of_frames else None)):\n",
        "\n",
        "            if sum_image is None:\n",
        "                with catch_oom(\"allocating preview buffer\", f\"{filename} sum_image\"):\n",
        "                    sum_image = np.zeros_like(frame, dtype=np.float32)\n",
        "                    if sum_image is None:\n",
        "                        print(f\"⚠️  Skipping {filename}: failed to allocate preview buffer.\")\n",
        "                        break\n",
        "\n",
        "            # Keep running average for preview\n",
        "            with catch_oom(\"accumulating preview\", f\"{filename} frame {i}\"):\n",
        "                sum_image += frame.astype(np.float32) / max(1, number_of_frames or 1)\n",
        "\n",
        "            # Interpolate to match model resolution\n",
        "            with catch_oom(\"interpolating frame\", f\"{filename} frame {i}\"):\n",
        "                if interpolate_based_on_imaging_parameters:\n",
        "                    frame_i = interpolate_frames(\n",
        "                        frame,\n",
        "                        model_pixel_size, pixel_size,\n",
        "                        model_wavelength, wavelength,\n",
        "                        model_NA, numerical_aperture\n",
        "                    )[0]\n",
        "                else:\n",
        "                    frame_i = frame\n",
        "\n",
        "            if 'frame_i' not in locals():\n",
        "                continue\n",
        "\n",
        "            M, N = frame_i.shape\n",
        "\n",
        "            # Background subtraction + standardization\n",
        "            fproc = np.asarray(frame_i, dtype=np.float32)\n",
        "            p35   = np.percentile(fproc, 35)\n",
        "            fproc = fproc - p35\n",
        "            fproc = fproc - fproc.min()\n",
        "\n",
        "            fmean = fproc.mean(dtype=np.float64)\n",
        "            fstd  = fproc.std(dtype=np.float64) + 1e-6\n",
        "            fproc = (fproc - fmean) / fstd\n",
        "\n",
        "            # Split into patches\n",
        "            with catch_oom(\"splitting into patches\", f\"{filename} frame {i}\"):\n",
        "                patches = split_image_to_patches(fproc, num_patches, overlap)\n",
        "            if 'patches' not in locals():\n",
        "                continue\n",
        "\n",
        "            offset = fproc.mean()\n",
        "\n",
        "            # Per-patch difficulty selection\n",
        "            for m in range(num_patches):\n",
        "                for n in range(num_patches):\n",
        "                    down  = overlap if m == 0 else 0\n",
        "                    up    = (M // num_patches) - overlap if m == num_patches - 1 else (M // num_patches)\n",
        "                    left  = overlap if n == 0 else 0\n",
        "                    right = (N // num_patches) - overlap if n == num_patches - 1 else (N // num_patches)\n",
        "\n",
        "                    with catch_oom(\"extracting features\", f\"{filename} frame {i} patch ({m},{n})\"):\n",
        "                        outputs = extract_features_frame(\n",
        "                            patches[m*num_patches+n][down:up, left:right],\n",
        "                            pixel_size,\n",
        "                            psf_sigma_pixels,\n",
        "                            offset=offset,\n",
        "                            verbose=verbose\n",
        "                        )\n",
        "                    if 'outputs' not in locals():\n",
        "                        continue\n",
        "\n",
        "                    curr_mean_noise, curr_std_noise, signal_amp, curr_emitter_density = outputs\n",
        "\n",
        "                    if (signal_amp == 0 or curr_mean_noise == 0):\n",
        "                        continue\n",
        "                    if any(np.isnan(v) for v in (signal_amp, curr_mean_noise, curr_std_noise, curr_emitter_density)):\n",
        "                        continue\n",
        "\n",
        "                    difficulty_choice = ChooseNetByDifficulty_2025(curr_emitter_density, signal_amp/curr_mean_noise)\n",
        "                    patches_list[difficulty_choice].append(patches[m*num_patches+n])\n",
        "                    patch_indices_list[difficulty_choice].append(m*num_patches+n)\n",
        "                    frame_numbers[difficulty_choice].append(i)\n",
        "\n",
        "        timing_prof.stop_timer('patch_splitting')\n",
        "\n",
        "        # Model histogram\n",
        "        selected_model_hist = np.array([len(p) for p in patches_list], dtype=float)\n",
        "        if HEADLESS_PREVIEW:\n",
        "            with catch_oom(\"plotting model histogram\", filename):\n",
        "                plt.figure(figsize=(5, 3))\n",
        "                plt.bar(np.arange(len(model_names)), selected_model_hist, width=0.8)\n",
        "                plt.xticks(np.arange(len(model_names)), model_names)\n",
        "                plt.xlabel('selected model')\n",
        "                plt.ylabel('number of patches to be analyzed by this model')\n",
        "                plt.show()\n",
        "\n",
        "        # If nothing collected, skip reconstruction\n",
        "        if sum(len(p) for p in patches_list) == 0:\n",
        "            print(f\"ℹ️  No usable patches for {filename}; skipping reconstruction.\")\n",
        "            continue\n",
        "\n",
        "        # M,N from last frame_i\n",
        "        patchwise_recon = np.zeros([M * upsampling_factor, N * upsampling_factor], dtype=np.float32)\n",
        "        frame_number_list, x_nm_list, y_nm_list, confidence_au_list = [], [], [], []\n",
        "\n",
        "        log('Analyzing patches for each model')\n",
        "        timing_prof.start_timer('reconstruction_all_models')\n",
        "        for model_num, model_name in enumerate(model_names):\n",
        "            model_dir = os.path.join(prediction_model_path, model_name)\n",
        "            if os.path.exists(model_dir):\n",
        "                log(f\"The {os.path.basename(model_dir)} model will be used.\")\n",
        "            else:\n",
        "                log('!! WARNING: The chosen model does not exist !!')\n",
        "                log('Please make sure you provide a valid model path before proceeding further.')\n",
        "\n",
        "            if use_local_average:\n",
        "                log('Using local averaging')\n",
        "\n",
        "            if not os.path.exists(Result_folder):\n",
        "                log('Result folder was created.')\n",
        "                os.makedirs(Result_folder, exist_ok=True)\n",
        "\n",
        "            if patches_list[model_num]:\n",
        "                log(\"Reconstructing in chunks\")\n",
        "                total_chunks = (len(patches_list[model_num]) // chunk_size) + 1\n",
        "                for chunk_num in tqdm(range(total_chunks)):\n",
        "                    chunk_start = chunk_num * chunk_size\n",
        "                    chunk_end = min((chunk_num + 1) * chunk_size, len(patches_list[model_num]))\n",
        "\n",
        "                    if chunk_start >= chunk_end:\n",
        "                        continue\n",
        "\n",
        "                    with catch_oom(\"reconstructing chunk\",\n",
        "                                   detail=f\"{model_name} [{chunk_start}:{chunk_end}] of {len(patches_list[model_num])}\"):\n",
        "                        pw_recon, loc_list = reconstruct_patches_2025(\n",
        "                            patches_list[model_num][chunk_start:chunk_end],\n",
        "                            patch_indices_list[model_num][chunk_start:chunk_end],\n",
        "                            frame_numbers[model_num][chunk_start:chunk_end],\n",
        "                            os.path.join(prediction_model_path, model_names[model_num], 'best_weights.h5'),\n",
        "                            num_patches,\n",
        "                            overlap * upsampling_factor,\n",
        "                            number_of_frames,\n",
        "                            threshold,\n",
        "                            neighborhood_size=neighborhood_size,\n",
        "                            use_local_avg=use_local_average,\n",
        "                            upsampling_factor=upsampling_factor,\n",
        "                            pixel_size=pixel_size,\n",
        "                            batch_size=batch_size\n",
        "                        )\n",
        "\n",
        "                        if 'pw_recon' in locals() and pw_recon is not None:\n",
        "                            frame_number_list += loc_list[0]\n",
        "                            x_nm_list += loc_list[1]\n",
        "                            y_nm_list += loc_list[2]\n",
        "                            confidence_au_list += loc_list[3]\n",
        "\n",
        "                            patchwise_recon[:M//num_patches*upsampling_factor*num_patches,\n",
        "                                            :N//num_patches*upsampling_factor*num_patches] += pw_recon\n",
        "\n",
        "        timing_prof.stop_timer('reconstruction_all_models')\n",
        "\n",
        "        ext = '_avg' if use_local_average else '_max'\n",
        "        base = os.path.splitext(filename)[0]\n",
        "\n",
        "        # Save outputs\n",
        "        with catch_oom(\"saving outputs\", base):\n",
        "            with open(os.path.join(Result_folder, f'Localizations_{base}{ext}.csv'), \"w\", newline='') as file:\n",
        "                writer = csv.writer(file)\n",
        "                writer.writerow(['frame', 'x [nm]', 'y [nm]', 'confidence [a.u]'])\n",
        "                sort_ind = np.argsort(frame_number_list)\n",
        "                locs = list(zip(list(np.array(frame_number_list)[sort_ind]),\n",
        "                                list(np.array(x_nm_list)[sort_ind]),\n",
        "                                list(np.array(y_nm_list)[sort_ind]),\n",
        "                                list(np.array(confidence_au_list)[sort_ind])))\n",
        "                writer.writerows(locs)\n",
        "\n",
        "            pw_recon_tif = np.copy(patchwise_recon)\n",
        "            cap = np.percentile(pw_recon_tif, 99.5)\n",
        "            pw_recon_tif[pw_recon_tif > cap] = cap\n",
        "            saveAsTIF(Result_folder, f'Predicted_patchwise_{base}', pw_recon_tif, pixel_size/upsampling_factor)\n",
        "        timing_prof.stop_timer('saving_results')\n",
        "\n",
        "        log('--------------------------------------------------------------------')\n",
        "        log('---------------------------- Previews ------------------------------')\n",
        "        log('--------------------------------------------------------------------')\n",
        "        if HEADLESS_PREVIEW:\n",
        "            with catch_oom(\"plotting previews\", filename):\n",
        "                fig, axes = plt.subplots(1, 3, figsize=(20,16))\n",
        "                axes[0].axis('off'); axes[0].imshow(sum_image); axes[0].set_title('Original', fontsize=15)\n",
        "                axes[1].axis('off'); axes[1].imshow(patchwise_recon); axes[1].set_title('Prediction', fontsize=15)\n",
        "                axes[2].axis('off'); axes[2].imshow(np.clip(patchwise_recon,\n",
        "                                                            np.percentile(patchwise_recon, 1),\n",
        "                                                            np.percentile(patchwise_recon, 99)))\n",
        "                axes[2].set_title('Normalized Prediction', fontsize=15)\n",
        "                plt.show()\n",
        "\n",
        "timing_prof.stop_timer('total_processing')\n",
        "\n",
        "# ============================================================================\n",
        "# PRINT TIMING SUMMARY\n",
        "# ============================================================================\n",
        "log('\\n' + '=' * 84)\n",
        "log('TIMING SUMMARY')\n",
        "log('=' * 84)\n",
        "timing_prof.print_timing_summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **V2: PyTorch Version**\n",
        "1. full pyTorch compatibility\n",
        "2. frame-wize preprocessing (instead of model-wize)"
      ],
      "metadata": {
        "id": "5UW6KRIzcVz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import urllib.request\n",
        "from contextlib import contextmanager\n",
        "\n",
        "import numpy as np\n",
        "import tifffile as tiff\n",
        "from PIL import Image\n",
        "from PIL.TiffTags import TAGS\n",
        "\n",
        "def log(*args, **kwargs):\n",
        "    if not config.QUIET:\n",
        "        print(*args, **kwargs)\n",
        "\n",
        "def list_files_multi(directory, extensions):\n",
        "    exts = {('.' + e.lower()) for e in extensions}\n",
        "    for f in os.listdir(directory):\n",
        "        if os.path.splitext(f)[1].lower() in exts:\n",
        "            yield f\n",
        "\n",
        "@contextmanager\n",
        "def catch_oom(phase: str, detail: str = \"\", on_oom=\"continue\"):\n",
        "    \"\"\"\n",
        "    Wrap any memory-heavy block. Prints a friendly message on OOM and continues.\n",
        "    on_oom: \"continue\" (default) just prints and returns; any other value re-raises.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        yield\n",
        "    except Exception as e:\n",
        "        if _is_oom(e):\n",
        "            print(f\"\\n⚠️  OOM while {phase}{(' - ' + detail) if detail else ''}.\")\n",
        "            print(\"   Tip: reduce chunk_size/batch_size/upsampling, or downsample input.\")\n",
        "            if isinstance(e, torch.cuda.OutOfMemoryError):\n",
        "                # PyTorch OOM messages are in str(e) directly\n",
        "                msg_line = str(e).splitlines()[0][:200]\n",
        "                print(\"   PyTorch says:\", msg_line)\n",
        "            else:\n",
        "                traceback.print_exc(limit=1, file=sys.stdout)\n",
        "            if on_oom != \"continue\":\n",
        "                raise\n",
        "        else:\n",
        "            # Non-OOM: re-raise so real bugs are visible\n",
        "            raise\n",
        "\n",
        "# ============================================================================\n",
        "# 1. TIFF File Operations\n",
        "# ============================================================================\n",
        "\n",
        "def getPixelSizeTIFFmetadata(TIFFpath, display=False):\n",
        "    \"\"\"Extract pixel size from TIFF metadata\"\"\"\n",
        "    with Image.open(TIFFpath) as img:\n",
        "        meta_dict = {TAGS[key]: img.tag[key] for key in img.tag.keys()}\n",
        "\n",
        "    ResolutionUnit = meta_dict['ResolutionUnit'][0]\n",
        "    width = meta_dict['ImageWidth'][0]\n",
        "    height = meta_dict['ImageLength'][0]\n",
        "    xResolution = meta_dict['XResolution'][0]\n",
        "\n",
        "    if len(xResolution) == 1:\n",
        "        xResolution = xResolution[0]\n",
        "    elif len(xResolution) == 2:\n",
        "        xResolution = xResolution[0] / xResolution[1]\n",
        "    else:\n",
        "        print('Image resolution not defined.')\n",
        "        xResolution = 1\n",
        "\n",
        "    if ResolutionUnit == 2:\n",
        "        pixel_size = 0.025 * 1e9 / xResolution\n",
        "    elif ResolutionUnit == 3:\n",
        "        pixel_size = 0.01 * 1e9 / xResolution\n",
        "    else:\n",
        "        print('Resolution unit not defined. Assuming: um')\n",
        "        pixel_size = 1e3 / xResolution\n",
        "\n",
        "    if display:\n",
        "        print(f'Pixel size from metadata: {pixel_size} nm')\n",
        "        print(f'Image size: {width}x{height}')\n",
        "\n",
        "    return pixel_size, width, height\n",
        "\n",
        "def saveAsTIF(path, filename, array, pixel_size):\n",
        "    \"\"\"Save array as TIFF with metadata\"\"\"\n",
        "    if array.dtype == np.uint16:\n",
        "        mode = 'I;16'\n",
        "    elif array.dtype == np.uint32:\n",
        "        mode = 'I'\n",
        "    else:\n",
        "        mode = 'F'\n",
        "\n",
        "    if len(array.shape) == 2:\n",
        "        im = Image.fromarray(array)\n",
        "        im.save(os.path.join(path, filename + '.tif'),\n",
        "               mode=mode,\n",
        "               resolution_unit=3,\n",
        "               resolution=0.01 * 1e9 / pixel_size)\n",
        "    elif len(array.shape) == 3:\n",
        "        imlist = []\n",
        "        for frame in array:\n",
        "            imlist.append(Image.fromarray(frame))\n",
        "        imlist[0].save(os.path.join(path, filename + '.tif'),\n",
        "                      save_all=True,\n",
        "                      append_images=imlist[1:],\n",
        "                      mode=mode,\n",
        "                      resolution_unit=3,\n",
        "                      resolution=0.01 * 1e9 / pixel_size)\n",
        "\n",
        "def is_tiff(path):\n",
        "    \"\"\"Check if file is TIFF\"\"\"\n",
        "    return path.lower().endswith(('.tif', '.tiff'))\n",
        "\n",
        "def iter_tiff_frames(path):\n",
        "    \"\"\"Iterate over TIFF frames\"\"\"\n",
        "    with tiff.TiffFile(path) as tif:\n",
        "        for page in tif.pages:\n",
        "            yield page.asarray().astype(np.float32)\n",
        "\n",
        "def count_tiff_frames(path):\n",
        "    \"\"\"Count frames in TIFF file\"\"\"\n",
        "    with tiff.TiffFile(path) as tif:\n",
        "        return len(tif.pages)\n",
        "\n",
        "# ============================================================================\n",
        "# 2. ND2 File Operations\n",
        "# ============================================================================\n",
        "\n",
        "def is_nd2(path):\n",
        "    \"\"\"Check if file is ND2\"\"\"\n",
        "    try:\n",
        "        import nd2\n",
        "        return nd2.is_supported_file(path)\n",
        "    except Exception:\n",
        "        return path.lower().endswith(\".nd2\")\n",
        "\n",
        "def count_nd2_frames(path):\n",
        "    \"\"\"Count frames in ND2 file\"\"\"\n",
        "    import nd2\n",
        "    with nd2.ND2File(path) as f:\n",
        "        try:\n",
        "            return len(f.loop_indices)\n",
        "        except Exception:\n",
        "            sz = getattr(f, \"sizes\", {}) or {}\n",
        "            prod = 1\n",
        "            for ax in (\"T\", \"Z\", \"C\", \"V\"):\n",
        "                prod *= int(sz.get(ax, 1))\n",
        "            return prod\n",
        "\n",
        "def _nd2_to_2d(arr, channel=None):\n",
        "    \"\"\"Convert ND2 frame to 2D\"\"\"\n",
        "    a = np.asarray(arr)\n",
        "    if a.ndim == 2:\n",
        "        return a\n",
        "    if a.ndim == 3:\n",
        "        if a.shape[-1] in (1, 3, 4):\n",
        "            idx = channel if (channel is not None and channel < a.shape[-1]) else 0\n",
        "            return a[..., idx]\n",
        "        if a.shape[0] in (1, 3, 4):\n",
        "            idx = channel if (channel is not None and channel < a.shape[0]) else 0\n",
        "            return a[idx, ...]\n",
        "        return a.mean(axis=0)\n",
        "    a = a.squeeze()\n",
        "    return a if a.ndim == 2 else a.reshape(a.shape[-2], a.shape[-1])\n",
        "\n",
        "def iter_nd2_frames(path, channel=None):\n",
        "    \"\"\"Iterate over ND2 frames\"\"\"\n",
        "    import nd2\n",
        "    n = count_nd2_frames(path)\n",
        "    with nd2.ND2File(path) as f:\n",
        "        for i in range(n):\n",
        "            fr = f.read_frame(i)\n",
        "            fr2d = _nd2_to_2d(fr, channel=channel)\n",
        "            yield fr2d.astype(np.float32, copy=False)\n",
        "\n",
        "def getPixelSizeND2metadata(path, display=False):\n",
        "    \"\"\"Extract pixel size from ND2 metadata\"\"\"\n",
        "    import nd2\n",
        "    with nd2.ND2File(path) as f:\n",
        "        vox_um = getattr(f, \"voxel_size\", None)\n",
        "        if vox_um is None:\n",
        "            return None, None, None\n",
        "        px_nm = vox_um[2] * 1e3\n",
        "        try:\n",
        "            h, w = f.shape[-2], f.shape[-1]\n",
        "        except Exception:\n",
        "            h = w = None\n",
        "        if display:\n",
        "            print(f\"Pixel size (ND2): {px_nm:.2f} nm | image ~ {w}x{h}\")\n",
        "        return px_nm, w, h\n",
        "\n",
        "# ============================================================================\n",
        "# 3. Drift Correction Functions\n",
        "# ============================================================================\n",
        "\n",
        "def correctDriftLocalization(xc_array, yc_array, frames, xDrift, yDrift):\n",
        "    \"\"\"Apply drift correction to localizations\"\"\"\n",
        "    n_locs = xc_array.shape[0]\n",
        "    xc_array_Corr = np.empty(n_locs)\n",
        "    yc_array_Corr = np.empty(n_locs)\n",
        "\n",
        "    for loc in range(n_locs):\n",
        "        xc_array_Corr[loc] = xc_array[loc] - xDrift[frames[loc] - 1]\n",
        "        yc_array_Corr[loc] = yc_array[loc] - yDrift[frames[loc] - 1]\n",
        "\n",
        "    return xc_array_Corr, yc_array_Corr\n",
        "\n",
        "def FromLoc2Image_SimpleHistogram(xc_array, yc_array, image_size=(64, 64), pixel_size=100):\n",
        "    \"\"\"Convert localizations to histogram image\"\"\"\n",
        "    w, h = image_size\n",
        "    locImage = np.zeros(image_size)\n",
        "    n_locs = len(xc_array)\n",
        "\n",
        "    for e in range(n_locs):\n",
        "        y_idx = int(max(min(round(yc_array[e] / pixel_size), w - 1), 0))\n",
        "        x_idx = int(max(min(round(xc_array[e] / pixel_size), h - 1), 0))\n",
        "        locImage[y_idx][x_idx] += 1\n",
        "\n",
        "    return locImage\n",
        "\n",
        "def estimate_drift_com_nm(img1, img2, pixel_size_nm, sigma=1.0, patch_radius=3):\n",
        "    \"\"\"Estimate drift using center of mass of cross-correlation\"\"\"\n",
        "    from scipy.ndimage import gaussian_filter\n",
        "    from scipy.signal import fftconvolve\n",
        "\n",
        "    # Smooth images\n",
        "    img1_smooth = gaussian_filter(img1.astype(np.float32), sigma=sigma)\n",
        "    img2_smooth = gaussian_filter(img2.astype(np.float32), sigma=sigma)\n",
        "\n",
        "    # Cross-correlation\n",
        "    corr = fftconvolve(img1_smooth, img2_smooth, mode='same')\n",
        "\n",
        "    # Center of image\n",
        "    center_y, center_x = np.array(corr.shape) // 2\n",
        "\n",
        "    # Crop around center\n",
        "    y_min = max(0, center_y - patch_radius)\n",
        "    y_max = min(corr.shape[0], center_y + patch_radius + 1)\n",
        "    x_min = max(0, center_x - patch_radius)\n",
        "    x_max = min(corr.shape[1], center_x + patch_radius + 1)\n",
        "\n",
        "    patch = corr[y_min:y_max, x_min:x_max]\n",
        "\n",
        "    # Center of mass\n",
        "    y_grid, x_grid = np.meshgrid(\n",
        "        np.arange(y_min, y_max), np.arange(x_min, x_max), indexing='ij'\n",
        "    )\n",
        "\n",
        "    total = np.sum(patch)\n",
        "    if total == 0:\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    y_com = np.sum(patch * y_grid) / total\n",
        "    x_com = np.sum(patch * x_grid) / total\n",
        "\n",
        "    # Drift in pixels\n",
        "    dy_px = y_com - center_y\n",
        "    dx_px = x_com - center_x\n",
        "\n",
        "    if abs(dy_px) > patch_radius or abs(dx_px) > patch_radius:\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    # Convert to nm\n",
        "    dy_nm = dy_px * pixel_size_nm\n",
        "    dx_nm = dx_px * pixel_size_nm\n",
        "\n",
        "    return dy_nm, dx_nm\n",
        "\n",
        "# ============================================================================\n",
        "# 4. Model Download Utilities\n",
        "# ============================================================================\n",
        "\n",
        "def ensure_models(model_names, target_root=\"/content/AutoDS_models\", model_manifest=None):\n",
        "    if model_manifest is None:\n",
        "        raise ValueError(\"model_manifest must be provided.\")\n",
        "\n",
        "    os.makedirs(target_root, exist_ok=True)\n",
        "\n",
        "    for m in model_names:\n",
        "        cfg = model_manifest[m]\n",
        "        mdir = os.path.join(target_root, m)\n",
        "        need_fetch = False\n",
        "\n",
        "        req = cfg.get(\"contains\", [])\n",
        "        if not os.path.isdir(mdir):\n",
        "            need_fetch = True\n",
        "        else:\n",
        "            for f in req:\n",
        "                if not os.path.exists(os.path.join(mdir, f)):\n",
        "                    need_fetch = True\n",
        "                    break\n",
        "\n",
        "        if not need_fetch:\n",
        "            print(f\"[models] found: {m}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"[models] preparing: {m}\")\n",
        "        os.makedirs(mdir, exist_ok=True)\n",
        "\n",
        "        if \"file_urls\" in cfg:\n",
        "            file_urls = cfg[\"file_urls\"]\n",
        "            for fname, url in file_urls.items():\n",
        "                dst = os.path.join(mdir, fname)\n",
        "                print(f\"[models] downloading: {url}\")\n",
        "                urllib.request.urlretrieve(url, dst)\n",
        "        else:\n",
        "            raise ValueError(f\"Model {m} manifest must have 'file_urls'.\")\n",
        "\n",
        "        for f in req:\n",
        "            if not os.path.exists(os.path.join(mdir, f)):\n",
        "                raise FileNotFoundError(f\"Model {m} missing required file: {f}\")\n",
        "\n",
        "        print(f\"[models] ready: {m}\")\n",
        "\n",
        "    return target_root\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import scipy.optimize as opt\n",
        "from numpy.lib.stride_tricks import sliding_window_view\n",
        "from scipy.ndimage import gaussian_filter, zoom\n",
        "from scipy.ndimage import gaussian_laplace, maximum_filter, binary_dilation\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ============================================================================\n",
        "# 1. Image Preprocessing Functions\n",
        "# ============================================================================\n",
        "\n",
        "def normalize_im_01(im):\n",
        "    \"\"\"Normalize image to [0, 1]\"\"\"\n",
        "    im = np.squeeze(im)\n",
        "    min_val = im.min()\n",
        "    max_val = im.max()\n",
        "    return (im - min_val) / (max_val - min_val)\n",
        "\n",
        "def normalize_im_01_ret_vals(im):\n",
        "    \"\"\"Normalize and return normalization parameters\"\"\"\n",
        "    im = np.squeeze(im)\n",
        "    min_val = im.min()\n",
        "    max_val = im.max()\n",
        "    return (im - min_val) / (max_val - min_val), min_val, max_val\n",
        "\n",
        "def normalize_im(im, dmean, dstd):\n",
        "    \"\"\"Normalize image with given mean and std\"\"\"\n",
        "    im = np.squeeze(im)\n",
        "    return (im - dmean) / dstd\n",
        "\n",
        "def subtract_smooth_background(im, sigma=3):\n",
        "    \"\"\"Subtract smoothed background\"\"\"\n",
        "    return im - gaussian_filter(im, sigma)\n",
        "\n",
        "def remove_zero_padding(image):\n",
        "    \"\"\"Remove zero padding from image\"\"\"\n",
        "    image_array = np.array(image)\n",
        "    non_zero_rows = np.where(image_array.sum(axis=1) != 0)\n",
        "    non_zero_cols = np.where(image_array.sum(axis=0) != 0)\n",
        "    cropped_image = image_array[non_zero_rows[0][0]:non_zero_rows[0][-1]+1,\n",
        "                                non_zero_cols[0][0]:non_zero_cols[0][-1]+1]\n",
        "    return cropped_image\n",
        "\n",
        "# ============================================================================\n",
        "# 2. Patch Splitting\n",
        "# ============================================================================\n",
        "\n",
        "def split_image_to_patches(img, num_patches, overlap):\n",
        "    \"\"\"\n",
        "    Split image into overlapping patches\n",
        "\n",
        "    Args:\n",
        "        img: Input image (H, W)\n",
        "        num_patches: Number of patches per dimension\n",
        "        overlap: Overlap size in pixels\n",
        "\n",
        "    Returns:\n",
        "        List of patches\n",
        "    \"\"\"\n",
        "    H, W = img.shape\n",
        "    patch_h = H // num_patches\n",
        "    patch_w = W // num_patches\n",
        "\n",
        "    # Pad image for border patches\n",
        "    padded_img = np.pad(img, ((overlap, overlap), (overlap, overlap)), mode='reflect')\n",
        "\n",
        "    # Window shape including overlap\n",
        "    window_shape = (patch_h + 2 * overlap, patch_w + 2 * overlap)\n",
        "\n",
        "    # Create sliding window view\n",
        "    patches_view = sliding_window_view(padded_img, window_shape)\n",
        "\n",
        "    # Sample at regular intervals\n",
        "    patches_array = patches_view[0::patch_h, 0::patch_w, :, :]\n",
        "\n",
        "    # Flatten to list\n",
        "    num_rows, num_cols, ph, pw = patches_array.shape\n",
        "    patches_list = [patches_array[i, j].copy()\n",
        "                   for i in range(num_rows)\n",
        "                   for j in range(num_cols)]\n",
        "\n",
        "    return patches_list\n",
        "\n",
        "# ============================================================================\n",
        "# 3. Interpolation and Scaling\n",
        "# ============================================================================\n",
        "\n",
        "def gaussian_interpolation_batch(data_batch, scale, sigma=1):\n",
        "    \"\"\"Apply Gaussian interpolation to batch of images\"\"\"\n",
        "    upsampled_data_batch = []\n",
        "\n",
        "    for data in data_batch:\n",
        "        smoothed_data = gaussian_filter(data, sigma=sigma)\n",
        "        upsampled_data = zoom(smoothed_data, scale, order=3)\n",
        "        upsampled_data_batch.append(upsampled_data)\n",
        "\n",
        "    return np.array(upsampled_data_batch)\n",
        "\n",
        "def interpolate_frames(tiff_stack, model_pixel_size, current_pixel_size,\n",
        "                      model_wavelength, current_wavelength,\n",
        "                      model_NA, current_NA):\n",
        "    \"\"\"Interpolate frames to match model parameters\"\"\"\n",
        "    # Set defaults\n",
        "    if model_pixel_size is None:\n",
        "        model_pixel_size = current_pixel_size\n",
        "    if model_wavelength is None:\n",
        "        model_wavelength = current_wavelength\n",
        "    if model_NA is None:\n",
        "        model_NA = current_NA\n",
        "    if current_wavelength is None:\n",
        "        current_wavelength = model_wavelength = 1\n",
        "    if current_NA is None:\n",
        "        current_NA = model_NA = 1\n",
        "\n",
        "    if len(tiff_stack.shape) == 2:\n",
        "        tiff_stack = tiff_stack[None, :, :]\n",
        "\n",
        "    # Compute scaling ratio based on optical parameters\n",
        "    scale_ratio_sq = ((0.21 * model_wavelength / model_NA) ** 2 -\n",
        "                     (0.21 * current_wavelength / current_NA) ** 2)\n",
        "\n",
        "    if scale_ratio_sq > 0:\n",
        "        scale_ratio = np.sqrt(scale_ratio_sq) / model_pixel_size\n",
        "        interpolated_stack = np.stack([\n",
        "            gaussian_filter(tiff_stack[i], scale_ratio)\n",
        "            for i in range(tiff_stack.shape[0])\n",
        "        ])\n",
        "    else:\n",
        "        zoom_factors = (1,\n",
        "                       model_pixel_size / current_pixel_size,\n",
        "                       model_pixel_size / current_pixel_size)\n",
        "        interpolated_stack = zoom(tiff_stack.astype(np.float32),\n",
        "                                 zoom_factors, order=3)\n",
        "\n",
        "    return interpolated_stack.astype(np.float32, copy=False)\n",
        "\n",
        "# ============================================================================\n",
        "# 4. Feature Extraction\n",
        "# ============================================================================\n",
        "\n",
        "def gauss2d(xy, offset, amp, x0, y0, sigma):\n",
        "    \"\"\"2D Gaussian function for fitting\"\"\"\n",
        "    x, y = xy\n",
        "    return offset + (amp * np.exp(-((x - x0) ** 2) / (2 * sigma ** 2) -\n",
        "                                  ((y - y0) ** 2) / (2 * sigma ** 2)))\n",
        "\n",
        "def extract_features_frame(OrigImage, pixel_size, psf_sigma, offset=None, verbose=False):\n",
        "    \"\"\"\n",
        "    Extract features from a single frame\n",
        "\n",
        "    Returns:\n",
        "        ADC_offset: Mean background\n",
        "        ReadOutNoise_ADC: Std of background\n",
        "        Signal_amp: Mean signal amplitude\n",
        "        emitter_density: Density of emitters (per μm²)\n",
        "    \"\"\"\n",
        "    M, N = OrigImage.shape\n",
        "\n",
        "    # Subtract smooth background\n",
        "    Image = OrigImage - gaussian_filter(OrigImage, sigma=5)\n",
        "\n",
        "    # Check if SNR is sufficient\n",
        "    if offset is not None:\n",
        "        if (np.percentile(gaussian_filter(Image, 2), 99) < 2 * Image.mean() or\n",
        "            np.percentile(OrigImage, 99) < 2 * offset):\n",
        "            if verbose:\n",
        "                print(\"SNR too low - ignoring patch\")\n",
        "            return np.mean(OrigImage), np.std(OrigImage), 0, 0\n",
        "\n",
        "    # Laplacian of Gaussian for blob detection\n",
        "    log_image = -gaussian_laplace(Image, sigma=psf_sigma)\n",
        "\n",
        "    # Local maxima filtering\n",
        "    neighborhood_size = 3\n",
        "    local_max = (log_image == maximum_filter(log_image, size=neighborhood_size))\n",
        "\n",
        "    # Intensity threshold\n",
        "    amp_threshold = np.mean(Image) + 0.5 * (np.percentile(Image, 99) - np.mean(Image))\n",
        "    pcntl_threshold = np.percentile(Image, 85)\n",
        "\n",
        "    # Binary mask for emitters\n",
        "    binary_mask = np.logical_and(local_max,\n",
        "                                 Image > np.max([amp_threshold, pcntl_threshold]))\n",
        "\n",
        "    # Dilate and create noise mask\n",
        "    dilated_mask = binary_dilation(binary_mask, structure=np.ones((5, 5)))\n",
        "    noise_mask = np.ones_like(binary_mask)\n",
        "    noise_mask[dilated_mask] = 0\n",
        "\n",
        "    if np.sum(binary_mask) > 0:\n",
        "        ADC_offset = np.mean(OrigImage[noise_mask])\n",
        "        ReadOutNoise_ADC = np.std(OrigImage[noise_mask])\n",
        "        Signal_amp = np.mean(OrigImage[binary_mask == 1])\n",
        "        emitter_density = (10 ** 6) * float(np.sum(binary_mask)) / (M * N * pixel_size ** 2)\n",
        "    else:\n",
        "        if verbose:\n",
        "            print(\"Didn't find any emitters\")\n",
        "        return np.mean(OrigImage), np.std(OrigImage), 0, 0\n",
        "\n",
        "    # Additional SNR check\n",
        "    if Signal_amp / ADC_offset < 2.5:\n",
        "        if emitter_density > 2:\n",
        "            if verbose:\n",
        "                print(\"SNR too low for emitter density estimation\")\n",
        "            return ADC_offset, ReadOutNoise_ADC, Signal_amp, 0\n",
        "\n",
        "    return ADC_offset, ReadOutNoise_ADC, Signal_amp, emitter_density\n",
        "\n",
        "# ============================================================================\n",
        "# 5. Model Selection\n",
        "# ============================================================================\n",
        "\n",
        "def ChooseNetByDifficulty_2025(density, SNR):\n",
        "    \"\"\" Choose network based on density and SNR \"\"\"\n",
        "    num_models = 4\n",
        "    norm_density = np.max([np.min([int(np.round(2 * density)), num_models - 1]), 0])\n",
        "    norm_SNR = num_models - 1 - np.max([np.min([SNR // 2, num_models - 1]), 0])\n",
        "    return int(np.round((norm_SNR + norm_density) / 2))\n",
        "\n",
        "# ============================================================================\n",
        "# Module-level kernel cache (shared across all calls)\n",
        "_kernel_cache = {}\n",
        "\n",
        "def _get_gaussian_kernel(sigma, device):\n",
        "    \"\"\"Generate Gaussian kernel for smoothing\"\"\"\n",
        "    key = f'gauss_{sigma}_{device}'\n",
        "    if key not in _kernel_cache:\n",
        "        kernel_size = int(2 * np.ceil(3 * sigma) + 1)\n",
        "        ax = torch.arange(-kernel_size // 2 + 1., kernel_size // 2 + 1., device=device)\n",
        "        xx, yy = torch.meshgrid(ax, ax, indexing='ij')\n",
        "        kernel = torch.exp(-(xx ** 2 + yy ** 2) / (2 * sigma ** 2))\n",
        "        kernel = kernel / kernel.sum()\n",
        "        _kernel_cache[key] = kernel.view(1, 1, kernel_size, kernel_size)\n",
        "    return _kernel_cache[key]\n",
        "\n",
        "\n",
        "def _get_log_kernel(sigma, device):\n",
        "    \"\"\"Generate Laplacian of Gaussian kernel for blob detection\"\"\"\n",
        "    key = f'log_{sigma}_{device}'\n",
        "    if key not in _kernel_cache:\n",
        "        kernel_size = int(2 * np.ceil(3 * sigma) + 1)\n",
        "        ax = torch.arange(-kernel_size // 2 + 1., kernel_size // 2 + 1., device=device)\n",
        "        xx, yy = torch.meshgrid(ax, ax, indexing='ij')\n",
        "        r2 = xx ** 2 + yy ** 2\n",
        "        kernel = -(1 / (np.pi * sigma ** 4)) * (1 - r2 / (2 * sigma ** 2)) * torch.exp(-r2 / (2 * sigma ** 2))\n",
        "        _kernel_cache[key] = kernel.view(1, 1, kernel_size, kernel_size)\n",
        "    return _kernel_cache[key]\n",
        "\n",
        "\n",
        "def percentile_batch(tensor, percentile):\n",
        "    \"\"\"Calculate percentile for batched tensors\"\"\"\n",
        "    flat = tensor.flatten(1)\n",
        "    result = torch.quantile(flat, percentile / 100.0, dim=1)\n",
        "    return result\n",
        "\n",
        "def extract_features_batch(patches_tensor, pixel_size, psf_sigma, offset_array=None,\n",
        "                           verbose=False, device='cuda'):\n",
        "    \"\"\"Fully GPU-accelerated batch feature extraction\"\"\"\n",
        "    B, H, W = patches_tensor.shape\n",
        "    device = patches_tensor.device\n",
        "\n",
        "    # Add channel dimension for conv operations: [B, 1, H, W]\n",
        "    patches_4d = patches_tensor.unsqueeze(1)\n",
        "\n",
        "    # 1. Gaussian filtering\n",
        "    gauss_kernel = _get_gaussian_kernel(5, device)\n",
        "    padding = gauss_kernel.shape[-1] // 2\n",
        "    smooth_bg = F.conv2d(patches_4d, gauss_kernel, padding=padding)\n",
        "    Image = patches_4d - smooth_bg # [B, 1, H, W]\n",
        "\n",
        "    # 2. LoG filtering\n",
        "    log_kernel = _get_log_kernel(psf_sigma, device)\n",
        "    padding = log_kernel.shape[-1] // 2\n",
        "    log_image = -F.conv2d(Image, log_kernel, padding=padding) # [B, 1, H, W]\n",
        "\n",
        "    # 3. Local maxima\n",
        "    local_max = F.max_pool2d(log_image, kernel_size=3, stride=1, padding=1) # [B, 1, H, W]\n",
        "\n",
        "    # 4. Thresholding (all with size [B, 1, 1, 1])\n",
        "    img_mean = Image.mean(dim=(2, 3), keepdim=True)\n",
        "    img_99 = percentile_batch(Image.squeeze(1), 99).view(B, 1, 1, 1)\n",
        "    img_85 = percentile_batch(Image.squeeze(1), 85).view(B, 1, 1, 1)\n",
        "    threshold = torch.max(img_mean + 0.5 * (img_99 - img_mean), img_85)\n",
        "\n",
        "    # 5. Binary masks (batch-wise)\n",
        "    binary_mask = torch.logical_and(log_image == local_max, Image >= threshold)\n",
        "    mask_float = binary_mask.float()\n",
        "    dilated = F.max_pool2d(mask_float, kernel_size=5, stride=1, padding=2)\n",
        "    noise_mask = (dilated < 0.5)\n",
        "\n",
        "    # 6. PRE-COMPUTE SNR check data on GPU as a batch\n",
        "    gauss_kernel_2 = _get_gaussian_kernel(2, device)\n",
        "    padding_2 = (gauss_kernel_2.shape[-1] // 2)\n",
        "    gauss_smooth = F.conv2d(Image, gauss_kernel_2, padding=padding_2)\n",
        "\n",
        "    # Pre-compute percentiles on GPU (batch-wise)\n",
        "    gauss_99 = percentile_batch(gauss_smooth.squeeze(1), 99) #[B]\n",
        "    patch_99 = percentile_batch(patches_tensor, 99)  # [B]\n",
        "    img_mean_flat = img_mean.squeeze()  # [B]\n",
        "\n",
        "    # 7. Statistics on CPU\n",
        "    patches_cpu = patches_tensor.cpu().numpy()\n",
        "    binary_mask_cpu = binary_mask.squeeze(1).cpu().numpy()\n",
        "    noise_mask_cpu = noise_mask.squeeze(1).cpu().numpy()\n",
        "\n",
        "    # Move pre-computed values to CPU\n",
        "    gauss_99_cpu = gauss_99.cpu().numpy()\n",
        "    patch_99_cpu = patch_99.cpu().numpy()\n",
        "    img_mean_cpu = img_mean_flat.cpu().numpy()\n",
        "\n",
        "    results = []\n",
        "    pixel_area = pixel_size * pixel_size\n",
        "\n",
        "    for i in range(B):\n",
        "        patch = patches_cpu[i]\n",
        "        emitter_mask = binary_mask_cpu[i]\n",
        "        noise_m = noise_mask_cpu[i]\n",
        "        patch_offset = offset_array[i]\n",
        "\n",
        "        if patch_offset is not None:\n",
        "            if (gauss_99_cpu[i] < 2 * img_mean_cpu[i] or\n",
        "                patch_99_cpu[i] < 2 * patch_offset):\n",
        "                if verbose:\n",
        "                    print(f\"Patch {i}: SNR too low - ignoring patch\")\n",
        "                results.append((patch.mean(), patch.std(), 0.0, 0.0))\n",
        "                continue\n",
        "\n",
        "        num_emitters = emitter_mask.sum()\n",
        "        if num_emitters == 0:\n",
        "            if verbose:\n",
        "                print(f\"Patch {i}: Didn't find any emitters\")\n",
        "            results.append((patch.mean(), patch.std(), 0.0, 0.0))\n",
        "            continue\n",
        "\n",
        "        ADC_offset = patch[noise_m].mean()\n",
        "        ReadOutNoise_ADC = patch[noise_m].std()\n",
        "        Signal_amp = patch[emitter_mask].mean()\n",
        "        emitter_density = 1e6 * float(num_emitters) / (H * W * pixel_area)\n",
        "\n",
        "        # Additional SNR check\n",
        "        if Signal_amp / (ADC_offset + 1e-8) < 2.5:\n",
        "            if emitter_density > 2:\n",
        "                if verbose:\n",
        "                    print(f\"Patch {i}: SNR too low for emitter density estimation\")\n",
        "                results.append((float(ADC_offset), float(ReadOutNoise_ADC),\n",
        "                                float(Signal_amp), 0.0))\n",
        "                continue\n",
        "\n",
        "        results.append((float(ADC_offset), float(ReadOutNoise_ADC),\n",
        "                        float(Signal_amp), float(emitter_density)))\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def preprocess_frames_batch(frames_batch, device='cuda'):\n",
        "    \"\"\"GPU-accelerated batch preprocessing of frames\"\"\"\n",
        "    B, H, W = frames_batch.shape\n",
        "\n",
        "    # Calculate 35th percentile for each frame (on GPU)\n",
        "    frames_flat = frames_batch.reshape(B, -1)\n",
        "    p35 = torch.quantile(frames_flat, 0.35, dim=1, keepdim=True)\n",
        "    p35 = p35.view(B, 1, 1)\n",
        "\n",
        "    # Subtract 35th percentile\n",
        "    frames_processed = frames_batch - p35\n",
        "\n",
        "    # Subtract minimum\n",
        "    frames_min = frames_processed.reshape(B, -1).min(dim=1, keepdim=True)[0]\n",
        "    frames_min = frames_min.view(B, 1, 1)\n",
        "    frames_processed = frames_processed - frames_min\n",
        "\n",
        "    # Calculate mean and std for normalization\n",
        "    frames_mean = frames_processed.reshape(B, -1).double().mean(dim=1).float()\n",
        "    frames_std = frames_processed.reshape(B, -1).double().std(dim=1).float() + 1e-6\n",
        "    frames_mean_batch = frames_mean.view(B, 1, 1)\n",
        "    frames_std_batch = frames_std.view(B, 1, 1)\n",
        "\n",
        "    # Normalize\n",
        "    frames_processed = (frames_processed - frames_mean_batch) / frames_std_batch\n",
        "\n",
        "    # Calculate offsets\n",
        "    offsets = frames_processed.reshape(B, -1).mean(dim=1)\n",
        "\n",
        "    return frames_processed, offsets\n",
        "\n",
        "\n",
        "def interpolate_frames_batch(frames_batch, model_pixel_size, current_pixel_size,\n",
        "                                  model_wavelength, current_wavelength,\n",
        "                                  model_NA, current_NA, device='cuda'):\n",
        "    \"\"\"GPU-accelerated batch interpolation for multiple frames\"\"\"\n",
        "    # Handle None values\n",
        "    if model_pixel_size is None: model_pixel_size = current_pixel_size\n",
        "    if model_wavelength is None: model_wavelength = current_wavelength\n",
        "    if model_NA is None: model_NA = current_NA\n",
        "    if current_wavelength is None: current_wavelength = model_wavelength = 1\n",
        "    if current_NA is None: current_NA = model_NA = 1\n",
        "\n",
        "    # Calculate scale ratio\n",
        "    scale_ratio_sq = (0.21 * model_wavelength / model_NA) ** 2 - \\\n",
        "                     (0.21 * current_wavelength / current_NA) ** 2\n",
        "\n",
        "    if scale_ratio_sq > 0:\n",
        "        # Gaussian smoothing path\n",
        "        scale_ratio = np.sqrt(scale_ratio_sq) / model_pixel_size\n",
        "        kernel = _get_gaussian_kernel(scale_ratio, device)\n",
        "\n",
        "        # Apply Gaussian filter to all frames at once\n",
        "        frames_4d = frames_batch.unsqueeze(1)  # (B, 1, H, W)\n",
        "        padding = kernel.shape[-1] // 2\n",
        "        interpolated = F.conv2d(frames_4d, kernel, padding=padding).squeeze(1)\n",
        "    else:\n",
        "        # Zoom/resize path\n",
        "        zoom_factor = model_pixel_size / current_pixel_size\n",
        "\n",
        "        if zoom_factor != 1.0:\n",
        "            # Use bilinear interpolation on GPU\n",
        "            new_h = int(frames_batch.shape[1] * zoom_factor)\n",
        "            new_w = int(frames_batch.shape[2] * zoom_factor)\n",
        "\n",
        "            frames_4d = frames_batch.unsqueeze(1)\n",
        "            interpolated = F.interpolate(frames_4d, size=(new_h, new_w),\n",
        "                                        mode='bicubic', align_corners=False).squeeze(1)\n",
        "        else:\n",
        "            interpolated = frames_batch\n",
        "\n",
        "    return interpolated\n",
        "\n",
        "def split_image_to_patches_batch(img_batch, num_patches, overlap, device='cuda'):\n",
        "    \"\"\" Split tensor of images into overlapping patches \"\"\"\n",
        "    # Handle both 2D and 3D input\n",
        "    if img_batch.dim() == 2:\n",
        "        img_batch = img_batch.unsqueeze(0)  # (H, W) -> (1, H, W)\n",
        "\n",
        "    # Determine the non-overlapping patch size\n",
        "    B, H, W = img_batch.shape\n",
        "    patch_h = H // num_patches\n",
        "    patch_w = W // num_patches\n",
        "\n",
        "    # Pad image for border patches (reflection padding as in the original)\n",
        "    padded = F.pad(img_batch.unsqueeze(1), # (B, 1, H, W)\n",
        "                    (overlap, overlap, overlap, overlap),\n",
        "                    mode='reflect').squeeze(1) # (B, H+2*overlap, W+2*overlap)\n",
        "\n",
        "    # Calculate window shape including overlap\n",
        "    window_h = patch_h + 2 * overlap\n",
        "    window_w = patch_w + 2 * overlap\n",
        "\n",
        "    # create sliding windows along height and then along width with the patch_h and patch_w as the step\n",
        "    patches = padded.unfold(1, window_h, patch_h).unfold(2, window_w, patch_w)\n",
        "    # Shape: (B, num_patches, num_patches, window_h, window_w)\n",
        "\n",
        "    # Reshape to (B, num_patches * num_patches, window_h, window_w)\n",
        "    # Flatten the 2D grid of patches for every frame (row-major order).\n",
        "    B, num_rows, num_cols, ph, pw = patches.shape\n",
        "    patches = patches.reshape(B, num_rows * num_cols, ph, pw)\n",
        "\n",
        "    return patches\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ============================================================================\n",
        "# 1. Basic CNN Model (without upsampling)\n",
        "# ============================================================================\n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, in_channels=1):\n",
        "        super(CNNModel, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.features1 = ConvBNReLU(in_channels, 32, 3)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.features2 = ConvBNReLU(32, 64, 3)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.features3 = ConvBNReLU(64, 128, 3)\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.features4 = ConvBNReLU(128, 512, 3)\n",
        "\n",
        "        # Decoder\n",
        "        self.upsample1 = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.features5 = ConvBNReLU(512, 128, 3)\n",
        "\n",
        "        self.upsample2 = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.features6 = ConvBNReLU(128, 64, 3)\n",
        "\n",
        "        self.upsample3 = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.features7 = ConvBNReLU(64, 32, 3)\n",
        "\n",
        "        # Prediction head\n",
        "        self.prediction = nn.Conv2d(32, 1, 1, stride=1, padding=0, bias=False)\n",
        "        nn.init.orthogonal_(self.prediction.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x = self.features1(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.features2(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = self.features3(x)\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        # Bottleneck\n",
        "        x = self.features4(x)\n",
        "\n",
        "        # Decoder\n",
        "        x = self.upsample1(x)\n",
        "        x = self.features5(x)\n",
        "\n",
        "        x = self.upsample2(x)\n",
        "        x = self.features6(x)\n",
        "\n",
        "        x = self.upsample3(x)\n",
        "        x = self.features7(x)\n",
        "\n",
        "        # Prediction\n",
        "        x = self.prediction(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 2. CNN Building Blocks - optimized with fused Conv+BN+ReL operations\n",
        "# ============================================================================\n",
        "\n",
        "class ConvBNReLU(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=None):\n",
        "        super(ConvBNReLU, self).__init__()\n",
        "\n",
        "        if padding is None:\n",
        "            padding = kernel_size // 2\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size,\n",
        "                              stride=stride, padding=padding, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Initialize with Orthogonal (similar to Keras)\n",
        "        nn.init.orthogonal_(self.conv.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 3. CNN Model with Upsampling - optimized with fused Conv+BN+ReL\n",
        "# ============================================================================\n",
        "\n",
        "class CNNUpsample(nn.Module):\n",
        "    def __init__(self, in_channels=1, upsampling_factor=8):\n",
        "        super(CNNUpsample, self).__init__()\n",
        "        self.upsampling_factor = upsampling_factor\n",
        "\n",
        "        # Encoder with fused blocks\n",
        "        self.conv_bn_relu1 = ConvBNReLU(in_channels, 32, 3, 1)\n",
        "        self.conv_bn_relu2 = ConvBNReLU(32, 64, 3, 1)\n",
        "        self.conv_bn_relu3 = ConvBNReLU(64, 128, 3, 1)\n",
        "        self.conv_bn_relu4 = ConvBNReLU(128, 256, 3, 1)\n",
        "\n",
        "        # Decoder with fused blocks\n",
        "        self.conv_bn_relu5 = ConvBNReLU(256, 128, 3, 1)\n",
        "        self.conv_bn_relu6 = ConvBNReLU(128, 64, 3, 1)\n",
        "\n",
        "        # OPTIMIZED: Upsampling blocks with 3x3 kernels + fused Conv+BN+ReLU\n",
        "        num_upsample_blocks = int(np.log2(upsampling_factor))\n",
        "        self.upsample_blocks = nn.ModuleList()\n",
        "\n",
        "        for i in range(num_upsample_blocks):\n",
        "            in_ch = 64 if i == 0 else 32\n",
        "            block = nn.ModuleDict({\n",
        "                'upsample': nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "                'conv_bn_relu': ConvBNReLU(in_ch, 32, 5, 1)\n",
        "            })\n",
        "            self.upsample_blocks.append(block)\n",
        "\n",
        "        # Prediction head\n",
        "        self.prediction = nn.Conv2d(32, 1, 1, stride=1, padding=0, bias=False)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.orthogonal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x = self.conv_bn_relu1(x)\n",
        "        x = self.conv_bn_relu2(x)\n",
        "        x = self.conv_bn_relu3(x)\n",
        "        x = self.conv_bn_relu4(x)\n",
        "\n",
        "        # Decoder\n",
        "        x = self.conv_bn_relu5(x)\n",
        "        x = self.conv_bn_relu6(x)\n",
        "\n",
        "        # Upsampling\n",
        "        for block in self.upsample_blocks:\n",
        "            x = block['upsample'](x)\n",
        "            x = block['conv_bn_relu'](x)\n",
        "\n",
        "        # Prediction\n",
        "        x = self.prediction(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 1. Gaussian Filter for Loss Computation\n",
        "# ============================================================================\n",
        "\n",
        "def matlab_style_gauss2D(shape=(7, 7), sigma=1):\n",
        "    \"\"\"Create 2D Gaussian kernel matching MATLAB style\"\"\"\n",
        "    m, n = [(ss - 1.) / 2. for ss in shape]\n",
        "    y, x = np.ogrid[-m:m+1, -n:n+1]\n",
        "    h = np.exp(-(x*x + y*y) / (2. * sigma * sigma))\n",
        "    h[h < np.finfo(h.dtype).eps * h.max()] = 0\n",
        "    sumh = h.sum()\n",
        "    if sumh != 0:\n",
        "        h /= sumh\n",
        "    h = h * 2.0\n",
        "    return h.astype(np.float32)\n",
        "\n",
        "# Create Gaussian filter as a tensor\n",
        "psf_heatmap = matlab_style_gauss2D(shape=(7, 7), sigma=1)\n",
        "# Shape: [out_channels, in_channels, height, width] -> [1, 1, 7, 7]\n",
        "gfilter = torch.from_numpy(psf_heatmap).view(1, 1, 7, 7)\n",
        "\n",
        "# ============================================================================\n",
        "# 2. Custom Loss Functions\n",
        "# ============================================================================\n",
        "\n",
        "class L1L2Loss(nn.Module):\n",
        "    \"\"\"Combined L1 + L2 loss with Gaussian filtering\"\"\"\n",
        "    def __init__(self, input_shape):\n",
        "        super(L1L2Loss, self).__init__()\n",
        "        self.input_shape = input_shape\n",
        "        # Register Gaussian filter as buffer (moves with model to GPU)\n",
        "        self.register_buffer('gfilter', gfilter)\n",
        "\n",
        "    def forward(self, spikes_pred, heatmap_true):\n",
        "        # Apply Gaussian convolution to predictions\n",
        "        heatmap_pred = F.conv2d(spikes_pred, self.gfilter, padding=3)\n",
        "\n",
        "        # MSE loss on heatmaps\n",
        "        loss_heatmaps = F.mse_loss(heatmap_pred, heatmap_true)\n",
        "\n",
        "        # L1 loss on spikes (sparsity)\n",
        "        loss_spikes = torch.mean(torch.abs(spikes_pred))\n",
        "\n",
        "        return loss_heatmaps + loss_spikes\n",
        "\n",
        "class CustomLoss(nn.Module):\n",
        "    \"\"\"Custom loss for upsampling model\"\"\"\n",
        "    def __init__(self, input_shape):\n",
        "        super(CustomLoss, self).__init__()\n",
        "        self.input_shape = input_shape\n",
        "        self.register_buffer('gfilter', gfilter)\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        # Apply Gaussian convolution\n",
        "        heatmap_pred = F.conv2d(y_pred, self.gfilter, padding=3)\n",
        "\n",
        "        # MSE on heatmaps\n",
        "        loss_heatmaps = torch.mean((y_true - heatmap_pred) ** 2)\n",
        "\n",
        "        # L1 on predictions (sparsity)\n",
        "        loss_spikes = torch.mean(torch.abs(y_pred))\n",
        "\n",
        "        return loss_heatmaps + loss_spikes\n",
        "\n",
        "# ============================================================================\n",
        "# 3. Maxima Finder Layer (Peak Detection)\n",
        "# ============================================================================\n",
        "\n",
        "class MaximaFinder(nn.Module):\n",
        "    \"\"\"Find local maxima in predicted density maps\"\"\"\n",
        "    def __init__(self, thresh=0.1, neighborhood_size=3, use_local_avg=False):\n",
        "        super(MaximaFinder, self).__init__()\n",
        "        self.thresh = thresh\n",
        "        self.nhood = neighborhood_size\n",
        "        self.use_local_avg = use_local_avg\n",
        "\n",
        "        if use_local_avg:\n",
        "            # Sobel-like kernels for local averaging\n",
        "            kernel_x = torch.tensor([[[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]],\n",
        "                                    dtype=torch.float32).view(1, 1, 3, 3)\n",
        "            kernel_y = torch.tensor([[[-1, -1, -1], [0, 0, 0], [1, 1, 1]]],\n",
        "                                    dtype=torch.float32).view(1, 1, 3, 3)\n",
        "            kernel_sum = torch.ones(1, 1, 3, 3, dtype=torch.float32)\n",
        "\n",
        "            self.register_buffer('kernel_x', kernel_x)\n",
        "            self.register_buffer('kernel_y', kernel_y)\n",
        "            self.register_buffer('kernel_sum', kernel_sum)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Max pooling to find local maxima\n",
        "        max_pool = F.max_pool2d(inputs, kernel_size=self.nhood,\n",
        "                               stride=1, padding=self.nhood//2)\n",
        "\n",
        "        # Condition: value is local max AND above threshold\n",
        "        cond = (max_pool > self.thresh) & (max_pool == inputs)\n",
        "\n",
        "        # Get indices where condition is True\n",
        "        indices = torch.nonzero(cond, as_tuple=False)  # (N, 4): [batch, channel, y, x]\n",
        "\n",
        "        bind = indices[:, 0]  # batch indices\n",
        "        yind = indices[:, 2]  # y coordinates\n",
        "        xind = indices[:, 3]  # x coordinates\n",
        "\n",
        "        # Gather confidence values\n",
        "        confidence = inputs[bind, indices[:, 1], yind, xind]\n",
        "\n",
        "        # Convert to float for potential subpixel refinement\n",
        "        xind = xind.float()\n",
        "        yind = yind.float()\n",
        "\n",
        "        # Subpixel refinement using local averaging\n",
        "        if self.use_local_avg:\n",
        "            # Ensure kernels match input dtype\n",
        "            kernel_x = self.kernel_x.to(inputs.dtype)\n",
        "            kernel_y = self.kernel_y.to(inputs.dtype)\n",
        "            kernel_sum = self.kernel_sum.to(dtype=inputs.dtype)\n",
        "\n",
        "            # Compute gradients\n",
        "            # Sobel-like kernels for local averaging\n",
        "            x_image = F.conv2d(inputs, kernel_x, padding=1)\n",
        "            y_image = F.conv2d(inputs, kernel_y, padding=1)\n",
        "            sum_image = F.conv2d(inputs, kernel_sum, padding=1)\n",
        "\n",
        "            # Gather at detected locations\n",
        "            gathered_sum = sum_image[bind, indices[:, 1], yind.long(), xind.long()]\n",
        "            gathered_x = x_image[bind, indices[:, 1], yind.long(), xind.long()]\n",
        "            gathered_y = y_image[bind, indices[:, 1], yind.long(), xind.long()]\n",
        "\n",
        "            # Compute local offsets\n",
        "            x_local = gathered_x / (gathered_sum + 1e-6)\n",
        "            y_local = gathered_y / (gathered_sum + 1e-6)\n",
        "\n",
        "            # Update positions and confidence\n",
        "            xind = xind + x_local\n",
        "            yind = yind + y_local\n",
        "            confidence = gathered_sum\n",
        "\n",
        "        return bind, xind, yind, confidence\n",
        "\n",
        "# ============================================================================\n",
        "# 6. Maxima Finder Layer (Peak Detection)\n",
        "# ============================================================================\n",
        "\n",
        "class MaximaFinder(nn.Module):\n",
        "    \"\"\"Find local maxima in predicted density maps\"\"\"\n",
        "    def __init__(self, thresh=0.1, neighborhood_size=3, use_local_avg=False):\n",
        "        super(MaximaFinder, self).__init__()\n",
        "        self.thresh = thresh\n",
        "        self.nhood = neighborhood_size\n",
        "        self.use_local_avg = use_local_avg\n",
        "\n",
        "        if use_local_avg:\n",
        "            # Sobel-like kernels for local averaging\n",
        "            kernel_x = torch.tensor([[[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]],\n",
        "                                    dtype=torch.float32).view(1, 1, 3, 3)\n",
        "            kernel_y = torch.tensor([[[-1, -1, -1], [0, 0, 0], [1, 1, 1]]],\n",
        "                                    dtype=torch.float32).view(1, 1, 3, 3)\n",
        "            kernel_sum = torch.ones(1, 1, 3, 3, dtype=torch.float32)\n",
        "\n",
        "            self.register_buffer('kernel_x', kernel_x)\n",
        "            self.register_buffer('kernel_y', kernel_y)\n",
        "            self.register_buffer('kernel_sum', kernel_sum)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Max pooling to find local maxima\n",
        "        max_pool = F.max_pool2d(inputs, kernel_size=self.nhood,\n",
        "                               stride=1, padding=self.nhood//2)\n",
        "\n",
        "        # Condition: value is local max AND above threshold\n",
        "        cond = (max_pool > self.thresh) & (max_pool == inputs)\n",
        "\n",
        "        # Get indices where condition is True\n",
        "        indices = torch.nonzero(cond, as_tuple=False)  # (N, 4): [batch, channel, y, x]\n",
        "\n",
        "        bind = indices[:, 0]  # batch indices\n",
        "        yind = indices[:, 2]  # y coordinates\n",
        "        xind = indices[:, 3]  # x coordinates\n",
        "\n",
        "        # Gather confidence values\n",
        "        confidence = inputs[bind, indices[:, 1], yind, xind]\n",
        "\n",
        "        # Convert to float for potential subpixel refinement\n",
        "        xind = xind.float()\n",
        "        yind = yind.float()\n",
        "\n",
        "        # Subpixel refinement using local averaging\n",
        "        if self.use_local_avg:\n",
        "            # Ensure kernels match input dtype\n",
        "            kernel_x = self.kernel_x.to(inputs.dtype)\n",
        "            kernel_y = self.kernel_y.to(inputs.dtype)\n",
        "            kernel_sum = self.kernel_sum.to(dtype=inputs.dtype)\n",
        "\n",
        "            # Compute gradients\n",
        "            x_image = F.conv2d(inputs, kernel_x, padding=1)\n",
        "            y_image = F.conv2d(inputs, kernel_y, padding=1)\n",
        "            sum_image = F.conv2d(inputs, kernel_sum, padding=1)\n",
        "\n",
        "            # Gather at detected locations\n",
        "            gathered_sum = sum_image[bind, indices[:, 1], yind.long(), xind.long()]\n",
        "            gathered_x = x_image[bind, indices[:, 1], yind.long(), xind.long()]\n",
        "            gathered_y = y_image[bind, indices[:, 1], yind.long(), xind.long()]\n",
        "\n",
        "            # Compute local offsets\n",
        "            x_local = gathered_x / (gathered_sum + 1e-6)\n",
        "            y_local = gathered_y / (gathered_sum + 1e-6)\n",
        "\n",
        "            # Update positions and confidence\n",
        "            xind = xind + x_local\n",
        "            yind = yind + y_local\n",
        "            confidence = gathered_sum\n",
        "\n",
        "        return bind, xind, yind, confidence\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import h5py\n",
        "import re\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 1. Model Builder Function\n",
        "# ============================================================================\n",
        "\n",
        "def build_model_upsample(input_shape, lr=0.001, upsampling_factor=8):\n",
        "    \"\"\"\n",
        "    Build upsampling model for PyTorch\n",
        "\n",
        "    Args:\n",
        "        input_shape: Tuple (H, W, C) - note: will be converted to (C, H, W)\n",
        "        lr: Learning rate\n",
        "        upsampling_factor: Upsampling factor\n",
        "\n",
        "    Returns:\n",
        "        model: PyTorch model\n",
        "        optimizer: Adam optimizer\n",
        "        criterion: Loss function\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert from (H, W, C) to (C, H, W)\n",
        "    in_channels = input_shape[2] if len(input_shape) == 3 else 1\n",
        "\n",
        "    model = CNNUpsample(in_channels=in_channels,\n",
        "                        upsampling_factor=upsampling_factor)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = CustomLoss(input_shape)\n",
        "\n",
        "    return model, optimizer, criterion\n",
        "\n",
        "# ============================================================================\n",
        "# 2. Weight Loading - Support both PyTorch and Keras formats\n",
        "# ============================================================================\n",
        "\n",
        "def load_model_weights(model, weights_path, verbose=True):\n",
        "    \"\"\"\n",
        "    Load model weights from either PyTorch (.pth) or Keras (.h5) format\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "        weights_path: Path to weights file (.pth or .h5)\n",
        "        verbose: Print loading progress\n",
        "    \"\"\"\n",
        "    if weights_path.endswith('.pth'):\n",
        "        load_pytorch_weights(model, weights_path, verbose=verbose)\n",
        "    elif weights_path.endswith('.h5'):\n",
        "        load_keras_weights_to_pytorch(model, weights_path, verbose=verbose)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported weights format: {weights_path}. \"\n",
        "                        f\"Expected .pth or .h5 file\")\n",
        "\n",
        "\n",
        "def load_pytorch_weights(model, pth_path, verbose=True):\n",
        "    \"\"\"\n",
        "    Load PyTorch native weights from .pth file\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "        pth_path: Path to .pth weights file\n",
        "        verbose: Print loading progress\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(f\"Loading PyTorch weights from {pth_path}\")\n",
        "\n",
        "    # Get device from model\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Load state dict\n",
        "    state_dict = torch.load(pth_path, map_location=device)\n",
        "\n",
        "    # Load weights into model\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"✓ PyTorch weights loaded successfully!\")\n",
        "\n",
        "\n",
        "def load_keras_weights_to_pytorch(model, h5_path, verbose=True):\n",
        "    \"\"\"\n",
        "    Load Keras weights from H5 file to PyTorch model\n",
        "\n",
        "    Supports fused Conv+BN+ReLU blocks while maintaining compatibility.\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model (CNNUpsample with fused blocks)\n",
        "        h5_path: Path to Keras H5 weights file\n",
        "        verbose: Print loading progress\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(f\"Loading Keras weights from {h5_path}\")\n",
        "\n",
        "    # Get device from model\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    with h5py.File(h5_path, 'r') as f:\n",
        "        # Get all layer names from the H5 file\n",
        "        if 'model_weights' in f:\n",
        "            weight_group = f['model_weights']\n",
        "        else:\n",
        "            weight_group = f\n",
        "\n",
        "        # Extract layer names\n",
        "        if hasattr(weight_group, 'attrs') and 'layer_names' in weight_group.attrs:\n",
        "            layer_names = [n.decode('utf8') if isinstance(n, bytes) else n\n",
        "                           for n in weight_group.attrs['layer_names']]\n",
        "        else:\n",
        "            layer_names = list(weight_group.keys())\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Found {len(layer_names)} layers in H5 file\")\n",
        "\n",
        "        # Create a dictionary to store weights\n",
        "        keras_weights = {}\n",
        "\n",
        "        for layer_name in layer_names:\n",
        "            if layer_name not in weight_group:\n",
        "                continue\n",
        "\n",
        "            layer_group = weight_group[layer_name]\n",
        "\n",
        "            if not hasattr(layer_group, 'keys'):\n",
        "                continue\n",
        "\n",
        "            # Get weight names for this layer\n",
        "            if hasattr(layer_group, 'attrs') and 'weight_names' in layer_group.attrs:\n",
        "                weight_names = [n.decode('utf8') if isinstance(n, bytes) else n\n",
        "                                for n in layer_group.attrs['weight_names']]\n",
        "            else:\n",
        "                weight_names = list(layer_group.keys())\n",
        "\n",
        "            # Extract weights\n",
        "            layer_weights = {}\n",
        "            for weight_name in weight_names:\n",
        "                if '/' in weight_name:\n",
        "                    weight_key = weight_name.split('/')[-1]\n",
        "                else:\n",
        "                    weight_key = weight_name\n",
        "\n",
        "                try:\n",
        "                    weight_value = layer_group[weight_name][()]\n",
        "                    layer_weights[weight_key] = weight_value\n",
        "                except:\n",
        "                    try:\n",
        "                        weight_value = layer_group[weight_key][()]\n",
        "                        layer_weights[weight_key] = weight_value\n",
        "                    except:\n",
        "                        if verbose:\n",
        "                            print(f\"  Warning: Could not load {weight_name} from {layer_name}\")\n",
        "\n",
        "            if layer_weights:\n",
        "                keras_weights[layer_name] = layer_weights\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Extracted weights from {len(keras_weights)} layers\")\n",
        "\n",
        "        # Assign to PyTorch model with fused blocks\n",
        "        _assign_weights_to_model(model, keras_weights, device, verbose=verbose)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"✓ Keras weights loaded successfully!\")\n",
        "\n",
        "\n",
        "def _assign_weights_to_model(model, keras_weights, device, verbose=True):\n",
        "    \"\"\"Helper function to assign Keras weights to PyTorch model with fused blocks\"\"\"\n",
        "\n",
        "    # Mapping from Keras layer names to PyTorch fused block names\n",
        "    name_mapping = {\n",
        "        'F1': 'conv_bn_relu1',\n",
        "        'BN_1': 'conv_bn_relu1',\n",
        "        'F2': 'conv_bn_relu2',\n",
        "        'BN_2': 'conv_bn_relu2',\n",
        "        'F3': 'conv_bn_relu3',\n",
        "        'BN_3': 'conv_bn_relu3',\n",
        "        'F4': 'conv_bn_relu4',\n",
        "        'BN_4': 'conv_bn_relu4',\n",
        "        'F5': 'conv_bn_relu5',\n",
        "        'BN_5': 'conv_bn_relu5',\n",
        "        'F6': 'conv_bn_relu6',\n",
        "        'BN_6': 'conv_bn_relu6',\n",
        "        'Prediction': 'prediction',\n",
        "    }\n",
        "\n",
        "    model_dict = dict(model.named_modules())\n",
        "    loaded_count = 0\n",
        "\n",
        "    # Load encoder and decoder layers (now fused blocks)\n",
        "    for keras_name, pytorch_name in name_mapping.items():\n",
        "        if keras_name not in keras_weights:\n",
        "            continue\n",
        "\n",
        "        if pytorch_name not in model_dict:\n",
        "            continue\n",
        "\n",
        "        module = model_dict[pytorch_name]\n",
        "        weights = keras_weights[keras_name]\n",
        "\n",
        "        # Check if this is a fused ConvBNReLU block\n",
        "        if hasattr(module, 'conv') and hasattr(module, 'bn'):\n",
        "            # This is a fused block - load into conv and bn sub-modules\n",
        "\n",
        "            # Load Conv2d weights\n",
        "            if 'kernel:0' in weights:\n",
        "                kernel = weights['kernel:0']\n",
        "                kernel_torch = np.transpose(kernel, (3, 2, 0, 1))\n",
        "                module.conv.weight.data = torch.from_numpy(kernel_torch).float().to(device)\n",
        "                loaded_count += 1\n",
        "                if verbose:\n",
        "                    print(f\"  ✓ Loaded {keras_name} -> {pytorch_name}.conv (Conv2d)\")\n",
        "\n",
        "            # Load BatchNorm weights\n",
        "            if 'gamma:0' in weights:\n",
        "                module.bn.weight.data = torch.from_numpy(weights['gamma:0']).float().to(device)\n",
        "            if 'beta:0' in weights:\n",
        "                module.bn.bias.data = torch.from_numpy(weights['beta:0']).float().to(device)\n",
        "            if 'moving_mean:0' in weights:\n",
        "                module.bn.running_mean.data = torch.from_numpy(weights['moving_mean:0']).float().to(device)\n",
        "            if 'moving_variance:0' in weights:\n",
        "                module.bn.running_var.data = torch.from_numpy(weights['moving_variance:0']).float().to(device)\n",
        "\n",
        "            if any(k in weights for k in ['gamma:0', 'beta:0']):\n",
        "                if verbose:\n",
        "                    print(f\"  ✓ Loaded {keras_name} -> {pytorch_name}.bn (BatchNorm)\")\n",
        "\n",
        "        # Load prediction layer (not fused)\n",
        "        elif isinstance(module, nn.Conv2d):\n",
        "            if 'kernel:0' in weights:\n",
        "                kernel = weights['kernel:0']\n",
        "                kernel_torch = np.transpose(kernel, (3, 2, 0, 1))\n",
        "                module.weight.data = torch.from_numpy(kernel_torch).float().to(device)\n",
        "                loaded_count += 1\n",
        "                if verbose:\n",
        "                    print(f\"  ✓ Loaded {keras_name} -> {pytorch_name} (Conv2d)\")\n",
        "\n",
        "            if 'bias:0' in weights and module.bias is not None:\n",
        "                bias = weights['bias:0']\n",
        "                module.bias.data = torch.from_numpy(bias).float().to(device)\n",
        "\n",
        "    # Load upsampling blocks (now with fused conv_bn_relu)\n",
        "    for keras_name in keras_weights.keys():\n",
        "        if 'conv_upsample' in keras_name or 'BN_upsample' in keras_name:\n",
        "            match = re.search(r'(\\d+)', keras_name)\n",
        "            if match:\n",
        "                idx = int(match.group(1)) - 1\n",
        "\n",
        "                if idx >= len(model.upsample_blocks):\n",
        "                    continue\n",
        "\n",
        "                weights = keras_weights[keras_name]\n",
        "\n",
        "                if 'conv_upsample' in keras_name:\n",
        "                    # Access the fused block's conv layer\n",
        "                    fused_block = model.upsample_blocks[idx]['conv_bn_relu']\n",
        "\n",
        "                    if 'kernel:0' in weights and hasattr(fused_block, 'conv'):\n",
        "                        kernel = weights['kernel:0']\n",
        "                        kernel_torch = np.transpose(kernel, (3, 2, 0, 1))\n",
        "                        fused_block.conv.weight.data = torch.from_numpy(kernel_torch).float().to(device)\n",
        "                        loaded_count += 1\n",
        "                        if verbose:\n",
        "                            print(f\"  ✓ Loaded {keras_name} -> upsample_blocks[{idx}]['conv_bn_relu'].conv\")\n",
        "\n",
        "                elif 'BN_upsample' in keras_name:\n",
        "                    # Access the fused block's bn layer\n",
        "                    fused_block = model.upsample_blocks[idx]['conv_bn_relu']\n",
        "\n",
        "                    if hasattr(fused_block, 'bn'):\n",
        "                        if 'gamma:0' in weights:\n",
        "                            fused_block.bn.weight.data = torch.from_numpy(weights['gamma:0']).float().to(device)\n",
        "                        if 'beta:0' in weights:\n",
        "                            fused_block.bn.bias.data = torch.from_numpy(weights['beta:0']).float().to(device)\n",
        "                        if 'moving_mean:0' in weights:\n",
        "                            fused_block.bn.running_mean.data = torch.from_numpy(weights['moving_mean:0']).float().to(\n",
        "                                device)\n",
        "                        if 'moving_variance:0' in weights:\n",
        "                            fused_block.bn.running_var.data = torch.from_numpy(weights['moving_variance:0']).float().to(\n",
        "                                device)\n",
        "                        loaded_count += 1\n",
        "                        if verbose:\n",
        "                            print(f\"  ✓ Loaded {keras_name} -> upsample_blocks[{idx}]['conv_bn_relu'].bn\")\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n✓ Successfully loaded {loaded_count} layer weights\")\n",
        "\n",
        "# ============================================================================\n",
        "# 3. Main Reconstruction Function with Global Profiling\n",
        "# ============================================================================\n",
        "\n",
        "def reconstruct_patches_2025_pytorch(\n",
        "        patches, patch_indices, frame_numbers,\n",
        "        model_num,\n",
        "        num_patches, overlap,\n",
        "        number_of_frames, threshold, neighborhood_size=3,\n",
        "        use_local_avg=True, upsampling_factor=8,\n",
        "        pixel_size=233, batch_size=32, L2_weighting_factor=100,\n",
        "        profiler=None):\n",
        "\n",
        "    profiler.start_timer(\"reconstruction.total\")\n",
        "\n",
        "    pixel_size_hr = pixel_size / upsampling_factor\n",
        "\n",
        "    # Convert patches to float32\n",
        "    device = get_device()\n",
        "    #patches = torch.stack(patches).float().to(device)\n",
        "    patches = patches.float().to(device)\n",
        "\n",
        "    if patches.ndim == 2:\n",
        "        patches = patches.unsqueeze(0)  # Ensure 3D shape\n",
        "    K_frames, M, N = patches.shape\n",
        "\n",
        "    # Determine dimensions of each predicted (cropped) patch\n",
        "    upsampled_patch_h = M * upsampling_factor - 2 * overlap\n",
        "    upsampled_patch_w = N * upsampling_factor - 2 * overlap\n",
        "\n",
        "    # Create full image tensor on GPU\n",
        "    reconstructed_image = torch.zeros((upsampled_patch_h * num_patches, upsampled_patch_w * num_patches), dtype=torch.float32, device=device)\n",
        "\n",
        "    # Prepare lists for detections\n",
        "    recon_xind, recon_yind, frame_index, confidence_list = [], [], [], []\n",
        "\n",
        "    with torch.cuda.device(0):\n",
        "        # Get model from cache\n",
        "        model = get_model(model_num)\n",
        "        model.eval()\n",
        "\n",
        "        # Create the post-processing layer\n",
        "        profiler.start_timer(\"reconstruction.maxima_finder_init\")\n",
        "        max_layer = MaximaFinder(threshold, neighborhood_size, use_local_avg).to(device)\n",
        "        profiler.stop_timer(\"reconstruction.maxima_finder_init\")\n",
        "\n",
        "        # Process in batches\n",
        "        n_batches = int(np.ceil(K_frames / batch_size))\n",
        "\n",
        "        for batch_idx in range(n_batches):\n",
        "            start_idx = batch_idx * batch_size\n",
        "            end_idx = min(K_frames, start_idx + batch_size)\n",
        "            nF = end_idx - start_idx\n",
        "\n",
        "            # --- Move input batch to GPU ---\n",
        "            batch_imgs = patches[start_idx:end_idx].to(device)  # Shape: (nF, M, N)\n",
        "\n",
        "            # add channel dim to match conv2D\n",
        "            batch_imgs = batch_imgs.unsqueeze(1) # Shape: (nF, 1, M, N)\n",
        "\n",
        "            # --- Run prediction on GPU ---\n",
        "            profiler.start_timer(\"reconstruction.model_forward_pass\")\n",
        "            # Enables Automatic Mixed Precision (AMP) for CUDA (use float16 instead of float32)\n",
        "            with torch.no_grad():\n",
        "                with torch.amp.autocast('cuda'):\n",
        "                    predicted_density = model(batch_imgs)\n",
        "            profiler.stop_timer(\"reconstruction.model_forward_pass\")\n",
        "\n",
        "            # Post-processing\n",
        "            predicted_density = torch.relu(predicted_density - 0.5)  # Faster than `predicted_density[predicted_density < 0] = 0`\n",
        "\n",
        "            # Crop off extra overlap\n",
        "            cropped_pred = predicted_density[:, 0, overlap:-overlap, overlap:-overlap]\n",
        "\n",
        "            # --- Post-processing on GPU ---\n",
        "            # Maxima detection\n",
        "            profiler.start_timer(\"reconstruction.maxima_detection\")\n",
        "            bind, xind, yind, conf = max_layer(predicted_density[:, :, overlap:-overlap, overlap:-overlap])\n",
        "\n",
        "            # Convert tensors to NumPy (only when needed)\n",
        "            bind_np = bind.cpu().numpy()\n",
        "            xind_np = xind.cpu().numpy()\n",
        "            yind_np = yind.cpu().numpy()\n",
        "            conf_np = conf.cpu().numpy() / L2_weighting_factor\n",
        "            profiler.stop_timer(\"reconstruction.maxima_detection\")\n",
        "\n",
        "            profiler.start_timer(\"reconstruction.reconstruct_image\")\n",
        "            # --- Place each patch in reconstructed image ---\n",
        "            for i in range(nF):\n",
        "                p_ind = patch_indices[start_idx + i]\n",
        "                y1 = upsampled_patch_h * (p_ind // num_patches)\n",
        "                x1 = upsampled_patch_w * (p_ind % num_patches)\n",
        "\n",
        "                # Use PyTorch addition instead of NumPy\n",
        "                reconstructed_image[y1:y1 + upsampled_patch_h,\n",
        "                    x1:x1 + upsampled_patch_w].add_(cropped_pred[i] / number_of_frames)\n",
        "\n",
        "                # Collect detections (CPU operations)\n",
        "                det_idx = np.where(bind_np == i)[0]\n",
        "                if det_idx.size:\n",
        "                    recon_xind.extend((x1 + xind_np[det_idx]).tolist())\n",
        "                    recon_yind.extend((y1 + yind_np[det_idx]).tolist())\n",
        "                    frame_index.extend([frame_numbers[start_idx + i] + 1] * det_idx.size)\n",
        "                    confidence_list.extend(conf_np[det_idx].tolist())\n",
        "\n",
        "            profiler.stop_timer(\"reconstruction.reconstruct_image\")\n",
        "\n",
        "    # Convert coordinates to physical units\n",
        "    xind_final = (np.array(recon_xind) * pixel_size_hr).tolist()\n",
        "    yind_final = (np.array(recon_yind) * pixel_size_hr).tolist()\n",
        "\n",
        "\n",
        "    profiler.stop_timer(\"reconstruction.total\")\n",
        "\n",
        "    return reconstructed_image, [frame_index, xind_final, yind_final, confidence_list]\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 4. Weight Validation Function\n",
        "# ============================================================================\n",
        "\n",
        "def validate_model_weights(model, verbose=True):\n",
        "    \"\"\"\n",
        "    Validate that model weights are loaded correctly\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model with loaded weights\n",
        "        verbose: Print validation details\n",
        "\n",
        "    Returns:\n",
        "        bool: True if weights appear valid\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"VALIDATING MODEL WEIGHTS\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "    issues = []\n",
        "\n",
        "    # Check encoder/decoder fused blocks\n",
        "    for i in range(1, 7):\n",
        "        block_name = f'conv_bn_relu{i}'\n",
        "        if hasattr(model, block_name):\n",
        "            block = getattr(model, block_name)\n",
        "\n",
        "            # Check conv weights\n",
        "            conv_weights = block.conv.weight.data\n",
        "            if torch.all(conv_weights == 0):\n",
        "                issues.append(f\"{block_name}.conv weights are all zeros\")\n",
        "            elif torch.isnan(conv_weights).any():\n",
        "                issues.append(f\"{block_name}.conv weights contain NaN\")\n",
        "\n",
        "            # Check BN parameters\n",
        "            if torch.all(block.bn.weight.data == 1) and torch.all(block.bn.bias.data == 0):\n",
        "                issues.append(f\"{block_name}.bn parameters are uninitialized (gamma=1, beta=0)\")\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"  {block_name}.conv: shape={tuple(conv_weights.shape)}, \"\n",
        "                      f\"mean={conv_weights.mean().item():.6f}, std={conv_weights.std().item():.6f}\")\n",
        "                print(f\"  {block_name}.bn: gamma_mean={block.bn.weight.mean().item():.6f}, \"\n",
        "                      f\"beta_mean={block.bn.bias.mean().item():.6f}\")\n",
        "\n",
        "    # Check upsampling blocks\n",
        "    if verbose:\n",
        "        print(f\"\\n  Upsampling blocks: {len(model.upsample_blocks)} blocks\")\n",
        "\n",
        "    for idx, block_dict in enumerate(model.upsample_blocks):\n",
        "        fused_block = block_dict['conv_bn_relu']\n",
        "\n",
        "        conv_weights = fused_block.conv.weight.data\n",
        "        expected_kernel_size = 5\n",
        "        actual_kernel_size = conv_weights.shape[2]\n",
        "\n",
        "        if actual_kernel_size != expected_kernel_size:\n",
        "            issues.append(f\"upsample_blocks[{idx}] has {actual_kernel_size}x{actual_kernel_size} kernel, \"\n",
        "                         f\"expected {expected_kernel_size}x{expected_kernel_size}\")\n",
        "\n",
        "        if torch.all(conv_weights == 0):\n",
        "            issues.append(f\"upsample_blocks[{idx}].conv weights are all zeros\")\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"  upsample_blocks[{idx}].conv: shape={tuple(conv_weights.shape)}, \"\n",
        "                  f\"kernel_size={actual_kernel_size}x{actual_kernel_size}, \"\n",
        "                  f\"mean={conv_weights.mean().item():.6f}\")\n",
        "\n",
        "    # Check prediction layer\n",
        "    pred_weights = model.prediction.weight.data\n",
        "    if torch.all(pred_weights == 0):\n",
        "        issues.append(\"prediction layer weights are all zeros\")\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n  prediction: shape={tuple(pred_weights.shape)}, \"\n",
        "              f\"mean={pred_weights.mean().item():.6f}\")\n",
        "\n",
        "    # Report results\n",
        "    if verbose:\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        if issues:\n",
        "            print(\"⚠️ VALIDATION WARNINGS:\")\n",
        "            for issue in issues:\n",
        "                print(f\"  - {issue}\")\n",
        "        else:\n",
        "            print(\"✓ ALL WEIGHTS VALIDATED SUCCESSFULLY\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "    return len(issues) == 0\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "# ============================================================================\n",
        "# Configuration\n",
        "# ============================================================================\n",
        "class Config:\n",
        "    \"\"\"Configuration for AutoDS inference\"\"\"\n",
        "    # Data paths\n",
        "    Data_folder = Data_folder\n",
        "    Result_folder = \"/content/gdrive/MyDrive/AutoDS/Results/TOM20_10nM/V2\"  #@param {type:\"string\"}\n",
        "\n",
        "    # --- Quiet/Preview flags ------------------------------------------------------\n",
        "    QUIET = False  # no training/inference chatter unless set to False\n",
        "    HEADLESS_PREVIEW = True  # set True if you want to see the preview figures\n",
        "\n",
        "    # Detection parameters\n",
        "    threshold = 10 #@param {type:\"number\"}\n",
        "    neighborhood_size = 3 #@param {type:\"integer\"}\n",
        "    use_local_average = True #@param {type:\"boolean\"}\n",
        "\n",
        "    # Patch parameters\n",
        "    num_patches = 8 #@param {type:\"number\"}\n",
        "    overlap = 4 #@param {type:\"number\"}\n",
        "    patch_batch_size = 32 #@param {type:\"number\"}\n",
        "    frame_batch_size = 10  #@param {type:\"number\"}\n",
        "\n",
        "    interpolate_based_on_imaging_parameters = True #@param {type:\"boolean\"}\n",
        "    get_pixel_size_from_file = False #@param {type:\"boolean\"}\n",
        "    pixel_size = 233 #@param {type:\"number\"}\n",
        "    wavelength = 233 #@param {type:\"number\"}\n",
        "    numerical_aperture = 1.49 #@param {type:\"number\"}\n",
        "\n",
        "    chunk_size = 10000 #@param {type:\"number\"}\n",
        "\n",
        "    # Timing parameters\n",
        "    enable_timing = True  # Set to True to enable detailed timing profiling\n",
        "\n",
        "    # Model parameters\n",
        "    use_pytorch_weights = False  # Set to True to use .pth weights, False to use .h5 weights\n",
        "\n",
        "    # Model paths\n",
        "    prediction_model_path = \"/content/AutoDS_models\"\n",
        "    model_names = ['diff_1', 'diff_2', 'diff_3', 'diff_4']\n",
        "\n",
        "    # Model manifest for downloading\n",
        "    MODEL_MANIFEST = {\n",
        "        \"diff_1\": {\n",
        "            \"file_urls\": {\n",
        "                # Add your PyTorch weights URL here if using use_pytorch_weights=True:\n",
        "                # \"best_weights.pth\": \"https://your-url/diff_1/best_weights.pth\",\n",
        "                # Or keep Keras weights if using use_pytorch_weights=False:\n",
        "                \"best_weights.h5\": \"https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/raw/main/AutoDS/models/diff_1/best_weights.h5\",\n",
        "                \"model_metadata.mat\": \"https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/raw/main/AutoDS/models/diff_1/model_metadata.mat\",\n",
        "            },\n",
        "            \"contains\": [\"model_metadata.mat\"]  # Weights file checked based on use_pytorch_weights\n",
        "        },\n",
        "        \"diff_2\": {\n",
        "            \"file_urls\": {\n",
        "                # \"best_weights.pth\": \"https://your-url/diff_2/best_weights.pth\",\n",
        "                \"best_weights.h5\": \"https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/raw/main/AutoDS/models/diff_2/best_weights.h5\",\n",
        "                \"model_metadata.mat\": \"https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/raw/main/AutoDS/models/diff_2/model_metadata.mat\",\n",
        "            },\n",
        "            \"contains\": [\"model_metadata.mat\"]\n",
        "        },\n",
        "        \"diff_3\": {\n",
        "            \"file_urls\": {\n",
        "                # \"best_weights.pth\": \"https://your-url/diff_3/best_weights.pth\",\n",
        "                \"best_weights.h5\": \"https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/raw/main/AutoDS/models/diff_3/best_weights.h5\",\n",
        "                \"model_metadata.mat\": \"https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/raw/main/AutoDS/models/diff_3/model_metadata.mat\",\n",
        "            },\n",
        "            \"contains\": [\"model_metadata.mat\"]\n",
        "        },\n",
        "        \"diff_4\": {\n",
        "            \"file_urls\": {\n",
        "                # \"best_weights.pth\": \"https://your-url/diff_4/best_weights.pth\",\n",
        "                \"best_weights.h5\": \"https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/raw/main/AutoDS/models/diff_4/best_weights.h5\",\n",
        "                \"model_metadata.mat\": \"https://github.com/alonsaguy/One-click-image-reconstruction-in-single-molecule-localization-microscopy-via-deep-learning/raw/main/AutoDS/models/diff_4/model_metadata.mat\",\n",
        "            },\n",
        "            \"contains\": [\"model_metadata.mat\"]\n",
        "        },\n",
        "    }\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Global state - simple module-level variables\n",
        "# ============================================================================\n",
        "\n",
        "_models = {}  # Dictionary to store loaded models\n",
        "_device = None  # Device (CPU or CUDA)\n",
        "_is_initialized = False  # Track if we've loaded models\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Simple functions to manage the cache\n",
        "# ============================================================================\n",
        "\n",
        "def initialize_model_cache(config, upsampling_factor, device=None, use_pytorch_weights=False):\n",
        "    \"\"\"\n",
        "    Load all models once and store them in memory\n",
        "\n",
        "    Args:\n",
        "        config: Configuration object with model paths and names\n",
        "        upsampling_factor: Upsampling factor for the models\n",
        "        device: Device to load models on (CPU or CUDA)\n",
        "        use_pytorch_weights: If True, load .pth weights. If False, load .h5 weights\n",
        "    \"\"\"\n",
        "    global _models, _device, _is_initialized\n",
        "\n",
        "    # Skip if already initialized\n",
        "    if _is_initialized:\n",
        "        print(\"⚠️ Models already loaded, skipping initialization\")\n",
        "        return\n",
        "\n",
        "    # Setup device\n",
        "    if device is None:\n",
        "        _device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    else:\n",
        "        _device = device\n",
        "\n",
        "    print(f\"Weight Format: {'PyTorch (.pth)' if use_pytorch_weights else 'Keras (.h5)'}\")\n",
        "\n",
        "    # Determine weight file extension\n",
        "    weight_extension = 'best_weights.pth' if use_pytorch_weights else 'best_weights.h5'\n",
        "    weight_type = \"PyTorch\" if use_pytorch_weights else \"Keras\"\n",
        "\n",
        "    # Load each model\n",
        "    for model_num, model_name in enumerate(config.model_names):\n",
        "        model_path = os.path.join(\n",
        "            config.prediction_model_path,\n",
        "            model_name,\n",
        "            weight_extension\n",
        "        )\n",
        "\n",
        "        if not os.path.exists(model_path):\n",
        "            raise FileNotFoundError(\n",
        "                f\"Model weights not found: {model_path}\\n\"\n",
        "                f\"Expected {weight_type} weights for model: {model_name}\"\n",
        "            )\n",
        "\n",
        "        print(f\"\\nLoading model {model_num + 1}/{len(config.model_names)}: {model_name} ({weight_type} weights)\")\n",
        "\n",
        "        # Create model\n",
        "        model = CNNUpsample(in_channels=1, upsampling_factor=upsampling_factor)\n",
        "        model = model.to(_device)\n",
        "        model.eval()\n",
        "\n",
        "        # Load weights\n",
        "        load_model_weights(model, model_path, verbose=False)\n",
        "\n",
        "        # Optimize for inference\n",
        "        if torch.cuda.is_available():\n",
        "            try:\n",
        "                model = model.to(memory_format=torch.channels_last)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Store in cache\n",
        "        _models[model_num] = model\n",
        "\n",
        "        # Print memory usage\n",
        "        if torch.cuda.is_available():\n",
        "            allocated = torch.cuda.memory_allocated() / 1024**2\n",
        "            print(f\"  ✓ Loaded {model_name} (GPU Memory: {allocated:.1f} MB)\")\n",
        "        else:\n",
        "            print(f\"  ✓ Loaded {model_name}\")\n",
        "\n",
        "    _is_initialized = True\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        total_allocated = torch.cuda.memory_allocated() / 1024**2\n",
        "        print(f\"Total GPU Memory Used: {total_allocated:.1f} MB\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "\n",
        "def get_model(model_num):\n",
        "    \"\"\"Get a cached model by its number\"\"\"\n",
        "    if not _is_initialized:\n",
        "        raise RuntimeError(\"Models not loaded. Call initialize_model_cache() first.\")\n",
        "\n",
        "    if model_num not in _models:\n",
        "        raise KeyError(f\"Model {model_num} not found. Available: {list(_models.keys())}\")\n",
        "\n",
        "    return _models[model_num]\n",
        "\n",
        "\n",
        "def get_device():\n",
        "    \"\"\"Get the device being used (CPU or CUDA)\"\"\"\n",
        "    if _device is None:\n",
        "        raise RuntimeError(\"Model cache not initialized. Call initialize_model_cache() first.\")\n",
        "\n",
        "    return _device\n",
        "\n",
        "\n",
        "def clear_cache():\n",
        "    \"\"\"Clear all cached models from memory\"\"\"\n",
        "    global _models, _device, _is_initialized\n",
        "\n",
        "    print(\"\\n⚠️ Clearing model cache...\")\n",
        "\n",
        "    # Move models to CPU and delete\n",
        "    for model in _models.values():\n",
        "        model.cpu()\n",
        "\n",
        "    _models.clear()\n",
        "    _device = None\n",
        "    _is_initialized = False\n",
        "\n",
        "    # Clear GPU memory\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"✓ Model cache cleared\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Compatibility wrapper (optional - for backwards compatibility)\n",
        "# ============================================================================\n",
        "\n",
        "class ModelCacheManager:\n",
        "    \"\"\"Simple wrapper class for compatibility with existing code\"\"\"\n",
        "\n",
        "    def initialize(self, config, upsampling_factor, device=None, use_pytorch_weights=False):\n",
        "        initialize_model_cache(config, upsampling_factor, device, use_pytorch_weights)\n",
        "\n",
        "    def get_model(self, model_num):\n",
        "        return get_model(model_num)\n",
        "\n",
        "    def get_device(self):\n",
        "        return get_device()\n",
        "\n",
        "    def clear_cache(self):\n",
        "        clear_cache()\n",
        "\n",
        "\n",
        "def get_model_cache():\n",
        "    \"\"\"Return a simple manager instance for compatibility\"\"\"\n",
        "    return ModelCacheManager()\n",
        "\n",
        "\n",
        "# Setup\n",
        "config = Config()\n",
        "\n",
        "# ============================================================================\n",
        "# Entry Point\n",
        "# ============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    if device.type != 'cuda':\n",
        "        log('You do not have GPU access.')\n",
        "        log('Did you change your runtime?')\n",
        "        log('If the runtime settings are correct then GPU might not be allocated to your session.')\n",
        "        log('Expect slow performance. To access GPU try reconnecting later.')\n",
        "    else:\n",
        "        log('You have GPU access')\n",
        "        log('PyTorch version is ' + str(torch.__version__)+'\\n')\n",
        "\n",
        "    # Initialize timing profiler\n",
        "    profiler = timing_profiler(enabled=config.enable_timing)\n",
        "\n",
        "    config.prediction_model_path = ensure_models(config.model_names, target_root=config.prediction_model_path,\n",
        "                                                 model_manifest=config.MODEL_MANIFEST)\n",
        "\n",
        "    MAX_FILE_GB = 5.0  # warn & skip when file is larger than this\n",
        "\n",
        "    # PSF parameters\n",
        "    psf_sigma_nm = 0.21 * config.wavelength / config.numerical_aperture\n",
        "    psf_sigma_pixels = psf_sigma_nm / config.pixel_size\n",
        "\n",
        "    if config.get_pixel_size_from_file:\n",
        "        pixel_size = None\n",
        "\n",
        "    # Load model metadata\n",
        "    matfile = sio.loadmat(os.path.join(config.prediction_model_path, config.model_names[0], 'model_metadata.mat'))\n",
        "    try:\n",
        "        model_wavelength = np.array(matfile['wavelength'].item())\n",
        "    except:\n",
        "        model_wavelength = None\n",
        "    try:\n",
        "        model_NA = np.array(matfile['numerical_aperture'].item())\n",
        "    except:\n",
        "        model_NA = None\n",
        "    try:\n",
        "        model_pixel_size = np.array(matfile['pixel_size'].item())\n",
        "    except:\n",
        "        model_pixel_size = None\n",
        "\n",
        "    if os.path.isdir(config.Data_folder):\n",
        "        # iterate both TIFF and ND2\n",
        "        for filename in list_files_multi(config.Data_folder, extensions=['tif', 'tiff', 'nd2']):\n",
        "\n",
        "            # Install nd2 reader only when needed (optional)\n",
        "            if filename.lower().endswith('.nd2'):\n",
        "                try:\n",
        "                    import nd2  # already installed?\n",
        "                except Exception:\n",
        "                    # Jupyter-only magic; remove if not on Colab\n",
        "                    # get_ipython().system('pip install -q nd2')\n",
        "                    import subprocess\n",
        "                    import sys\n",
        "\n",
        "                    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'nd2'])\n",
        "                    import nd2  # now import it\n",
        "\n",
        "            in_path = os.path.join(config.Data_folder, filename)\n",
        "\n",
        "            # --------- file size guard ----------\n",
        "            try:\n",
        "                file_size_gb = os.path.getsize(in_path) / 1e9\n",
        "                if file_size_gb > MAX_FILE_GB:\n",
        "                    print(f\"\\n⚠️  {filename}: {file_size_gb:.2f} GB > {MAX_FILE_GB:.2f} GB.\")\n",
        "                    print(\"   Video size is too big, please use Google Colab Pro or run locally.\")\n",
        "                    continue\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            # --- Resolve pixel size if requested ---\n",
        "            if config.get_pixel_size_from_file:\n",
        "                if is_tiff(in_path):\n",
        "                    with catch_oom(\"reading TIFF pixel size\", filename):\n",
        "                        pixel_size, _, _ = getPixelSizeTIFFmetadata(in_path, True)\n",
        "                elif is_nd2(in_path):\n",
        "                    with catch_oom(\"reading ND2 pixel size\", filename):\n",
        "                        px_nm, _, _ = getPixelSizeND2metadata(in_path, True)\n",
        "                        pixel_size = px_nm if px_nm is not None else pixel_size  # leave unchanged if unknown\n",
        "\n",
        "            # --- Common model params ---\n",
        "            upsampling_factor = np.array(matfile['upsampling_factor']).item()\n",
        "            try:\n",
        "                L2_weighting_factor = np.array(matfile['Normalization factor']).item()\n",
        "            except:\n",
        "                L2_weighting_factor = 100\n",
        "\n",
        "            # save all models to cache\n",
        "            initialize_model_cache(config, upsampling_factor, device,\n",
        "                                   use_pytorch_weights=config.use_pytorch_weights)\n",
        "\n",
        "            # --- Choose reader & frame count ---\n",
        "            number_of_frames, frame_iter = None, None\n",
        "            with catch_oom(\"opening stack\", filename):\n",
        "                if is_tiff(in_path):\n",
        "                    number_of_frames = count_tiff_frames(in_path)\n",
        "                    frame_iter = iter_tiff_frames(in_path)\n",
        "                    log(f'\\nLoaded tiff stack with {number_of_frames} frames')\n",
        "                elif is_nd2(in_path):\n",
        "                    number_of_frames = count_nd2_frames(in_path)\n",
        "                    frame_iter = iter_nd2_frames(in_path)\n",
        "                    log(f'\\nLoaded ND2 stack with ~{number_of_frames} planes (T*Z*C)')\n",
        "                else:\n",
        "                    log(f\"Skipping unsupported file: {filename}\")\n",
        "\n",
        "            if frame_iter is None:\n",
        "                print(f\"⚠️  Skipping {filename} due to earlier error.\")\n",
        "                continue\n",
        "\n",
        "            # Initialize patch lists for each model\n",
        "            patches_list = [[] for _ in config.model_names]\n",
        "            patch_indices_list = [[] for _ in config.model_names]\n",
        "            frame_numbers = [[] for _ in config.model_names]\n",
        "\n",
        "            # Initialize accumulator variables\n",
        "            M, N = None, None\n",
        "            sum_image = None\n",
        "            patchwise_recon = None\n",
        "            frame_number_list, x_nm_list, y_nm_list, confidence_au_list = [], [], [], []\n",
        "            total_selected_model_hist = np.zeros(len(config.model_names), dtype=float)\n",
        "\n",
        "            # Progress bar for overall process\n",
        "            pbar = tqdm(total=number_of_frames, desc=\"Processing frames\")\n",
        "\n",
        "            for frame_start in range(0, number_of_frames, config.frame_batch_size):\n",
        "                frame_end = min(frame_start + config.frame_batch_size, number_of_frames)\n",
        "\n",
        "                # Collect patches from multiple frames\n",
        "                all_valid_patches = []\n",
        "                all_full_patches = []\n",
        "                all_patches_local_indices = []\n",
        "                all_frames_numbers = []\n",
        "                all_patches_offset = []\n",
        "\n",
        "                # for DEBUG\n",
        "                fproc_list = []\n",
        "                offset_list = []\n",
        "\n",
        "                frames_list = []\n",
        "                # Process frames in this batch\n",
        "\n",
        "                for frame_idx in range(frame_start, frame_end):\n",
        "                    # Start preprocessing timing for EACH frame\n",
        "                    profiler.start_timer(\"preprocessing.frame_reading_and_splitting\")\n",
        "\n",
        "                    # Get frame\n",
        "                    frame_i = next(frame_iter)\n",
        "\n",
        "                    # Initialize sum_image and dimensions on first frame\n",
        "                    if sum_image is None:\n",
        "                        sum_image = np.zeros_like(frame_i, dtype=np.float32)\n",
        "\n",
        "                        # Interpolate first to get actual dimensions\n",
        "                        if config.interpolate_based_on_imaging_parameters:\n",
        "                            temp_frame = interpolate_frames(\n",
        "                                frame_i,\n",
        "                                model_pixel_size, config.pixel_size,\n",
        "                                model_wavelength, config.wavelength,\n",
        "                                model_NA, config.numerical_aperture\n",
        "                            )[0]\n",
        "                            M, N = temp_frame.shape\n",
        "                        else:\n",
        "                            M, N = frame_i.shape\n",
        "\n",
        "                    # Accumulate for preview\n",
        "                    sum_image += frame_i.astype(np.float32) / number_of_frames\n",
        "\n",
        "                    # Interpolate frame\n",
        "                    if config.interpolate_based_on_imaging_parameters:\n",
        "                        frame_i = interpolate_frames(\n",
        "                            frame_i,\n",
        "                            model_pixel_size, config.pixel_size,\n",
        "                            model_wavelength, config.wavelength,\n",
        "                            model_NA, config.numerical_aperture\n",
        "                        )[0]\n",
        "                    frames_list.append(frame_i)\n",
        "\n",
        "                # Preprocess on GPU\n",
        "                frames_torch = torch.from_numpy(np.array(frames_list)).float().to(device)\n",
        "                fproc_tensor, frames_offsets = preprocess_frames_batch(frames_torch, device)\n",
        "                #fproc_tensor, frames_offsets = preprocess_frames_batch(frames_tensor, device)\n",
        "\n",
        "\n",
        "                # Split all frames to patches (GPU)\n",
        "                all_patches_tensor = split_image_to_patches_batch(\n",
        "                    fproc_tensor,\n",
        "                    config.num_patches,\n",
        "                    config.overlap,\n",
        "                    device=device\n",
        "                )\n",
        "\n",
        "                for frame_idx in range(frame_start, frame_end):\n",
        "                    #fproc = fproc_list[frame_idx - frame_start]\n",
        "                    offset = frames_offsets[frame_idx - frame_start].cpu().item()\n",
        "                    #fproc = fproc_tensor[frame_idx - frame_start].cpu().numpy()\n",
        "                    # Split into patches\n",
        "                    patches = all_patches_tensor[frame_idx - frame_start]\n",
        "\n",
        "                    # Process each patch\n",
        "                    for m in range(config.num_patches):\n",
        "                        for n in range(config.num_patches):\n",
        "                            down = config.overlap if m == 0 else 0\n",
        "                            up = (M // config.num_patches) - config.overlap if m == config.num_patches - 1 else (\n",
        "                                    M // config.num_patches)\n",
        "                            left = config.overlap if n == 0 else 0\n",
        "                            right = (N // config.num_patches) - config.overlap if n == config.num_patches - 1 else (\n",
        "                                    N // config.num_patches)\n",
        "\n",
        "                            local_patch_idx = m * config.num_patches + n\n",
        "                            full_patch = patches[local_patch_idx]\n",
        "                            valid_patch = full_patch[down:up, left:right]\n",
        "\n",
        "                            all_full_patches.append(full_patch)\n",
        "                            all_valid_patches.append(valid_patch)\n",
        "                            all_patches_local_indices.append(local_patch_idx)\n",
        "                            all_patches_offset.append(offset)\n",
        "                            all_frames_numbers.append(frame_idx)\n",
        "\n",
        "\n",
        "                    pbar.update(1)\n",
        "\n",
        "                profiler.stop_timer(\"preprocessing.frame_reading_and_splitting\")\n",
        "                profiler.start_timer(\"preprocessing.feature_extraction\")\n",
        "\n",
        "                # Group patches by size and extract features\n",
        "                shape_groups = defaultdict(lambda: {'patches': [], 'indices': [], 'offsets': []})\n",
        "\n",
        "                for idx, patch in enumerate(all_valid_patches):\n",
        "                    shape = patch.shape\n",
        "                    shape_groups[shape]['patches'].append(patch)\n",
        "                    shape_groups[shape]['indices'].append(idx)\n",
        "                    shape_groups[shape]['offsets'].append(all_patches_offset[idx])\n",
        "\n",
        "                # Process each size group\n",
        "                all_features = []\n",
        "\n",
        "                for shape, group_data in shape_groups.items():\n",
        "                    patches_tensor  = torch.stack(group_data['patches'])\n",
        "                    offsets_array = np.array(group_data['offsets'])\n",
        "\n",
        "                    features_batch = extract_features_batch(\n",
        "                        patches_tensor,\n",
        "                        config.pixel_size,\n",
        "                        psf_sigma_pixels,\n",
        "                        offsets_array,\n",
        "                        verbose=False,\n",
        "                        device=device\n",
        "                    )\n",
        "\n",
        "                    for feat, idx in zip(features_batch, group_data['indices']):\n",
        "                        all_features.append((feat, idx))\n",
        "\n",
        "                profiler.stop_timer(\"preprocessing.feature_extraction\")\n",
        "                profiler.start_timer(\"preprocessing.patch_classification\")\n",
        "\n",
        "                # Classify and accumulate patches for reconstruction\n",
        "                for features, idx in all_features:\n",
        "                    curr_mean_noise, curr_std_noise, signal_amp, curr_emitter_density = features\n",
        "\n",
        "                    # Skip invalid patches\n",
        "                    if signal_amp == 0 or curr_mean_noise == 0:\n",
        "                        continue\n",
        "                    if any(np.isnan(v) for v in (signal_amp, curr_mean_noise, curr_std_noise, curr_emitter_density)):\n",
        "                        continue\n",
        "\n",
        "                    # Choose difficulty level\n",
        "                    difficulty_choice = ChooseNetByDifficulty_2025(\n",
        "                        curr_emitter_density,\n",
        "                        signal_amp / curr_mean_noise\n",
        "                    )\n",
        "\n",
        "                    # Store patch data\n",
        "                    patches_list[difficulty_choice].append(all_full_patches[idx])\n",
        "                    patch_indices_list[difficulty_choice].append(all_patches_local_indices[idx])\n",
        "                    frame_numbers[difficulty_choice].append(all_frames_numbers[idx])\n",
        "\n",
        "                profiler.stop_timer(\"preprocessing.patch_classification\")\n",
        "\n",
        "                # Initialize reconstruction array on first batch and move it to the GPU\n",
        "                if patchwise_recon is None:\n",
        "                    M, N = fproc_tensor.shape[1], fproc_tensor.shape[2]\n",
        "                    patchwise_recon = torch.zeros(M * upsampling_factor, N * upsampling_factor,\n",
        "                                                  dtype=torch.float32, device=device)\n",
        "\n",
        "                # Check if there are any patches to process\n",
        "                total_patches = sum(len(patches) for patches in patches_list)\n",
        "                if total_patches > 0:\n",
        "                    # Process with each model\n",
        "                    for model_num, model_name in enumerate(config.model_names):\n",
        "                        if not patches_list[model_num]:\n",
        "                            continue\n",
        "\n",
        "                        # Process in chunks\n",
        "                        t_chunks = (len(patches_list[model_num]) // config.chunk_size) + 1\n",
        "\n",
        "                        for chunk_num in range(t_chunks):\n",
        "                            chunk_start = chunk_num * config.chunk_size\n",
        "                            chunk_end = min((chunk_num + 1) * config.chunk_size, len(patches_list[model_num]))\n",
        "\n",
        "                            if chunk_start >= chunk_end:\n",
        "                                continue\n",
        "\n",
        "                            # Reconstruct using CACHED model\n",
        "                            pw_recon, loc_list = reconstruct_patches_2025_pytorch(\n",
        "                                torch.stack(patches_list[model_num][chunk_start:chunk_end]),\n",
        "                                patch_indices_list[model_num][chunk_start:chunk_end],\n",
        "                                frame_numbers[model_num][chunk_start:chunk_end],\n",
        "                                model_num,\n",
        "                                config.num_patches,\n",
        "                                config.overlap * upsampling_factor,\n",
        "                                number_of_frames,\n",
        "                                config.threshold,\n",
        "                                neighborhood_size=config.neighborhood_size,\n",
        "                                use_local_avg=config.use_local_average,\n",
        "                                upsampling_factor=upsampling_factor,\n",
        "                                pixel_size=config.pixel_size,\n",
        "                                batch_size=config.patch_batch_size,\n",
        "                                L2_weighting_factor=L2_weighting_factor,\n",
        "                                profiler=profiler\n",
        "                            )\n",
        "\n",
        "                            # Accumulate results\n",
        "                            frame_number_list.extend(loc_list[0])\n",
        "                            x_nm_list.extend(loc_list[1])\n",
        "                            y_nm_list.extend(loc_list[2])\n",
        "                            confidence_au_list.extend(loc_list[3])\n",
        "\n",
        "                            patchwise_recon[:M // config.num_patches * upsampling_factor * config.num_patches,\n",
        "                                            :N // config.num_patches * upsampling_factor * config.num_patches] += pw_recon\n",
        "\n",
        "                    # Clear patches lists after processing to free memory\n",
        "                    for i in range(len(patches_list)):\n",
        "                        patches_list[i].clear()\n",
        "                        patch_indices_list[i].clear()\n",
        "                        frame_numbers[i].clear()\n",
        "\n",
        "                    ## Force garbage collection\n",
        "                    #if torch.cuda.is_available():\n",
        "                    #    torch.cuda.empty_cache()\n",
        "\n",
        "            # close progress bar\n",
        "            pbar.close()\n",
        "\n",
        "            if not os.path.exists(config.Result_folder):\n",
        "                print('Result folder was created.')\n",
        "                os.makedirs(config.Result_folder, exist_ok=True)\n",
        "\n",
        "            print(f\"\\n{'=' * 70}\")\n",
        "            print(f\"Streaming processing complete for {filename}\")\n",
        "            print(f\"Total localizations found: {len(frame_number_list)}\")\n",
        "            print(f\"{'=' * 70}\")\n",
        "\n",
        "            # Save results\n",
        "            os.makedirs(config.Result_folder, exist_ok=True)\n",
        "            ext = '_avg' if config.use_local_average else '_max'\n",
        "            base = os.path.splitext(filename)[0]\n",
        "\n",
        "            # Save localizations\n",
        "            with open(os.path.join(config.Result_folder, f'Localizations_{base}{ext}.csv'), \"w\", newline='') as file:\n",
        "                writer = csv.writer(file)\n",
        "                writer.writerow(['frame', 'x [nm]', 'y [nm]', 'confidence [a.u]'])\n",
        "                sort_ind = np.argsort(frame_number_list)\n",
        "                locs = list(zip(\n",
        "                    list(np.array(frame_number_list)[sort_ind]),\n",
        "                    list(np.array(x_nm_list)[sort_ind]),\n",
        "                    list(np.array(y_nm_list)[sort_ind]),\n",
        "                    list(np.array(confidence_au_list)[sort_ind])\n",
        "                ))\n",
        "                writer.writerows(locs)\n",
        "\n",
        "            print(f\"Saved {len(frame_number_list)} localizations\")\n",
        "\n",
        "            # move image to CPU for saving\n",
        "            patchwise_recon = patchwise_recon.cpu().numpy()\n",
        "\n",
        "            # Save reconstruction\n",
        "            pw_recon_tif = np.copy(patchwise_recon)\n",
        "            cap = np.percentile(pw_recon_tif, 99.5)\n",
        "            pw_recon_tif[pw_recon_tif > cap] = cap\n",
        "            saveAsTIF(config.Result_folder, f\"Predicted_patchwise_{base}\", pw_recon_tif, config.pixel_size / upsampling_factor)\n",
        "\n",
        "            # Create preview\n",
        "            fig, axes = plt.subplots(1, 3, figsize=(20, 16))\n",
        "            axes[0].axis('off')\n",
        "            axes[0].imshow(sum_image)\n",
        "            axes[0].set_title('Original', fontsize=15)\n",
        "            axes[1].axis('off')\n",
        "            axes[1].imshow(patchwise_recon)\n",
        "            axes[1].set_title('Prediction', fontsize=15)\n",
        "            axes[2].axis('off')\n",
        "            axes[2].imshow(np.clip(patchwise_recon,\n",
        "                                   np.percentile(patchwise_recon, 1),\n",
        "                                   np.percentile(patchwise_recon, 99)))\n",
        "            axes[2].set_title('Normalized Prediction', fontsize=15)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(config.Result_folder, f'preview_{base}.png'), dpi=150)\n",
        "            plt.close()\n",
        "\n",
        "            print(f\"\\nCompleted processing: {filename}\")\n",
        "\n",
        "            # Print timing summary at the end\n",
        "            profiler.print_timing_summary()\n",
        "\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "# Clear GPU memory\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.synchronize()\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "di0hVAvzawp9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}